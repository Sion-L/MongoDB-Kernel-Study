# 1. 导语
如何做好连接控制，基本上是使用网络服务都需要考虑的问题。     
在 MongoDB 场景下连接模型更为复杂。除了需要考虑 client 和 mongos 的多对多的关系，还要考虑 mongos 和 shard 之间的多对多的关系。    

如果连接太少，可能会造成请求排队超时，发挥不出 MongoDB 的整体性能。    
如果连接太多，可能会导致资源浪费，线程数太多，导致 MongoDB 性能受损。    
除了控制连接数，如何正确地利用连接超时、刷新、动态调整，都是本文要重点讨论的问题。    

# 2. 连接模型的性能问题
为什么连接控制会是一个值得讨论的话题呢？   

说到连接，首先会想到 “短连接” 的使用方式：创建一个连接，然后执行命令并返回结果，最后断开连接并结束整个使用流程。    
但是 “短连接” 模式并不是那么高效，特别是对于注重请求处理时延的场景，连接建立和断开对应 TCP 握手和挥手的多轮 RTT 延迟。如果使用了 TLS 模式，则 TLS 又会带来额外的延迟。      

对 MongoDB 而言，除了 RTT 带来的延迟问题，还有 2 个特别显著的性能问题需要关注：    

- 连接和线程的绑定关系
- 连接之后带来的认证开销

## 2.1 连接和线程
不论是 mongos 还是 mongod 进程，在完成 TCP 连接的 accept 动作之后，会创建一个线程用于服务这个 TCP 连接上的所有请求。    
具体来说，会创建一个 ServiceStateMachine 对象用来完成：接受 MongoDB 命令-> 执行命令（读写数据，过滤结果等） -> 返回执行结果给客户端 -> 接受下一个 MongoDB 命令 -> 执行下一个命令 -> 返回下一个命令的结果 -> 以此类推。     

也就是说，在 MongoDB 服务端（mongos 和 mongod），连接和线程是一一对应的。**连接的创建和断开，也会伴随着后台线程的创建和销毁。**    

## 2.2 认证开销
连接建立后，必须要在这个连接上成功执行认证流程，才能执行具体的命令。    
MongoDB 默认的认证算法是 [SCRAM-SHA-1](https://github.com/mongodb/specifications/blob/master/source/auth/auth.md#scram-sha-1) 和 [SCRAM-SHA-256](https://github.com/mongodb/specifications/blob/master/source/auth/auth.md#scram-sha-256) （高版本），都需要客户端和服务端交互 2 次，即 2 个 RTT。以 SCARM-SHA-1 为例，客户端需要顺序执行 saslStart 和 saslContinue 命令：   
```
// 逻辑流程
CMD = "n,,n=user,r=fyko+d2lbbFgONRv9qkxdawL"
RESP = "r=fyko+d2lbbFgONRv9qkxdawLHo+Vgk7qvUOKUwuWLIWg4l/9SraGMHEE,s=rQ9ZY3MntBeuP3E1TDVC4w==,i=10000"
CMD = "c=biws,r=fyko+d2lbbFgONRv9qkxdawLHo+Vgk7qvUOKUwuWLIWg4l/9SraGMHEE,p=MC2T8BvbmWRckDw8oWl5IVghwCY="
RESP = "v=UMWeI25JD1yNYZRMpZ4VHvhZ9e0="

// 对应的命令
CMD = {saslStart: 1, mechanism: "SCRAM-SHA-1", payload: BinData(0, "biwsbj11c2VyLHI9ZnlrbytkMmxiYkZnT05Sdjlxa3hkYXdM"), options: { skipEmptyExchange: true }}
RESP = {conversationId : 1, payload: BinData(0,"cj1meWtvK2QybGJiRmdPTlJ2OXFreGRhd0xIbytWZ2s3cXZVT0tVd3VXTElXZzRsLzlTcmFHTUhFRSxzPXJROVpZM01udEJldVAzRTFURFZDNHc9PSxpPTEwMDAw"), done: false, ok: 1}
CMD = {saslContinue: 1, conversationId: 1, payload: BinData(0, "Yz1iaXdzLHI9ZnlrbytkMmxiYkZnT05Sdjlxa3hkYXdMSG8rVmdrN3F2VU9LVXd1V0xJV2c0bC85U3JhR01IRUUscD1NQzJUOEJ2Ym1XUmNrRHc4b1dsNUlWZ2h3Q1k9")}
RESP = {conversationId: 1, payload: BinData(0,"dj1VTVdlSTI1SkQxeU5ZWlJNcFo0Vkh2aFo5ZTA9"), done: true, ok: 1}
```

示意图如下：

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9a9654e3-cebd-4492-bc6e-b55a8d84429a" width=700>
</p>

除了网络交互带来的延迟问题，认证命令还会导致 MongoDB 服务端的 CPU 使用率变高。    
从上面的流程可以看到，认证过程中需要生成随机数来保证流程的安全性。MongoDB 为了保证随机数的“真随机”，会通过系统调用读取 [/dev/urandom](https://github.com/mongodb/mongo/blob/r5.0.26/src/mongo/platform/random.cpp#L160) 中的数据，消耗较多的 CPU 资源。    
如果将这里的算法调整为真随机数和伪随机数相结合的方式，能够有效缓解 CPU 消耗。    

# 3. 连接池
连接池是一个能够有效解决连接问题的方案，对 MongoDB 来说，也是如此。     
下面探讨使用连接池需要注意的问题：超时处理、刷新机制以及如何进行动态调整。    

## 3.1 Client 到 MongoDB 
以官方的 go driver 为例，客户端指定连接参数构建 MongoClient，然后通过 MongoClient 执行 MongoDB 命令。    

在构建 MongoClient 时，可以指定一个或者多个 IP+Port 作为种子节点。如果连接的是副本集，而且没有指定 directConnection 模式，则 MongoClient 还会自动探测副本集中包含的，但是没有在种子节点列表中指定的其他节点。MongoClient 会对这些节点分别创建连接池。    
MongoClient 支持多个线程/协程之间共享。但是 MongoClient 连接池中的每个连接不能同时被多个线程和协程使用，官方 driver 已经提供了这项安全性保证。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/04f5f024-1222-4a25-8a32-020a8212a27b" width=600>
</p>

### 3.1.1 超时机制

MongoDB 的超时处理，可以分为客户端超时和服务端超时，2 者的行为存在不同：    
- 客户端超时。客户端在指定时间内收不到响应包，可能是服务端迟迟没有返回数据，或者是网络存在问题。对于这种场景，客户端的处理方法是销毁当前连接。
- 服务端超时。服务端执行命令耗时超过指定时间（比如一个扫描很多数据的大查询），主动给客户端发送一个响应包说明情况。对于这种场景，能够明确知道不是网络原因，因此也没有必要销毁当前连接。

正如在介绍 [maxTimeMS 服务端超时](https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/blob/main/5.%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/3.%E8%AD%A6%E6%83%95%E8%AF%B7%E6%B1%82%E6%8C%A4%E5%8E%8B%EF%BC%8C%E9%98%B2%E6%AD%A2%E9%9B%AA%E5%B4%A9.md) 章节中的描述，理想的情况是将服务端超时和客户端超时统一：
- 如果客户端超时了，不想再等待服务端的结果，那么服务端最好也要终止毫无意义的请求。   
- 最好是服务端超时，而不是客户端超时。这样可以避免不必要的连接销毁和创建。    

幸运的是，高版本的官方 driver（比如 go driver v1.10） 已经为用户做了这些优化。    

在使用 go driver 时，有多处地方可以设置超时：     
- context 的 deadline.
- 创建 client 时设置 timeout.
- 在执行命令（比如 find）时，可以指定 maxTimeMS.

前 2 个超时都属于客户端超时。用户不必 2 个都设置，如果设置了 client 的 timeout，也会自动配置到 context 的 deadline 中，参考 [Operation.Execute](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/x/mongo/driver/operation.go#L311) 的实现：   
```
  // If no deadline is set on the passed-in context, op.Timeout is set, and context is not already
	// a Timeout context, honor op.Timeout in new Timeout context for operation execution.
	if _, deadlineSet := ctx.Deadline(); !deadlineSet && op.Timeout != nil && !internal.IsTimeoutContext(ctx) {
    // 构造一个带 deadline 的 Context
		newCtx, cancelFunc := internal.MakeTimeoutContext(ctx, *op.Timeout)
		// Redefine ctx to be the new timeout-derived context.
		ctx = newCtx
		// Cancel the timeout-derived context at the end of Execute to avoid a context leak.
		defer cancelFunc()
	}
```

如果没有设置服务端超时 maxTimeMS，则 driver 会自动使用 context 的 deadline 进行设置，参考 Operation.Execute 中的实现：    
```
  // Calculate value of 'maxTimeMS' field to potentially append to the wire message based on the current
  // context's deadline and the 90th percentile RTT if the ctx is a Timeout Context.
  var maxTimeMS uint64
  if internal.IsTimeoutContext(ctx) {
    if deadline, ok := ctx.Deadline(); ok {
      remainingTimeout := time.Until(deadline)

      // 使用 deadline 时间，减去 RTT 的 90% 分位时间，作为服务端的超时时间
      // 这里减去 RTT 的时间是非常明智的。否则会由于 RTT 的存在，客户端先超时。
      maxTimeMSVal := int64(remainingTimeout/time.Millisecond) -
        int64(srvr.RTT90()/time.Millisecond)

      // A maxTimeMS value <= 0 indicates that we are already at or past the Context's deadline.
      if maxTimeMSVal <= 0 {
        return internal.WrapErrorf(ErrDeadlineWouldBeExceeded,
          "Context deadline has already been surpassed by %v", remainingTimeout)
      }
      maxTimeMS = uint64(maxTimeMSVal)
    }
  }

  // convert to wire message
  if len(scratch) > 0 {
    scratch = scratch[:0]
  }
  // 将 maxTimeMS 放在请求包中，发给服务端
  wm, startedInfo, err := op.createWireMessage(ctx, scratch, desc, maxTimeMS, conn)
```

如果上述 3 个地方的超时都有配置，而且配置的还不一样，那么以哪个为准呢？    
结合代码分析和实际测试，优先级为 context.deadline > client.timeout > FindOptions.MaxTimeout.         

### 3.1.2 连接池动态调整   
在创建 MongoClient 时，可以指定连接池的上下限。默认情况下，连接池的下限是 [0](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/options/clientoptions.go#L597)，可以通过 [MinPoolSize](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/options/clientoptions.go#L111) 配置，上限是 [100](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/client.go#L39)，可以通过 [MaxPoolSize](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/options/clientoptions.go#L110) 配置。       

>注意这里的连接池设置是 mongod 节点级别的。比如一个副本集有 3 个节点，那么 MongoClient 的出连接数量上限是 3 * pool_max.       

**连接池向上调整**     

当应用程序从连接池中取不到连接时，会在不超过配置上限的情况下，触发创建新的连接。        
为了避免请求突发，导致瞬间创建太多连接，默认同时创建的连接数为 [2](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/options/clientoptions.go#L605)，可以通过 [MaxConnecting](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/mongo/options/clientoptions.go#L112) 配置。        

每个连接池维护 [idleConns](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/x/mongo/driver/topology/pool.go#L120) 的 slice 结构，通过对数据结构的尾部操作，实现一个连接栈。在连接的 CheckOut 流程中，会从 idleConns 的末尾取连接。在连接用完后，Close 操作并不会真正断开 TCP 连接，而是 CheckIn 回收到连接池的 idleConns 中。     

**连接池向下调整**      

由于 idleConns 逻辑上是一个栈，因此在**业务空闲时**，可能只有栈顶的几个连接被使用，而处于**栈底的连接会长期得不到使用**。此时会有相应的机制，将栈底的连接回收，实现连接池的向下调整。    

具体来说，每个连接都有一个 [idleTimeout/idleDeadline](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/x/mongo/driver/topology/connection.go#L56-L57) 配置来控制何时回收（即真正断开 TCP 连接）。当连接被使用完毕，并 CheckIn 回到连接池时，会**调用 [bumpIdleDeadline](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/x/mongo/driver/topology/connection.go#L517) 根据最新时间和 idleTimeout 刷新其 idleDeadline.**      
在 driver 代码中有多处会检测连接的 idleDeadline，包括 CheckOut/CheckIn 以及后台流程（[maintain协程](https://github.com/mongodb/mongo-go-driver/blob/release/1.10/x/mongo/driver/topology/pool.go#L877)，默认 10 秒检查一轮）。如果连接长期没有被使用，则会被踢出连接池并关闭 TCP 连接。最终达到连接池自动向下调整的目的。

默认 idleTimeout 为 [0](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/mongo/options/clientoptions.go#L579-L585)，可以通过 clientOptions 或者在 URI 中通过 maxIdleTimeMS 设置。   

除了连接长时间未使用会导致回收释放之外，如果连接在使用的过程中遇到了网络错误，也会被回收。     


### 3.1.3 负载均衡    
正如我们在介绍 readPreference 章节中的描述，MongoClient 可以将读请求随机打散到多个 secondary 节点或者 mongos 节点来实现负载均衡。      
但是现实情况下，一次查询操作可能涉及到多个连续的请求，比较典型的情况有：    
- 批量扫描：一般对应 1 次 find 命令和后续的多次 getmore 命令。
- 事务：一般对应多次读写操作和事务提交操作。

如果一个查询任务对应多次相关联的“子”请求，则不能每次执行子请求时随机选择对应的 host 和 连接。

**批量扫描问题**        

当 MongoDB 无法将 find 结果一次性返回时，会先返回第一批数据 + cursorID, 客户端通过这个 cursorID 不断 getMore 迭代剩余的数据。所以一次批量扫描请求可能会对应 1 次 find 请求和多次 getMore 请求，并通过 cursorID 关联。 每个 mongos 节点在内存中维护了一个全局的 [ClusterCursorManager](https://github.com/mongodb/mongo/blob/r4.2.11/src/mongo/s/query/cluster_cursor_manager.h#L72)，里面通过 [HashMap](https://github.com/mongodb/mongo/blob/r4.2.11/src/mongo/s/query/cluster_cursor_manager.h#L721) 维护了 cursorID 和 ClusterClientCusor 的映射关系。    
其中 cursorID 是一个 int64 的随机数，ClusterClientCursor 则维护了请求的执行计划，当前状态等信息。 如果查询结果不能一次性返回（比如超过了 16MB 限制），则会生成一个非 0 的 cursorID，并将这个 ID 和 ClusterClientCusor 本身[注册到 ClusterCursorManager 中](https://github.com/mongodb/mongo/blob/r4.2.11/src/mongo/s/query/cluster_cursor_manager.cpp#L263-L328)。     
如果客户端需要后续的结果，可以携带前面返回的 cursorID 进行 getMore 请求，mongos 会[找到之前缓存的 ClusterClientCusor](https://github.com/mongodb/mongo/blob/r4.2.11/src/mongo/s/query/cluster_cursor_manager.cpp#L330-L383), 继续执行查询计划并返回结果。 ID 和 cursor 的信息独立存在于各个 mongos 节点中，因此必须要保证 find 及其相关联的 getMore 请求必须发往同一个 mongos 节点。 如下图所示，如果 getMore 请求发给了其他的 mongos 节点，会因为找不到 cursor 返回 CursorNotFound 错误。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/da9366a1-8692-4d98-a1c0-06cd26c58a95" width=600>
</p>

**事务问题**    

MongoDB 在 4.2 版本支持了分布式事务，用户可以连接 mongos 节点发起事务操作。 在 startTransaction 和 commitTransaction/abortTransaction 之间可以执行多次[读写操作](https://www.mongodb.com/docs/manual/core/transactions-operations/)，mongos 在内存中记录了事务中每次请求携带的 logicalSessionId 和 txnId 等元数据来维护上下文关系。因此，MongoDB 的设计决定了需要保证事务中的每个操作都发到同一个 mongos 上执行：     

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/6d51d002-86b4-4b10-bff1-89e442dd3a3b" width=900>
</p>

**Driver 的解决方案**      

如果客户端连接了多个 Host，则需要在 Driver 侧进行进行负载均衡。并且对于存在上下文关联的请求，需要绑定 Server (pinnedServer).    

1. **特殊请求**（mongos 侧的 getMore 和事务）    
在生成 BatchCursor 时，会[将第一次请求的 Server 绑定起来](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/x/mongo/driver/batch_cursor.go#L77)；同样，在事务开始阶段会[进行 Server 绑定](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/x/mongo/driver/session/client_session.go#L507-L512)；
后续的 getMore 请求会采用前面绑定的 Server; 同样，在事务的后续命令（insert，findAndModify 等）执行阶段会 SelectServer，[优先使用 pinnedServer](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/mongo/collection.go#L1859-L1876).    
2. **普通请求**     
对于不存在上下文关联的请求，尽量打散在多个 mongos 节点。
对于读请求，默认的 Server 选择逻辑是 ReadPrefSelector + LatencySelector.     
[ReadPrefSelector](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/mongo/description/server_selector.go#L131-L172): 根据 readPreference 来选择节点。如果连的是 mongos ，则对客户端来说各个节点没有区别（将 readPreference 的选择逻辑交给 mongos 执行）     
[LatencySelector](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/mongo/description/server_selector.go#L62-L110)： 根据延迟进行选择，先确定最近节点的网络距离 t，然后随机选择 [t, t+15ms] 内的节点，这也是 MongoDB 就近接入的原理
对于写请求，默认的 Server 选择逻辑是 WriteSelector + LatencySelector.     
[WriteSelector](https://github.com/mongodb/mongo-go-driver/blob/v1.10.0/mongo/description/server_selector.go#L112-L129): 选择所有能执行写操作的节点，包括 mongos， Primary， Standalone.


## 3.2 内部连接池：mongos 到 mongod
如果使用了 MongoDB 分片集群，那 mongos 到 mongod 的内部连接池控制也同等重要。如果控制不好，也会出现服务抖动。       
内部连接池的原理，和上一小节介绍的外部连接池类似。以 5.0 版本为例，提供的配置参数如下：    

|参数|作用|默认值|运行时配置|启动时配置|
|:--|:--|:--|:--|:--|
|[ShardingTaskExecutorPoolHostTimeoutMS](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolHostTimeoutMS)|如果到某个 host 的连接池一段时间没使用，则将到这个 host 的整个连接池全部销毁|300000 (5分钟)|支持|支持|
|[ShardingTaskExecutorPoolMaxConnecting](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolMaxConnecting)|新建连接的并发数（以taskExecutor 为单位）|2|支持|支持|
|[ShardingTaskExecutorPoolMaxSize](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolMaxSize)|每个taskExecutor到每个mongod 节点的连接池上限|UINT64_MAX|支持|支持|
|[ShardingTaskExecutorPoolMinSize](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolMinSize)|每个taskExecutor到每个mongod 节点的连接池下限|1|支持|支持|
|[ShardingTaskExecutorPoolMaxSizeForConfigServers](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolMaxSizeForConfigServers)|每个taskExecutor到每个 configSvr 节点的连接池上限|-1,表示复用ShardingTaskExecutorPoolMaxSize 的设置|支持|支持|
|[ShardingTaskExecutorPoolMinSizeForConfigServers](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolMinSizeForConfigServers)|每个taskExecutor到每个 configSvr 节点的连接池下限|-1,表示复用ShardingTaskExecutorPoolMinSize 的设置|支持|支持|
|[ShardingTaskExecutorPoolRefreshRequirementMS](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolRefreshRequirementMS)|连接长期未使用时，需要先发心跳（[isMaster命令](https://github.com/mongodb/mongo/blob/r5.0.26/src/mongo/executor/connection_pool_tl.cpp#L416)）刷新一下，然后真正投入使用|60000 (1分钟)|支持|支持|
|[ShardingTaskExecutorPoolRefreshTimeoutMS](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolRefreshTimeoutMS)|连接刷新命令的超时时间|20000 (20秒)|支持|支持|
|[ShardingTaskExecutorPoolReplicaSetMatching](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.ShardingTaskExecutorPoolReplicaSetMatching)|连接池保持下限的策略|automatic|支持|支持|
|[taskExecutorPoolSize](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.taskExecutorPoolSize)|taskExecutor 连接池的个数|1|支持|支持|

有 2 个参数需要特别进行说明：    
- **ShardingTaskExecutorPoolReplicaSetMatching**： 很多场景下， primary 和 secondary 节点的读写请求压力是不一样的。如果 primary 的读写请求多，则 mongos 到 primary 节点的连接也多（比如 100个），而到 secondary 节点的连接较少（比如 1个）。如果出现了主从切换，secondary 变成了新 primary，则 mongos 到新 primary 的连接是不够用的，需要迅速创建新连接。前面介绍了，创建连接是比较耗时耗资源的，而且存在 ShardingTaskExecutorPoolMaxConnecting 并发控制。因此需要较长的时间将 mongos 到新 primary 的连接池从 1 涨到 100。而这段时间会存在连接不够用的情况，造成请求排队甚至超时。    
因此，MongoDB 提供了 automatic、matchPrimaryNode（以 primary 作为参考）、matchBusiestNode（以最忙的节点作为参考）、disabled 几种配置。比如在 mongos 上默认是 automatic，则等同  matchPrimaryNode 模式，mongos 到 secondary 连接池的下限会保持和 primary 的连接池同等水平，这样即使发生了切主，也不会由于连接池的创建造成请求毛刺和超时。    
- **taskExecutorPoolSize**： 每个 mongos 节点的 taskExecutor 连接池个数。在较低版本（比如 [3.6](https://www.mongodb.com/docs/v3.6/reference/parameters/#param.taskExecutorPoolSize) ）的 MongoDB，这个参数的默认值是 CPU 核数，也就是 mongos 的每个 CPU 都维护了一个连接池。而在高版本之后，这个值默认是 1，而且不能在 Linux 操作系统下调整这项设置：
  ```
  When running MongoDB on Linux, you cannot modify the taskExecutorPoolSize from the default value of 1. You may modify this parameter when running MongoDB on Windows or macOS.
  ```

**连接数估算**    

明确了内部连接池的原理和参数配置之后，我们可以评估 mongos 和 mongod 节点的连接情况（注意这里占大头的用户连接池，管理连接和用户连接是分开的）。    

每个 mongos 的**出连接下限**：    
```
taskExecutor个数（默认为1） *  mongod 节点个数（主从节点都算） *   ShardingTaskExecutorPoolMinSize
```
每个 mongos 的**出连接上限**：      
```
taskExecutor个数（默认为1） *  mongod 节点个数（主从节点都算） *   ShardingTaskExecutorPoolMaxSize
```

每个 mongod 节点的 **入连接下限**：    
```
taskExecutor个数（默认为1） *  mongos 节点个数  *   ShardingTaskExecutorPoolMinSize
```
每个 mongod 节点的 **入连接上限**：     
```
taskExecutor个数（默认为1） *  mongos 节点个数  *   ShardingTaskExecutorPoolMaxSize
```

如果出现 mongos 节点的增删，则 mongod 的入连接数会出现相应的增减。同理，如果出现 mongod 节点的增删，则 mongos 的出连接数也会对应的增减。    

因此，在对 mongos 和 mongod 进行横向扩缩容时，也需要进行对应的连接池参数配置，确保连接池处于可控范围。
> 每个节点能接受多少连接？有一些经验参考：    
>一般来说，每个 mongos 和 mongod 节点的连接数不要上万。对于硬件配置比较高的集群，可以将每个节点的入连接数控制在几千，对于硬件配置一般的集群，可以控制在几百。    
>可以使用连接数乘以每秒处理的请求数，然后和业务的峰值 QPS 进行对比。并预留 1~2 倍的 buffer 即可。


# 4. 总结

1. MongoDB 中的连接创建和销毁是比较耗时，耗资源的操作。官方使用连接池来避免频繁的连接创建。      
2. 在实际使用中，需要结合客户端个数、mongos 节点数、 mongod 节点数、各自的资源情况以及请求数，对连接池进行合理的配置。确保每个节点的入连接和出连接处于合理的范围。

# 5. 参考资料
1. https://zhuanlan.zhihu.com/p/561776899
2. https://github.com/mongodb/specifications/blob/master/source/auth/auth
3. https://github.com/mongodb/mongo-go-driver/tree/release/1.10
4. https://www.mongodb.com/docs/v5.0/reference/parameters/#sharding-parameters
5. https://github.com/mongodb/mongo/blob/r5.0.26/src/mongo



