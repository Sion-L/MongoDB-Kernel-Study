# 1. 导语
近年来，互联网数据出现了爆炸式增长，单机数据库在容量和性能上往往难以满足各个互联网服务的需求。在此背景下，很多数据库通过支持横向扩展能力来满足业务需求，通过分片的方式将数据打散到多台服务器上，使得整体性能和容量得到成倍提升。    
MongoDB 从最初设计上考虑到了海量数据的需求，因此原生就支持分片集群。本文将对 MongoDB 的分片原理进行分析，阐述分片架构和实现原理，并说明使用分片的注意事项。    

# 2. MongoDB 分片架构
MongoDB 的分片架构如下，包含 3 种角色：    
- mongos： 作为分片集群的接入层，接受用户的读写请求，并根据路由转发到底层 shard.  1 个分片集群可以有 1 个或者多个  mongos 节点。   
- shard：1 个 shard 存储了一部分分片表的数据，1 个分片集群可以有 1 个或者多个 shard. 每个 shard 有多个 mongod 节点组成的副本集构成。   
- config servers: 存储配置元数据以及分片表的路由等信息。 1 个分片集群有 1 个 config servers, config servers 是由多个 mongod 节点组成的副本集。   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/4c481478-4b4a-49c8-a54d-a3f3c2012498" width=400>
</p>

# 3. 从 0 开始探究分片原理
分片无非是选好 shardKey 之后，按照某种规则将数据均匀分布到多个shard 中，保证每个 shard 的数据量和请求量基本均衡。因此，本质上来说要解决 2 个核心问题：   
1. 如何选择 shardKey？   
2. 如何选取合适的分片算法？   

## 3.1 ShardKey 选取
分片键的选取需要保障数据分布足够离散，每个分片的存储容量均衡，并能够将数据操作均匀分发到集群中的所有分片中。    
如果分片键选取不佳，可能会导致各个分片负载不均，出现 jumbo chunk 导致无法分裂等问题。而且分片键一旦确定之后，不能在运行过程中进行变更，需要按新分片键创建新表后重新导入数据（4.4及以上版本支持 shardKey 变更，但是并不建议做这种在线操作）。    
一般选取分片键时，会考虑以下因素：    
1. **分片键的区分度**    
分片键的取值基数决定了最多能包含多少个 chunk，如果取值基数太小，则会导致 chunk 数量很低，可能会有负载不均的问题。     
比如按照“性别”来设置分片键就不是合理的选择，因为 “性别” 只有 “男”、“女” 2 种取值，这样最多 2 个 chunk.   
2. **分片键的取值分布是否均匀**    
如果分片键的取值存在热点，也可能导致分片负载不均。比如以 “国家” 作为片建，会由于各个国家之间人口的差异出现负载不均，人口多的国家存储量大请求多，而人口少的国家存储量小请求少。    
对于这种场景，可以考虑使用复合键来作为分片键，降低出现热点的概率。    
3. **是否会按照分片键单调写入**   
如果业务按照分片键递增或者递减的趋势进行读写，则可能在某一时刻请求集中在某个 chunk 上，无法发挥集群多个分片的优势。    
比如对于存储日志的场景，如果按照日志创建时间进行范围分片，则在某一时间内一直对同一个 chunk 进行写入。对于这种场景，可以考虑复合分片键或者哈希分片来避免这种情况。    
4. **查询模型是否包含分片键**    
在确定分片键后，需要考虑业务的查询请求中是否包含有分片键。mongos 节点会根据查询请求中的分片键将请求转发到对应的 shard server 中。如果查询请求中不包含分片键，则 mongos 节点会将请求广播到后端的所有  shard server，进行 scatter/gather 模式的查询，此时查询性能会有所下降。

## 3.2 分片算法
**传统的 Hash**    
提到将数据打散，往往第一个想到的就是 hash. 比如有 N 个 shard，可以通过 Hash(shardKey) mod N 的方式，将每条数据按照 shardKey 映射到 编号为 [0, N-1] 的 shard 中。    
这种简单 hash 方式虽然将数据打散的非常均匀，但是无法处理增加和删除 shard 的问题，一旦出现 shard 变更，所有数据都要重新分布。    
在实际的业务场景中，增加和删除 shard 是非常常见的操作。因此，一般很少直接使用简单 hash 的方式。    

**传统的一致性 Hash**    
一致性 hash 算法能够解决简单 hash 在 shard 数量变动时，所有数据需要重新分布的问题。    
其基本思路时将hash 后的线性空间当作一个环，然后根据 shard 个数在环上分配相应的 node，每条数据根据 shardKey 计算出的 hash 值找到距离最近的弄得，而且 node 值不大于 hash 值。    
如果新增或者删除 shard，会导致 node 发生变化，但是只有部分涉及到的数据需要重新分布。相比简单 hash 方式，对系统的影响相对较小。    
另外，通过引入虚拟 node，可以将重新分布时的数据迁移操作分散到多个节点，并且尽量保证均匀。    
不过尽管如此，在增加和删除 node 时，仍然会出现数据不均衡的情况。    

**MongoDB 的 Range 分片**    
MongoDB 引入了 chunk 的概念，作为分片、路由以及数据迁移的单元。 1 个 chunk 可以看作是分片表中 shardKey 连续排列的数据集合，是一个逻辑概念，各个 chunk 在底层存储引擎中并不是分开存储。 1 个 chunk 默认是 64MB，用户也可以根据实际需要自行调整。1个 chunk 中的数据数据总大小超过 64MB 会自动进行分裂，如果各个 shard 之间 chunk 的个数不均匀，会自动触发负载均衡操作。    
以 shardKey 是 “x” 为例，其取值空间由 1 个或者多个 chunk 组成：    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/458fc089-8fbd-403e-9606-fa2a749421e8" width=600>
</p>

需要注意的是，1个 chunk 只能分布在某1个 shard中。如果读请求是一个小范围的查询，则 mongos 只需要将请求转发到对应的 shard 上，这样能充分利用 range 分片的优势。如果涉及多个 shard， 则 mongos 需要使用 scatter/gather 方式和多个 shard 交互。    

**MongoDB 的 Hash 分片**    
如果用户的写入操作存在按 shardKey  的单调性，比如以时间戳为 shardKey并按照时间顺序写入。此时会导致写入操作始终集中在 shardKey 最大的 chunk 上，压力始终在某一个 shard 中，而且会触发大量的分裂和迁移操作。对于 range 分片来说简直就是灾难。    
MongoDB 的 hash 分片方式能够很好的解决这个问题。和 range 分片不同，shardKey 首先会经过 hash函数（md5）计算出一个整数类型的 hash 值，然后根据这个 hash 值分散到对应的 chunk 和 shard 上。如下图所示。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/73f53354-1f61-4aa1-b095-0e59fc89dd68" width=600>
</p>

Hash 函数对用户来说是不感知的，而且代码中固定为 md5 算法。     
MongoDB 的 hash 分片算法和一致性 hash 算法有点类似，但是不同之处在于 chunk 分裂机制和自动负载均衡机制使得数据分布理论上更加均匀。   

**MongoDB 的 Zone 分区**    
MongoDB 可以根据 shardKey 对 shard 设置 zone 属性，比如对于多地域部署场景，可以将华南机房的 shard 设置为 “South” zone，将华北机房的 shard 设置为 “North” zone，通过这种方式可以将对应的数据限定在特定的 shard 中存储。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/06fc2a61-9db0-4a43-bf53-cf5125c1dbff" width=600>
</p>

Zone 必须按照 shardKey 或者 shardKey 的前缀进行设置。属于某个 zone 的数据，在数据迁移时只会在同一个 zone 的多个shard 之间移动。    

# 4. 路由管理


