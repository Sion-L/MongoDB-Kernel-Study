# 1. 设计模式

## 1.1 Decorable机制

> 这里声明一下，MongoDB中的Decorable机制跟OOP中的装饰器模式还是有区别的。装饰器模式的一个要点是类被装饰后暴露的接口不变，利用多态的特点增加代码逻辑。


在了解Decorable机制之前，我们先考虑以下两个问题：
1. MongoDB中有许多组件，比如Metrics、VectorClock、Top。这些组件一般在某个层次是唯一的，比如VectorClock是全局唯一的，ReadConcern、WriteConcern是每个请求唯一的。
2. 组件之间的调用关系非常复杂。

这种情况下，我们一般会使用单例模式，提供一个静态方法获取这个全局唯一的实例。但是单例模式的缺点很明显：
- 单例类需要自己控制自己的生命周期，违反了单一职责原则
- 代码之间的耦合变深，很难通过fake或者mock的方法进行test
- 相比于显式传递参数，给代码增加了隐形依赖

有一个更致命的缺点是：当一个组件不是全局唯一不太好办。而且单例模式加起来容易，去掉就难了。

所以MongoDB代码中提供了ServiceContext、OperationContext等概念，并且利用Decorable机制，来管理各个“挂件”。从上层代码来看使用方式如下，以ServiceContext为例：
```cpp
// 声明AuthorizationManager附属于ServiceContext类
const auto getAuthorizationManager =
    ServiceContext::declareDecoration<std::unique_ptr<AuthorizationManager>>();

class AuthorizationManager {
public:
    // 定义一个静态方法，用来获取service中的AuthorizationManager实例
    static AuthorizationManager* get(ServiceContext* service){
        return getAuthorizationManager(service).get();
    };
}

// 通过静态方法，拿到serviceContext中的authzManager
AuthorizationManager* authzManager = AuthorizationManager::get(opCtx->getServiceContext());
```


### 内部实现

一对多的关系下，一个比较简单的做法是使用composition（组合）来管理它们之间的关系。比如说逻辑时钟、认证管理、replication coordinator这些模块都变成ServiceContext的一个成员变量或方法。缺点就是每增删一个模块都要改动ServiceContext的代码，非常不方便。

那么Decorable是如何做的呢？它的定义如下，是一种非典型的[CRTP](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern)的应用：
```cpp
class ServiceContext final : public Decorable<ServiceContext> {
}
```

重点看这个Decorable类，它有一个嵌套类Decoration，两者是从属的关系——一个Decorable可以有若干个Decoration：
```cpp
template <typename D>     // D是被“装饰”的对象，例如ServiceContext
class Decorable {         // 继承Decorable，使D变得可被装饰
public:
    template <typename T>     // T是“装饰品”，例如ServiceContext的认证管理组件
    class Decoration {       
    public:
        // 重载了双括号，用来从实例D中取出实例T，比如从一个ServiceContext实例中取出它的AuthorizationManager
        T& operator()(D& d) const {
            return static_cast<Decorable&>(d)._decorations.getDecoration(this->_raw);
        }

		// 获取实例T所属的D实例
        D* owner(T* const t) const {
            return static_cast<D*>(getOwnerImpl(t));
        }

    private:
        // 这里利用了对象的内存布局手动获取Decorable实例
        const Decorable* getOwnerImpl(const T* const t) const {
            return *reinterpret_cast<const Decorable* const*>(
                reinterpret_cast<const unsigned char* const>(t) - _raw._raw._index);
        }

        friend class Decorable;

        explicit Decoration(
            typename DecorationContainer<D>::template DecorationDescriptorWithType<T> raw)
            : _raw(std::move(raw)) {}

        typename DecorationContainer<D>::template DecorationDescriptorWithType<T> _raw;         // 此装饰品的位置/描述符
    };

    // 唯一的公开方法，用来为此Decorable声明/申请一个装饰品
    template <typename T>
    static Decoration<T> declareDecoration() {
        return Decoration<T>(getRegistry()->template declareDecoration<T>());
    }

protected:
    Decorable() : _decorations(this, getRegistry()) {}
    ~Decorable() = default;

private:
	// 根据模板类型D的不同，会生成不同的Decorable<D>类和getRegistry方法，每个getRegistry方法都会有一个不同类型的局部静态变量
    static DecorationRegistry<D>* getRegistry() {
        static DecorationRegistry<D>* theRegistry = new DecorationRegistry<D>();
        return theRegistry;
    }

    DecorationContainer<D> _decorations;    // 唯一的成员变量
};
```

declareDecoration静态方法是用来添加一个Decoration的外部接口，比如往ServiceContext中添加一个AuthorizationManager组件——`ServiceContext::declareDecoration<std::unique_ptr<AuthorizationManager>>();`。这样，之后new出来的ServiceContext实例都会有各自的AuthorizationManager。

以基础类为 D，需要给其装饰 T1 和 T2 为例。**核心思路就是给每个 D 对象分配一段内存区域存放 T1 和 T2，并且负责管理 T1 和 T2 的生命周期。**   

主要涉及到以下数据结构：    
- **Decorable**：定义在 [util/decorable.h](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/util/decorable.h) 文件中。在定义 D 时，需要继承 public Decorable<D>，表示 D 是一个可装饰的类。代码的其他位置就可以使用 D::declareDecoration<T1> 和 D::declareDecoration<T2> 声明 T1 和 T2 是 D 的 挂件。每次调用 declareDecoration 函数会返回一个 Decoration 对象，这个对象中会记录 T1/T2 装饰在 D 的哪个位置（可以理解为一段堆内存中的偏移），通过不同的 Decoration 对象可以快速找到具体的挂件。                  
- **DecorationRegistry**：定义在 [util/decorable_registry.h](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/util/decoration_registry.h) 文件中。当 D 通过前面的步骤声明自己为 Decorable 时，就会实例化一个 DecorationRegistry 对象（static 的），这个对象会完成装饰器模式的主要控制逻辑：比如 declareDecoration 时，会根据 T1/T2 的大小进行对齐后，统计堆 buffer 的总大小，并给 T1/T2 分配具体的位置；在 D 进行构造时，会在指定的内存位置构造 T1/T2 对象；在 D 进行析构时，会调用 T1/T2 的析构函数。      
- **DecorationContainer**：定义在 [util/decorable_container.h](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/util/decoration_container.h) 文件中。每个 Decorable 对象都会实例化一个 DecorationContainer 对象，这个对象中会保存 D 的所有挂件。DecorationContainer 本质上是一段堆内存（std::unique_ptr<unsigned char[]>），内存长度由 DecorationRegistry 提供，每当构建 D 对象时，会根据 DecorationRegistry 提供的长度信息进行内存分配。      


### 示例

以 [util/decorable_test.cpp](https://github.com/mongodb/mongo/blob/r4.2.25/src/mongo/util/decorable_test.cpp) 中的单元测试为例。     
```cpp
// 定义 主类/被装饰类 是可被装饰的
class MyDecorable : public Decorable<MyDecorable> {};

// 定义 挂件
static int numConstructedAs;   // A 被构造的总数
static int numDestructedAs;    // A 被析构的总数
class A {
public:
    A() : value(0) {
        ++numConstructedAs;
    }
    ~A() {
        ++numDestructedAs;
    }
    int value;
};

// 具体的执行逻辑
TEST(DecorableTest, DecorableType) {
    const auto dd1 = MyDecorable::declareDecoration<A>();  // 声明 A 是 MyDecorable 的挂件，并返回 A 在堆内存上的位置
    const auto dd2 = MyDecorable::declareDecoration<A>();  // 再次声明 A 是 MyDecorable 的挂件。并返回第 2 个 A 在堆内存上的位置
    const auto dd3 = MyDecorable::declareDecoration<int>();  // 声明 int 是 MyDecorable 的挂件，并返回 int 在堆内存上的位置
    numConstructedAs = 0;
    numDestructedAs = 0;
    {
        MyDecorable decorable1;  // 构造 MyDecorable 对象
        ASSERT_EQ(2, numConstructedAs);  // 构造了 2 个 A (在内存中顺序排列)
        ASSERT_EQ(0, numDestructedAs);
        MyDecorable decorable2;  // 构造另一个 MyDecorable 对象
        ASSERT_EQ(4, numConstructedAs);  // 构造了 2 个 A (在 decorable2 d的内存中顺序排列)
        ASSERT_EQ(0, numDestructedAs);

        ASSERT_EQ(0, dd1(decorable1).value);  // decorable1 的 第 1 个 A 对象的 value
        ASSERT_EQ(0, dd2(decorable1).value);  // decorable1 的 第 2 个 A 对象的 value
        ASSERT_EQ(0, dd1(decorable2).value);  // decorable2 的 第 1 个 A 对象的 value
        ASSERT_EQ(0, dd2(decorable2).value);  // decorable2 的 第 2 个 A 对象的 value
        ASSERT_EQ(0, dd3(decorable2));  // decorable2 的 int 对象
        dd1(decorable1).value = 1;  // 给上述对象赋值 
        dd2(decorable1).value = 2;
        dd1(decorable2).value = 3;
        dd2(decorable2).value = 4;
        dd3(decorable2) = 5;
        ASSERT_EQ(1, dd1(decorable1).value);  // 再次确认赋值后的对象
        ASSERT_EQ(2, dd2(decorable1).value);
        ASSERT_EQ(3, dd1(decorable2).value);
        ASSERT_EQ(4, dd2(decorable2).value);
        ASSERT_EQ(5, dd3(decorable2));
    }
    ASSERT_EQ(4, numDestructedAs);  // decorable1 和 decorable2 都被析构，每个对象都析构 2 个 A
}
```

### 构造和析构流程
整体流程如下图所示：
1. 在初始化阶段，使用 declareDecoration 函数声明装饰器，计算装饰器总内存大小，并确定每个装饰器在内存中的位置。
2. 在运行时阶段，在构造主类对象（被装饰的对象）时，会在堆上分配内存空间并在指定位置构造装饰器对象。在析构主对象时则会析构装饰器并释放内存。    

<p align="center">
  <img src="https://github.com/user-attachments/assets/ce2dd6fd-9b9f-40cb-859d-61d71855e38a" width=800>
</p>


## 1.2 装饰器模式

### 什么是装饰器

在软件设计中，装饰器模式允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。     
这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。这种模式通常用来为现有对象添加装饰，但是它不能用于已将对象作为参数传递给构造函数的情形。    

比如在 MongoDB 中定义了 Client 类描述客户端信息，包括 TCP 会话（源端地址，目标端地址）、是否为内部连接，连接 id 等。    
但是除了上述基本信息之外，Client 的认证信息（AuthenticationSession）、权限信息（AuthorizationSession）等也是和自身一一对应的。

常见的解决方法有：    
1. 直接在 Client 中添加认证信息、权限信息等字段，但是这样会破坏类的封装性。    
2. 使用继承的方式，在 Client 基础上添加认证信息、权限信息等类。比如在 Client 基础上添加 AuthenticationClient 等类。 但是如果其他基础类也是用了 AuthenticationSession 信息，也要定义自己的派生类，这样会带来大量的代码冗余。   

装饰器模式可以解决上述问题，通过装饰器模式，可以将认证信息、权限信息等独立出来，然后通过 Client 类的装饰器字段指向认证信息、权限信息等对象，这样既不会破坏类的封装性，代码上也会非常简洁。     
   

## 1.3 单例模式


## 1.4 观察者模式


# 2. 算法和数据结构

## 2.1 WriteConflict 的运用
在前面章节中，提到了 WT 引擎采用乐观锁机制实现高并发。如果 2 个事务同时修改了同 1 条文档，由于 first-update-win 机制，只有一个事务会提交成功，另一个事务会由于 writeConflict 失败并依赖上层的重试。     

**如果利用事务的原子性，再结合 writeConflict 的冲突检测能力，就能在上层应用中实现乐观锁。**        

上述 “乐观锁” 机制在 MongoServer 层的索引中得以广泛使用，下面列举 2 个例子。

### 索引的唯一性检测
在 1.2 章节中，提到了 MongoDB 中索引的格式为： {Key: IndexKey+RecordId, Value: typeBits}, 其中 IndexKey 是索引的 key，RecordId 是文档的唯一id（一般为自增 int64）， typeBits 用于将 KeyString 格式的字符串转回原始类型。     
索引的唯一性检测需要通过 Key 的前缀来判断。    

假设我们对 "a" 字段创建了唯一索引，然后同时插入了 2 条 "a" 字段为 1 的文档：    
- 事务 1：**检测**没有前缀为 "a=1" 的索引，于是往索引中插入记录 {Key: "a=1" + record1, Values: typeBits}.
- 事务 2：**检测**没有前缀为 "a=1" 的索引，于是往索引中插入记录 {Key: "a=1" + record2, Values: typeBits}.

此时，由于 2 个事务都插入成功，导致索引中出现了重复的记录。    

MongoDB 通过构造写冲突的方式来解决上述问题，在上述流程中增加了**额外步骤**：    

- 事务 1：插入 {Key:"a=1", Value: typeBits}, 然后删除之。
- 事务 2：插入 {Key:"a=1", Value: typeBits}, 然后删除之。

通过上述处理，2 个事务出现了写冲突，只有一个事务能够插入成功。而另外一个事务失败后进入重试，然后检测到有 "a=1" 的索引前缀，并给客户端报错。

### 后台建索引的冲突检测
在 MongoDB 早期版本有 background 建索引功能（4.2 之后逐步被更优的 hybrid 模式取代）。      
在 [MongoDB 索引使用总结](https://mp.weixin.qq.com/s/g0mNt73nnFS_xyVGuAPe-g) 中提到一个后台建索引时处理冲突的 case。核心思想是主动构造写冲突，避免索引和表的数据不一致。这里不再赘述。    

## 2.2 为多核架构设计的 LRU 

>完整内容参考前面 [5.4 章节](https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/blob/zhenyi-dev/5.%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/4.Eviction%3A%E4%B8%8D%E5%8F%AF%E5%BF%BD%E8%A7%86%E7%9A%84%E6%80%A7%E8%83%BD%E6%9D%80%E6%89%8B.md)介绍 WT 引擎 evict 流程的描述。

比较经典的 LRU 算法是 HashMap + List 来实现。但是在多核场景下，锁竞争会成为性能瓶颈。     
业界的一种常见的解决方案（比如 RocksDB），就是对将数据划分为多个 LRU 链表来管理，降低锁竞争，但是会牺牲一定的 LRU 算法的准确性。

WT 直接摒弃了传统的 LRU 算法，并没有使用 list 去实时维护 cache 中的访问情况，当然也就没有对应的哈希 map。    
WT 中的每个内存 page 有一个 int64 类型的 read_gen 来标记访问情况。笼统来说，这个值越高就表示最近更频繁被访问，就越不会淘汰。可以参考 [__wt_page](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/btmem.h#L704) 的定义：    
```
struct __wt_page {
    ...

    /*
    * The page's read generation acts as an LRU value for each page in the
    * tree; it is used by the eviction server thread to select pages to be
    * discarded from the in-memory tree.
    *
    * The read generation is a 64-bit value, if incremented frequently, a
    * 32-bit value could overflow.
    *
    * The read generation is a piece of shared memory potentially read
    * by many threads.  We don't want to update page read generations for
    * in-cache workloads and suffer the cache misses, so we don't simply
    * increment the read generation value on every access.  Instead, the
    * read generation is incremented by the eviction server each time it
    * becomes active.  To avoid incrementing a page's read generation too
    * frequently, it is set to a future point.
    *
    * Because low read generation values have special meaning, and there
    * are places where we manipulate the value, use an initial value well
    * outside of the special range.
    */
    uint64_t read_gen;

    ...
}
```

代码看到这里，似乎能猜到 WT 中 LRU 的实现思路了：   
1. ~~每次访问到 page 时（无论读还是写），将 read_gen 设置为当前时间。~~
2. ~~负责淘汰的 eviction 线程扫描所有 page 的 read_gen，做一次排序，然后将末尾的 page 淘汰。~~

但是继续研究代码，会发现上述说法一条都不正确。。

首先要说明的是并发修改 read_gen 的问题。    
我们知道，在一个高并发的数据库中，内存 page 是极有可能被**多个线程并发访问**的，特别是 internal page，尤其是 root page。如果多个 CPU 同时读取并修改 root page 的 read_gen，很有可能**由于 cacheline 失效的问题导致性能受限**。   
关于为什么多核场景下，更新同一个变量会有性能下降问题。可以参考[多线程的性能杀手](https://zhuanlan.zhihu.com/p/633269064) 这篇博客中关于 cache 乒乓问题的描述。

再说说全局排序问题。     
对于 list 实现的 LRU 算法来说，每次 page 的访问都会对应调整 list 中的排序，相当于将 list 的排序分摊到了每次的 page 访问请求中。而 WT 的 LRU 算法中并没有 list，如果将排序操作都放在 eviction 后台线程扫描的时候来做，耗时可能会很大。想象一个 cacheSize 有 100GB，如果扫描这 100GB 再排序，将会花费多长时间呢？有没有可能还没完成排序，内存就已经爆了？    

**WT 的解决方案**

WT 通过巧妙的设计来解决并发修改 read_gen 的性能问题。个人认为有 2 个关键点：    
1. 整个 cache 维护了一个全局的 read_gen，在 eviction 流程每次启动 1 轮扫描时，会将这个值[加 1](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.i#L34)。
2. 用户线程在获取到 page 的 hazard_pointer 进行访问时，将该 page 的 read_gen 提前[设置成一个较大值](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/include/cache.i#L44)，即 page->read_gen = cache->read_gen + 100。后续其他线程也获得 hazard_pointer 时，会判断 read_gen 是否已经设置为一个较大值，如果是则不会更新。这种方式将 read_gen 的更新降低了很多个数量级。

WT 的 LRU 算法能够避免多个 CPU 频繁更新共享内存导致 cacheline 失效的问题，提升整体性能，但与此同时也**牺牲了一定的 LRU 准确性**：      
- cache->read_gen 全局变量会随着每轮扫描递增，这个值本身就具有**时效性**，因此可以作为 LRU 的参考。根据实测结果，每轮扫描的间隔时间在空闲时为 200ms 左右，在忙时为几毫秒或者几十毫秒都有可能（上述数据，是在腾讯云2C2G规格的云主机，自建 4.2.24 版本副本集，使用 YCSB 压测的结果，仅做参考，下同）。因此，cache->read_gen 就像逻辑时钟一样持续推进。
- 如果 page 被访问，设置 page->read_gen = cache->read_gen+100。如果 page 没有被访问，则不会增加 page->read_gen。因此，使用 page->read_gen 作为 LRU 打分的依据也是合适的。但是如果在同一个 evict 扫描周期内，先后扫描了 page A 和 B，2 者的 read_gen 相同，因此使用 read_gen 来作为 LRU 打分也会存在一定的误差。

如下图所示，cache->read_gen 推进到 2000，每个 page 的 read_gen 都是 1000（这些 page 都有一段时间没有访问了）。随后， A、B、C、D、E 都被正常访问，因此 read_gen 都设置为 2100。F 没有被访问，read_gen 保持在 1000。 G 被访问并且被删除了非常多的数据，read_gen 直接被设置为 1（优先淘汰）。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/810f83d0-24b0-451a-b701-d908f19f7c16" width=800>
</p>

另外，为了解决全局排序耗时太长的问题，WT 将全局排序转换为局部排序。不寻求 LRU 的全局最优，而是需求局部最优，这样可以快速扫描完成并立即开展 page 的淘汰工作。不过这种方式也会带来一些问题，比如是否会导致某些值得 evict 的 page 迟迟扫描不到呢？WT 在每轮扫描结果时，需要记录当时的扫描位置，避免重复扫描。      
那么，每次扫描多少个 page 合适呢？如果扫描的多了，server 和 worker 的流水线不是很“顺畅”，影响性能；如果扫描少了，得到的是个非常局部的最优解，影响 cache 命中率，也会影响性能。     

根据实测结果，每轮扫描的 page 从几百到几千不等，dirty 高的场景下可能到几万。具体策略可以参考代码 [__evict_walk_tree](https://github.com/mongodb/mongo/blob/r4.2.24/src/third_party/wiredtiger/src/evict/evict_lru.c#L1656) 的实现。    
扫描后，会将多达 300 个 page 放到 queue 中，然后排序后选取几十到几百个 page 执行 evict 操作。     
>特别说明，上述数据也是自建 4.2.24 版本副本集，使用 YCSB 进行 1:1 测试的结果。数据列出来主要是为了比较量化的理解执行流程。不同版本和硬件环境下的数据可能会有些差异。    

## 2.3 Hazard Pointer: 为多核架构设计的读写锁
>完整内容参考 4.1 章节对 WT 锁机制的介绍。

对于普通的读写操作，可以共享一个 page 的访问。对于 evict 流程，涉及到 page 的逐出、分裂等流程，需要采用独占 page。    

理论上来说，可以参考 WT_CONNECTION 和 WT_DATA_HANDLE，给 page设计一个 RWLOCK 就可以搞定上述需求。      
但是需要注意的是，page 的访问非常频繁。对于一次查询请求，可能只会对 WT_CONNECTION 和 WT_DATA_HANDLE 加1 次或者若干次共享锁，但是对 page 的加锁次数可能有很多次。如果使用引用计数实现的 RWLOCK，会出现多个 CPU 频繁修改同一个内存值的情况，此时造成的 cpu cache miss 问题，会导致多核场景下的性能下降。    

因此，WT 使用了 hazard pointer （后文简称 HP）来作为 page 的读写锁，而不是 RWLOCK。在 HP 机制下，每个WT_SESSION（一般对应一个独立的线程）都维护着自己的 HP 数组，当需要使用共享方式访问某个 page 时，就在自己的 HP 数组中引用该 page 的指针 WT_REF。当 evict 流程需要独占式使用这个 page 时，需要保证没有 WT_SESSION 包含这个 page 的 HP。     
为了保证上述流程的安全性，还为每个 WT_REF 定义了多种使用状态，如下所示：     

```
#define WT_REF_DISK 0       /* Page is on disk */
#define WT_REF_DELETED 1    /* Page is on disk, but deleted */
#define WT_REF_LOCKED 2     /* Page locked for exclusive access */
#define WT_REF_MEM 3        /* Page is in cache and valid */
#define WT_REF_SPLIT 4      /* Parent page split (WT_REF dead) */
```

这里要说明的是 WT_REF_LOCKED 状态，在独占式访问时，需要将 WT_REF 设置为该状态。    
假如有 3 个线程（WT_SESSION）同时对一个 page 加锁，则流程如下（_前置条件：page在内存中，且没有被独占访问，状态为 WT_REF_MEM_）：    
|线程1（给 page 写数据，**共享**锁）|线程2（读 page 数据，**共享**锁）|EVICT_SERVER线程（逐出 page，**独占**锁|
|:--|:--|:--|
|1. 设置自己的 HP = WT_REF|1. 设置自己的 HP = WT_REF|1. 使用CAS将 WT_REF 的状态变更为LOCKED，保存之前的状态用于回滚。并先清理自己 session 的 hp 对 page 的引用。如果这一步出错，则操作回滚并返回报错|
|2. 检查 WT_REF 状态仍然是WT_REF_MEM（是否为 LOCKED）|2. 检查 WT_REF 状态仍然是WT_REF_MEM（是否为 LOCKED）|2. 检查**所有 WT_SESSION， 确保没有 HP 引用该 WT_REF**|
|3. 如果第 2 步确认不是 WT_REF_MEM，则回滚 HP=NULL, 加锁失败|3. 如果第 2 步确认不是 WT_REF_MEM，则回滚 HP=NULL, 加锁失败|3. 如果第 2 步检查失败，则加锁失败，并回滚 WT_REF 的状态|
|4. 如果第2 步是 WT_REF_MEM，则加锁成功|4. 如果第2 步是 WT_REF_MEM，则加锁成功|4. 如果第 2 步检查成功，则加锁成功|

需要注意的是，上述 3 个流程中的 第 1 步和第 2 步访问了不同的数据，是有可能乱序执行的。为了防止出现指令重排乱序执行，WT 代码中也实现了相应的 BARRIER 函数，底层会调用 mfence/lfence/sfence 内存屏障，来保证代码严格按照上表中的步骤去执行。    

在 page 使用完毕后，会有对应的 release 释放锁的流程，可以看做是 加锁的逆过程，这里就不再赘述。    

对于加共享锁的流程，可以参考 [__wt_page_in_func](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/btree/bt_read.c#L203) -> [__wt_hazard_set_func](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/support/hazard.c#L64) 的实现：    
```c
int __wt_hazard_set_func(WT_SESSION_IMPL *session, WT_REF *ref, bool *busyp, const char *func, int line) {
    ...
  
    hp->ref = ref;  // 设置 hp 指针
    WT_FULL_BARRIER();  // 内存屏障, 避免设置 hp 和下面读 page state 的流程乱序
    current_state = ref->state;
    if (current_state == WT_REF_MEM) {  // page 状态正常，当前没有其他线程独占访问
        ++session->nhazard;
        WT_READ_BARRIER();
        return (0);  // 加锁成功
    }
    hp->ref = NULL;  // 否则加锁失败，重置 hp
    *busyp = true;
    return (0);
}
```

对于加独占锁的流程，可以参考 [__wt_page_release_evict](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/evict/evict_page.c#L53) -> [__wt_evict](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/evict/evict_page.c#L176) -> [__evict_exclusive](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/evict/evict_page.c#L33) -> [__wt_hazard_check](https://github.com/mongodb/mongo/blob/r5.0.26/src/third_party/wiredtiger/src/support/hazard.c#L293) 的实现：    

```c
// __wt_page_release_evict
int __wt_page_release_evict(WT_SESSION_IMPL *session, WT_REF *ref, uint32_t flags) {
    ...
    previous_state = ref->state;
    // 当前的状态是  WT_REF_MEM， 则使用 CAS 变更为 LOCKED 状态
    locked =
      previous_state == WT_REF_MEM && WT_REF_CAS_STATE(session, ref, previous_state, WT_REF_LOCKED);
    // 尝试清理自身session 中 hp 对这个 page 的引用
    if ((ret = __wt_hazard_clear(session, ref)) != 0 || !locked) {
        if (locked)
            WT_REF_SET_STATE(ref, previous_state); // 复原状态
        return (ret == 0 ? EBUSY : ret); // 返回错误
    }
    ... 
    ret = __wt_evict(session, ref, previous_state, evict_flags);
    ...
}

// __evict_exclusive
static inline int __evict_exclusive(WT_SESSION_IMPL *session, WT_REF *ref)
{
    // 首先确保 page state 已经设置为了 LOCKED 状态
    WT_ASSERT(session, ref->state == WT_REF_LOCKED); 

    // 检查是否有 session 包含了这个 page的 hp， 如果没有，则加独占锁成功
    if (__wt_hazard_check(session, ref, NULL) == NULL)
        return (0);

    // 如果检查 hp 失败，则加锁失败
    WT_STAT_CONN_DATA_INCR(session, cache_eviction_hazard);
    return (__wt_set_return(session, EBUSY));
}

// __wt_hazard_check: 检查系统中是否有 session 的 hp 引用了指定的 page
WT_HAZARD * __wt_hazard_check(WT_SESSION_IMPL *session, WT_REF *ref, WT_SESSION_IMPL **sessionp)
{
    ...
    
    WT_ORDERED_READ(session_cnt, conn->session_cnt);
    for (s = conn->sessions, i = j = max = walk_cnt = 0; i < session_cnt; ++s, ++i) {
        if (!s->active)  // 如果 session 不是 active，则跳过
            continue;

        hazard_get_reference(s, &hp, &hazard_inuse);
        ... 
        for (j = 0; j < hazard_inuse; ++hp, ++j) {
            ++walk_cnt;
            if (hp->ref == ref) {  // 找到了引用的ref 
                WT_STAT_CONN_INCRV(session, cache_hazard_walks, walk_cnt);
                if (sessionp != NULL)
                    *sessionp = s;
                goto done;  // 返回这个 hp的指针
            }
        }
    }
    WT_STAT_CONN_INCRV(session, cache_hazard_walks, walk_cnt);
    hp = NULL;  // 没有找到引用这个 page 的 hp，返回 NULL

done:
    ...
    return (hp);
}
```

## 2.4 实时预估数据压缩率
### 问题背景
在 [1.3 章](https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/blob/zhenyi-dev/1.%E6%96%87%E6%A1%A3%E6%A8%A1%E5%9E%8B/3.%E6%96%87%E6%A1%A3%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E.md)介绍 WT 引擎时，提到 WT 引擎会使用压缩算法将内存 page 压缩之后，再按照 4KB 对齐后写入磁盘。      
可以使用 stats() 命令查看表的创建参数：

>mongos> db.coll1.stats().wiredTiger.creationString
access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=1),assert=(commit_timestamp=none,durable_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=snappy,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=false),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=false,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u

其中本次要讨论的算法，涉及的参数有：
- **internal_page_max=4KB**：硬盘上 btree 的非叶子节点最大 4KB
- **leaf_page_max=32KB**: 硬盘上 btree 的叶子节点最大 32KB
- **memory_page_max=10m**： 内存中的 page 最大 10MB

> 为什么硬盘上的 page 大小会有 4KB 和 32KB 的限制？    
个人认为：    
1.如果限制太小，则不能发挥硬盘的 IO 能力（硬盘都是按块读取的，而且为了提升 IO 能力都会有一定的 readahead 预读机制）。另外也会导致 btree 的高度更高以及压缩效率降低。    
2.如果限制太大，会导致读放大比较严重。

内存 page 可能比硬盘上的 page(也叫做 block/extent) 大很多，因此在 checkpoint/evict 流程将内存 page 写入硬盘时，**需要先 split 成多个大小合适的 page**，然后再压缩后写入。

<p align="center">
  <img src="https://github.com/user-attachments/assets/dd91f071-fbd9-40cc-92d1-98e19fab7d87" width=600>
</p>

理想的分隔结果是：分隔后的 page 在压缩之后，大小正好等于或者非常接近 internal_page_max 和 leaf_page_max。 这样 padding 所占的比例就会比较小，空间浪费较小。    

但是数据的压缩比例可能是动态变化的，如何准确预估压缩比例并确定分割点，就是本次要讨论的问题。

### 算法实现
每个 btree 都维护了变量，来记录理想情况下，压缩前 page 的原始大小。也就是上文提到的理想分隔位置：    
- maxintlpage_precomp：理想情况下，非叶子节点 page 的原始大小，初始值 4KB.
- maxleafpage_precomp：理想情况下，叶子节点 page 的原始大小，初始值 32KB * 4 = 128KB.

每次内存 page 会根据上述 precomp 值进行分隔。然后在压缩并写入硬盘后，根据实际压缩大小以及数据原始大小，再去跟新 precomp 值。    

简单来说，就是按照压缩后的大小，计算到 internal_page_max 或 leaf_page_max 的距离是否超过 10%，来动态调整。      
以 leaf page 为例，如果本次压缩后的大小为 28KB，距离 32KB 有 4KB 差距，大于 32KB * 10%. 此时 precomp 值需要向上调整。    
反之，需要向下调整。    

核心代码参考 [__rec_compression_adjust](https://github.com/mongodb/mongo/blob/r4.2.25/src/third_party/wiredtiger/src/reconcile/rec_write.c#L1755) ：
```c
/*
 * __rec_compression_adjust --
 *     Adjust the pre-compression page size based on compression results.
 */
static inline void
__rec_compression_adjust(WT_SESSION_IMPL *session, uint32_t max, size_t compressed_size,
  bool last_block, uint64_t *adjustp)
{
    WT_BTREE *btree;
    uint64_t adjust, current, new;
    u_int ten_percent;

    btree = S2BT(session);
    ten_percent = max / 10;   // 是否调整的参考

    /*
     * Changing the pre-compression size updates a shared memory location
     * and it's not uncommon to be pushing out large numbers of pages from
     * the same file. If compression creates a page larger than the target
     * size, decrease the pre-compression size. If compression creates a
     * page smaller than the target size, increase the pre-compression size.
     * Once we get under the target size, try and stay there to minimize
     * shared memory updates, but don't go over the target size, that means
     * we're writing bad page sizes.
     *	Writing a shared memory location without a lock and letting it
     * race, minor trickiness so we only read and write the value once.
     */
    WT_ORDERED_READ(current, *adjustp);
    WT_ASSERT(session, current >= max);

    if (compressed_size > max) {  // 压缩后大小大于目标值，向下调整
        /*
         * The compressed size is GT the page maximum. Check if the pre-compression size is larger
         * than the maximum. If 10% of the page size larger than the maximum, decrease it by that
         * amount. Else if it's not already at the page maximum, set it there.
         *
         * Note we're using 10% of the maximum page size as our test for when to adjust the
         * pre-compression size as well as the amount by which we adjust it. Not updating the value
         * when it's close to the page size keeps us from constantly updating a shared memory
         * location, and 10% of the page size is an OK step value as well, so we use it in both
         * cases.
         */
        adjust = current - max;
        if (adjust > ten_percent)
            new = current - ten_percent;
        else if (adjust != 0)
            new = max;
        else
            return;
    } else {  // 压缩后大小小于目标值，向上调整
        /*
         * The compressed size is LTE the page maximum.
         *
         * Don't increase the pre-compressed size on the last block, the last block might be tiny.
         *
         * If the compressed size is less than the page maximum by 10%, increase the pre-compression
         * size by 10% of the page, or up to the maximum in-memory image size.
         *
         * Note we're using 10% of the maximum page size... see above.
         */
        if (last_block || compressed_size > max - ten_percent)
            return;

        adjust = current + ten_percent;
        if (adjust < btree->maxmempage_image)
            new = adjust;
        else if (current != btree->maxmempage_image)
            new = btree->maxmempage_image;
        else
            return;
    }
    // 调整完成，更新 precomp 值
    *adjustp = new;
}
```

# 3. 参考文档
1. https://www.runoob.com/design-pattern/decorator-pattern.html
2. https://github.com/mongodb/mongo/tree/r4.2.25
3. MongoDB 索引使用总结：https://mp.weixin.qq.com/s/g0mNt73nnFS_xyVGuAPe-g
