# 1. 导语
事务是数据库系统中的核心功能，它确保数据可以一致地从一个状态到另一个状态。
MongoDB 从 4.0 内核版本提供了 ACID 事务，支持跨文档和集合的事务。从 4.2 版本开始，MongoDB 提供了跨分片的 2PC 事务。

本文依据 4.2 版本内核代码，自顶向下介绍 MongoDB 中分布式事务、副本集事务、单机事务的实现。

>有些内容在前面章节也有涉及，比如事务的 oplog 复制，混合时钟等。但是为了不影响本文的阅读性，会适当进行重复介绍。

# 2. 分布式事务
## 2.1 基本概念
MongoDB 采用基于混合逻辑时钟的 2PC 协议实现分布式事务。
### 混合逻辑时钟
MongoDB 通过[混合逻辑时钟](https://dl.acm.org/doi/pdf/10.1145/3299869.3314049)(Hybrid Logical Clock, HLC) 来维护每个 client 内部多个请求的顺序。    
**HLC 由 <int32: unix second> 和 <int32: counter> 共同组成**，也叫 clusterTime 在集群中传播。Primary 节点在执行写请求时推进 HLC，并将其写入到 oplog，也会跟随数据写入到存储引擎，作为 MVCC 版本控制的依据。    
Primary 节点作为 HLC 的推动者，集群中的其他节点（client, mongos, config server, shard secondary）作为 HLC 的传播者，通过 gossip 机制实现全局同步。     
下图说明 Client 如何通过 clusterTime 在 secondary 节点上完成”read own write“:

<p align="center">
  <img src="https://github.com/user-attachments/assets/fab405a1-0831-4d16-b8e9-0482b20d2d28" width=500>
</p>

1. Client 给 primary 节点发送写请求。
2. Primary 执行请求，并推进 opTime 到 T2，并记录 oplog 进行异步复制。
3. Primary 将 opTime T2 作为返回结果的元数据传递到 Client.
4. Client 推进 opTime 到 T2.
5. Client 给 secondary 节点发起读请求，并携带 {afterClusterTime: T2}，表示要读取 T2 及之后版本的数据。
6. Secondary 判断自己的同步进度有没有到 T2, 如果没有则等待。
7. 如果满足条件，则执行读请求，并返回文档。

HLC 主要还是解决如何定序，管理数据版本的问题。在此之前，业界已经存在一些经典的解决方案，比较知名的有 Lamport 的逻辑时钟，Google Spanner 的时钟服务等。MongoDB 最终还是采用了 HLC 方案，可以从以下几个问题进行分析。    
1.  **为什么不采用逻辑时钟，而加入了物理时间（wall clock）?**    
真实的业务场景往往是伴随物理时间的，比如按时间点读取数据，按时间点回档数据库等，能精确到 xx年xx月xx 日xx时xx分xx秒。如果只用逻辑时间，很多业务场景都无法满足。
2.  **为什么不使用 Spanner 的 TrueTime 时钟服务？**    
MongoDB 官方认为会增加读写请求的延迟（每次执行请求之前都要调用 TrueTime 的 API），而且增加部署难度（额外的服务和硬件）。
3.  **为什么不直接用更精细的纳秒时钟，而是采用 秒级时钟 + 秒内counter 的方式？**    
引入物理时钟之后，面临的问题是如何保证时钟一直会持续递增？一般部署 MongoDB 的机器都会采用 NTP 服务进行时钟对齐，那物理时间就有回拨的风险。引入 counter 能够保证即使机器时钟回拨，也不倒退 HLC的物理时钟，而是推进 counter 来保证递增。换句话说，机器时间只是 HLC 的参考，但不是绝对权威，具体可以参考 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98) 代码的实现。但是一般来说，NTP 能够保证机器时钟不会有巨幅波动，所以可认为 HLC 的物理时间 == 机器时间。
4. **HLC如何保证安全性？**       
如果由于黑客攻击，将集群中的 HLC 推进到了 Unix 时间的最大值，则集群将无法继续处理写请求。因此，MongoDB 对集群中传递的 HLC 增加了 Hash 签名校验，密钥只有集群内部知道，而且定期更新。另外代码中也对 HLC的推进作了限制，默认每次推进的时间不能超过 1年。
Hash 校验保证了安全性，但是牺牲了计算资源。MongoDB 为了提升性能进行了2点针对性地优化：a. 区分[特权认证](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L200)的连接，这些被信任连接的请求可以免去 Hash 校验；b. 充分利用 HLC [只能向前推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L161)的特性，通过[缓存](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/time_proof_service.h#L44) 避免重复的 Hash 计算。
5.  **HLC 如何传播？**  
采用 gossip 机制，所有读写请求涉及的节点（Client 和 MongoDB中的各个节点）都会参与传播。但是只有 primary 节点有权限推进 HLC。

### 2PC 协议
在 MongoDB 的 2PC 协议中主要包含以下 3 种角色：
1. Router: 由 mongos 节点承担，负责将事务中的操作路由到正确的 Shard 节点，将 commit/abort 操作路由到 coordinator。并承担一定的事务决策，比如统计参与事务的 shard 列表，是否为只读事务，是否需要进行 2PC 提交等。      
2. Coordinator：由第 1 个参与事务的 shard 承担，是 2PC 的协调者，负责协调各个参与事务的 shard 节点执行事务。    
3. Participant：参与事务的 shard 节点，负责执行事务中的操作，并执行 Coordinator 发过来的 prepare/commit/abort 请求。    

### 全局事务 ID
不使用传统 GTID 会面临至少以下  2 个问题：    
1. 如何在多个节点之间串联这个事务？    
多个 shard 节点之间通过 lsid(id+uid+txnNumber) 来串联全局的分布式事务。
2. 如何给多个事务定序？    
在现有架构下，无法保证线性一致。比如：2 个不相关的事务 t1 和 t2，也分别操作了不同的 shard，即使 t1 在真实世界中先提交，也无法保证 t1->commit_timestamp < t2->commit_timestamp.

但是，基于现有逻辑时钟的方式，能够保证因果一致性。比如同一个客户端先后发起的事务 t1 和 t2，能保证 t1->commit_timestamp < t2->commit_timestamp.

## 2.2 分布式事务执行流程
### 2 阶段提交
如果涉及到多个 shard 更新，会走 2 阶段提交方式。
以下图为例，客户端插入 2 条数据，涉及到 2 个 shard：

<p align="center">
  <img src="https://github.com/user-attachments/assets/8978f05d-59f5-4b7c-926d-f74fc1357071" width=600>
</p>

mongos 侧的行为：    
1. client -> mongos 发送第1条 insert 命令，携带 lsid, startTransaction:true, txnNumber:1, autocommit: false
2. client -> mongos 发送第2条 insert 命令，携带的参数少了 startTransaction: true, 其他类似
3. client -> mongos 发送 commitTransaction 命令，携带参数有 lsid,txnNumber,autocommit(false)和 recoveryToken: { recoveryShardId: "shard2" }

shard1 侧的行为：    
1. mongos->shard1 发送 insert {a:3} 命令， 携带 lsid,uid, txnNumber, startTransaction: true, autocommit: false.
2. coordinator(shard2) -> shard1 发送 prepareTransaction 命令，携带 lsid,uid, txnNumber, autocommit: false. 
3. coordinator(shard2) -> shard1 发送 commitTransaction 命令，携带 lsid,uid,txnNumber,autocommit 以及  commitTimestamp: Timestamp(1720856237, 4) 信息。

shard2（coordinator shard） 侧的行为:    
1. mongos -> shard2 发送 insert 命令，携带参数 lsid, uid, txnNumber, startTransaction: true, coordinator: true, autocommit: false.  
2. mongos -> shard2 发送 coordinateCommitTransaction 命令，携带的参数除了 lsid, uid, txnNumber, coordinator:true 之外，还有重要的 participants: [ { shardId: "shard2" }, { shardId: "shard1" } ].    
    - 2.1 shard2 更新本地的 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ]。
    - 2.2 shard2 执行 prepareTransaction 命令， prepare 时间为（Timestamp(1720856237, 4)）
    - 2.3 shard2 更新 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ], decision: { decision: "commit", commitTimestamp: Timestamp(1720856237, 4) } }
    - 2.4 shard2 上执行 commitTransaction，指定 commitTimestamp: Timestamp(1720856237, 4)，并在几毫秒后结束。
    - 2.5 shard2 上删除 config.transaction_coordinators 中的内容，过滤条件是 lsid+uid+txnNumber.

### 1 阶段提交
如果只涉及到 1 个 shard 的更新，则mongos 直接透传 commitTransaction 给对应的 shard，走 1 阶段提交流程。    

<p align="center">
  <img src="https://github.com/user-attachments/assets/4c383cae-05f4-4ead-b986-4adc27374084" width=600>
</p>

mongos 侧行为：    
1. client->mongos 发送一条 insert 命令，并指定 lsid,txnNumber, autocommit:false, 以及 startTransaction: true
2. client->mongos 发送 commitTransaction 命令，

shard1 侧的行为：    
1. mongos->shard2 发送第 1 条 insert 命令，并指定 startTransaction: true, coordinator: true.
2. mongos->shard2 发送 commitTransaction 命令。


## 2.3 多副本的一致性保证
每个 shard 是多副本架构，所有的读写和事务操作都在 primary 节点上进行，通过 oplog 同步到其他节点。    

在 4.0 版本，由于没有 2 阶段事务，事务涉及的所有操作，都是在事务提交之后打包成 1 条 oplog， 然后同步到其他节点。这种方式处理起来简单，但是会受限 BSON 的 16MB 大小限制。    

### 2 阶段事务的 oplog 同步
在 4.2 版本引入 2 阶段事务之后，事务的 oplog 同步机制有以下变化：    
1. 事务在 prepare 和 commit 阶段都会生成 oplog，分别进行同步。其中  prepare 的 oplog 会包含具体的操作和数据， commit 的 oplog 只会包含最后的提交决定；    
2. prepare 的oplog 不再局限于 1 条，可以是多条（每条 oplog会存储前一条的指针，每条 oplog 会尽量打包到 16MB，以此降低 oplog 链表的长度），即突破了 16MB 的限制；    

举个例子，集群中有 2 个 shard，客户端开启一个事务后，往 shard2（coordinator shard） 写入了 3 条数据，往 shard1 写入了 1 条数据，则对应的 oplog 信息如下。     
shard1 有 2 条 oplog，分别是 prepare 和 commit (日志展示为从新到旧)：
```
{ "ts" : Timestamp(1720856237, 7), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "d", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "wall" : ISODate("2024-07-13T07:37:17.978Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) } } }
{ "ts" : Timestamp(1720856237, 6), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-13T07:37:17.970Z"), "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(1720856237, 4), "t" : NumberLong(1) }, "o" : { "commitTransaction" : 1, "commitTimestamp" : Timestamp(1720856237, 4) } }
{ "ts" : Timestamp(1720856237, 5), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "o2" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) } }, "wall" : ISODate("2024-07-13T07:37:17.958Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) }, "participants" : [ "shard1", "shard2" ], "decision" : { "decision" : "commit", "commitTimestamp" : Timestamp(1720856237, 4) } } }
{ "ts" : Timestamp(1720856237, 4), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-13T07:37:17.945Z"), "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) }, "o" : { "applyOps" : [ { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88812"), "a" : 1 } }, { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88813"), "a" : 2 } }, { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66922ead7b91f46778b88815"), "a" : 4 } } ], "prepare" : true } }
{ "ts" : Timestamp(1720856237, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "config.transaction_coordinators", "ui" : UUID("1ccf732c-1733-4c37-80cc-a8b251fad60e"), "wall" : ISODate("2024-07-13T07:37:17.935Z"), "o" : { "_id" : { "lsid" : { "id" : UUID("a3b2abaa-5223-4473-a358-dbf914067f40"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1) }, "participants" : [ "shard1", "shard2" ] } 
```

### 1 阶段事务的 oplog 同步
如果分布式事务走的是 1 阶段提交，或者客户端使用的是 4.2 版本的副本集事务，则只有 commit 的 oplog，并且也可以是多条。    

举个例子，客户端只往 shard2 上写入了1 条数据，然后提交。此时不涉及 prepare 流程。    
shard2 涉及的 oplog 日志，只有 1 条：    
```
{ "ts" : Timestamp(1720938340, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "c", "ns" : "admin.$cmd", "wall" : ISODate("2024-07-14T06:25:40.858Z"), "lsid" : { "id" : UUID("295440e5-c972-4c9e-819c-97655f8d9885"), "uid" : BinData(0,"p7MxbwRFANsxxxnZIap+ho8v8Gvc5VLdCcD6t77QNwQ=") }, "txnNumber" : NumberLong(1), "prevOpTime" : { "ts" : Timestamp(0, 0), "t" : NumberLong(-1) }, "o" : { "applyOps" : [ { "op" : "i", "ns" : "db1.coll1", "ui" : UUID("216d9637-01bd-4db5-b44f-779438ef0294"), "o" : { "_id" : ObjectId("66936f64200c042e039d8211"), "a" : 1 } } ] } }
```

### 事务中的 oplog 回放流程
对于 4.0 内核版本的副本集事务，是比较好处理的。事务在提交时，会将所有操作通过一个数组打包好，并放在一条 oplog 中。这种方式虽然对事务大小有 [16MB 限制](https://www.mongodb.com/docs/manual/core/transactions-production-consideration/#oplog-size-limit)，但是 secondary 节点在回放时非常好处理。回放时只需要将事务 oplog 中的数组解开，得到具体的写操作后进行 hash 就能并发回放了。本质上和普通的写操作并无太大区别。      

但是， 4.2 内核版本 oplog 的变化给回放流程带来了新的挑战，主要有：    
1. 如何保证**原子性**。假如 1 个事务有 4 条 oplog，secondary 节点在第 1 次拉取时拉到了 2 条 oplog（每次拉 oplog 的大小也是有限制的，可能刚好拉到前 2 条 oplog 时就达到了大小限制（每次拉取的 oplog 大小总和不超过 16MB），后面 2 条只能下一次再拉）。那么前面 2 条 oplog 能被回放吗，只回放事务的部分操作，岂不是破坏了事务的原子性？另外，第 2 次将事务的后 2 条 oplog 也拉过来了，如何找到这个事务前 2 条 oplog 呢？
2. 如何进行 **2 阶段事务**的并发。为了回放的正确性，要保证同一事务的 prepare 在 commit/abort 之前执行。Prepare 中的操作也不能直接解析成普通的写操作进行回放，因为此时还不确定事务能否成功提交。另外，Commit/abort 操作也不能和普通的写操作并发回放。

我们通过一个副本集事务的 oplog 回放流程，来探讨如何解决回放的原子性问题。    
假设一个很大的副本集事务依次生成了 4 条 oplog，时间戳分别是 t1、t2、t3、t4。Secondary 节点第 1 批拉取到了 t1 和t2, 第 2 批拉取到了t3 和 t4。如下图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/f1810c15-8de6-400b-9af0-c20b12cca90f" width=700>
</p>

先看第 1 批的回放流程：    
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行，**包括 t1 和 t2**）。
2. 执行 oplog 的 hash 流程：    
2.1 处理到 t1, 发现是事务的一部分 oplog，即 partial = true。先不回放，而是将这条 oplog 放在[内存 cache](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1199-L1212) 中。这里是为了方便后续回溯，而 第 1 步的oplog 写表异步任务可能还没完成，因此必须引入内存 cache。    
2.2 处理到 t2，还是 partial = true, 也放到内存 cache 中。    
3. 等待第 1 步写 oplog 表的异步任务执行完，然后将第 1 批 oplog 全部回放完。在此过程中，t1 和 t2 中的操作并没有被执行，而且内存 cache 也被清空。

接着再回放第 2 批：    
1. Secondary节点先将拉取到的所有 oplog 都写到自己的 oplog 表中（异步执行）。
2. 执行 oplog 的 hash 流程：    
2.1 处理到 t3，发现 partial = true, 放到内存 cache中。    
2.2 处理到 t4，发现是事务的最后一条 oplog，要准备真正执行回放了。则通过 sessionId 找 cache 中的 t3，然后通过 t3->[prevOpTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L269)字段找到上一条 oplog 的时间戳是 t2, 并使用这个时间戳到[本地 oplog 表](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/transaction_history_iterator.cpp#L59-L61)中把 t2 的完整 oplog 找到。接着用同样的方法找 t2->prevOpTime，得到t1 的完整 oplog。最后发现 t1->prevOpTime = null，此时 oplog 的回溯流程结束。得到事务的完整 oplog： t1、t2、t3、t4。    
2.3 在回溯 oplog 的过程中，也会将上述 4 条 oplog 中包含的操作都解析出来，放在一个数组中并且[排好序](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L290-L305)。然后将这个数组 hash 到多个并发线程中。    
3. 等待第 1 步写 oplog 表的异步任务执行完，然后执行这批 oplog 的操作。由于是在一个 batch 中回放完成的，因此能够保证事务回放的原子性。

---

我们再通过一个例子，看看如何处理 2 阶段事务。
假设事务包含 5 条 oplog，前 4 条是 prepare 阶段产生的，最后 1 条是 commit。如下图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/2e53457c-234f-4145-8e5c-a42bcaea9907" width=700>
</p>

第 1 阶段的回放流程和上述副本集事务例子的第 1 阶段相同，不再赘述。    
第2 阶段流程和副本集事务有些区别：     
1. 处理到 t3，由于 partial = true, 先放到 cache 中。    
2. 由于 t4 是 prepare = true 的 oplog， 会独占一个 oplog batch，不和其他操作共享。处理到 t4 时，会从 oplog 表中把  t3、t2、t1 都回溯出来，然后根据这些 oplog 中的操作[执行 prepareTransaction](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L458)。注意这里是 prepareTransaction，而不是hash成普通的写操作执行，因为此时还不确定这些事务能否提交。    
3. 处理到 t5，这是 1 条 commit  oplog，会独占一个 oplog batch。Secondary 节点根据  commit oplog 中携带的 txnId sessionId 信息，找到之前 prepare 的上下文，然后[执行 commit](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/transaction_oplog_application.cpp#L163)。    


## 2.4 关键的数据结构


## 2.5 异常处理


# 3. 副本集事务

# 4. 单机事务
## 4.1 MongoServer 层的封装

## 4.2 WT 引擎的事务处理

# 5. 总结

# 6. 参考资料
