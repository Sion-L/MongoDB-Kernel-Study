# 1. 导语
事务是数据库系统中的核心功能，它确保数据可以一致地从一个状态到另一个状态。
MongoDB 从 4.0 内核版本提供了 ACID 事务，支持跨文档和集合的事务。从 4.2 版本开始，MongoDB 提供了跨分片的 2PC 事务。

本文依据 4.2 版本内核代码，自顶向下介绍 MongoDB 中分布式事务、副本集事务、单机事务的实现。

>有些内容在前面章节也有涉及，比如事务的 oplog 复制，混合时钟等。但是为了不影响本文的阅读性，会适当进行重复介绍。

# 2. 分布式事务
## 2.1 基本概念
MongoDB 采用基于混合逻辑时钟的 2PC 协议实现分布式事务。
### 混合逻辑时钟
MongoDB 通过[混合逻辑时钟](https://dl.acm.org/doi/pdf/10.1145/3299869.3314049)(Hybrid Logical Clock, HLC) 来维护每个 client 内部多个请求的顺序。    
**HLC 由 <int32: unix second> 和 <int32: counter> 共同组成**，也叫 clusterTime 在集群中传播。Primary 节点在执行写请求时推进 HLC，并将其写入到 oplog，也会跟随数据写入到存储引擎，作为 MVCC 版本控制的依据。    
Primary 节点作为 HLC 的推动者，集群中的其他节点（client, mongos, config server, shard secondary）作为 HLC 的传播者，通过 gossip 机制实现全局同步。     
下图说明 Client 如何通过 clusterTime 在 secondary 节点上完成”read own write“:

<p align="center">
  <img src="https://github.com/user-attachments/assets/fab405a1-0831-4d16-b8e9-0482b20d2d28" width=500>
</p>

1. Client 给 primary 节点发送写请求。
2. Primary 执行请求，并推进 opTime 到 T2，并记录 oplog 进行异步复制。
3. Primary 将 opTime T2 作为返回结果的元数据传递到 Client.
4. Client 推进 opTime 到 T2.
5. Client 给 secondary 节点发起读请求，并携带 {afterClusterTime: T2}，表示要读取 T2 及之后版本的数据。
6. Secondary 判断自己的同步进度有没有到 T2, 如果没有则等待。
7. 如果满足条件，则执行读请求，并返回文档。

HLC 主要还是解决如何定序，管理数据版本的问题。在此之前，业界已经存在一些经典的解决方案，比较知名的有 Lamport 的逻辑时钟，Google Spanner 的时钟服务等。MongoDB 最终还是采用了 HLC 方案，可以从以下几个问题进行分析。    
1.  **为什么不采用逻辑时钟，而加入了物理时间（wall clock）?**    
真实的业务场景往往是伴随物理时间的，比如按时间点读取数据，按时间点回档数据库等，能精确到 xx年xx月xx 日xx时xx分xx秒。如果只用逻辑时间，很多业务场景都无法满足。
2.  **为什么不使用 Spanner 的 TrueTime 时钟服务？**    
MongoDB 官方认为会增加读写请求的延迟（每次执行请求之前都要调用 TrueTime 的 API），而且增加部署难度（额外的服务和硬件）。
3.  **为什么不直接用更精细的纳秒时钟，而是采用 秒级时钟 + 秒内counter 的方式？**    
引入物理时钟之后，面临的问题是如何保证时钟一直会持续递增？一般部署 MongoDB 的机器都会采用 NTP 服务进行时钟对齐，那物理时间就有回拨的风险。引入 counter 能够保证即使机器时钟回拨，也不倒退 HLC的物理时钟，而是推进 counter 来保证递增。换句话说，机器时间只是 HLC 的参考，但不是绝对权威，具体可以参考 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98) 代码的实现。但是一般来说，NTP 能够保证机器时钟不会有巨幅波动，所以可认为 HLC 的物理时间 == 机器时间。
4. **HLC如何保证安全性？**       
如果由于黑客攻击，将集群中的 HLC 推进到了 Unix 时间的最大值，则集群将无法继续处理写请求。因此，MongoDB 对集群中传递的 HLC 增加了 Hash 签名校验，密钥只有集群内部知道，而且定期更新。另外代码中也对 HLC的推进作了限制，默认每次推进的时间不能超过 1年。
Hash 校验保证了安全性，但是牺牲了计算资源。MongoDB 为了提升性能进行了2点针对性地优化：a. 区分[特权认证](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L200)的连接，这些被信任连接的请求可以免去 Hash 校验；b. 充分利用 HLC [只能向前推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_time_validator.cpp#L161)的特性，通过[缓存](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/time_proof_service.h#L44) 避免重复的 Hash 计算。
5.  **HLC 如何传播？**  
采用 gossip 机制，所有读写请求涉及的节点（Client 和 MongoDB中的各个节点）都会参与传播。但是只有 primary 节点有权限推进 HLC。

### 2PC 协议
在 MongoDB 的 2PC 协议中主要包含以下 3 种角色：
1. Router: 由 mongos 节点承担，负责将事务中的操作路由到正确的 Shard 节点，将 commit/abort 操作路由到 coordinator。并承担一定的事务决策，比如统计参与事务的 shard 列表，是否为只读事务，是否需要进行 2PC 提交等。      
2. Coordinator：由第 1 个参与事务的 shard 承担，是 2PC 的协调者，负责协调各个参与事务的 shard 节点执行事务。    
3. Participant：参与事务的 shard 节点，负责执行事务中的操作，并执行 Coordinator 发过来的 prepare/commit/abort 请求。    

### 全局事务 ID
不使用传统 GTID 会面临至少以下  2 个问题：    
1. 如何在多个节点之间串联这个事务？    
多个 shard 节点之间通过 lsid(id+uid+txnNumber) 来串联全局的分布式事务。
2. 如何给多个事务定序？    
在现有架构下，无法保证线性一致。比如：2 个不相关的事务 t1 和 t2，也分别操作了不同的 shard，即使 t1 在真实世界中先提交，也无法保证 t1->commit_timestamp < t2->commit_timestamp.

但是，基于现有逻辑时钟的方式，能够保证因果一致性。比如同一个客户端先后发起的事务 t1 和 t2，能保证 t1->commit_timestamp < t2->commit_timestamp.

## 2.2 分布式事务执行流程
### 2 阶段提交
如果涉及到多个 shard 更新，会走 2 阶段提交方式。
以下图为例，客户端插入 2 条数据，涉及到 2 个 shard：

<p align="center">
  <img src="https://github.com/user-attachments/assets/8978f05d-59f5-4b7c-926d-f74fc1357071" width=600>
</p>

mongos 侧的行为：    
1. client -> mongos 发送第1条 insert 命令，携带 lsid, startTransaction:true, txnNumber:1, autocommit: false
2. client -> mongos 发送第2条 insert 命令，携带的参数少了 startTransaction: true, 其他类似
3. client -> mongos 发送 commitTransaction 命令，携带参数有 lsid,txnNumber,autocommit(false)和 recoveryToken: { recoveryShardId: "shard2" }

shard1 侧的行为：    
1. mongos->shard1 发送 insert {a:3} 命令， 携带 lsid,uid, txnNumber, startTransaction: true, autocommit: false.
2. coordinator(shard2) -> shard1 发送 prepareTransaction 命令，携带 lsid,uid, txnNumber, autocommit: false. 
3. coordinator(shard2) -> shard1 发送 commitTransaction 命令，携带 lsid,uid,txnNumber,autocommit 以及  commitTimestamp: Timestamp(1720856237, 4) 信息。

shard2（coordinator shard） 侧的行为:    
1. mongos -> shard2 发送 insert 命令，携带参数 lsid, uid, txnNumber, startTransaction: true, coordinator: true, autocommit: false.  
2. mongos -> shard2 发送 coordinateCommitTransaction 命令，携带的参数除了 lsid, uid, txnNumber, coordinator:true 之外，还有重要的 participants: [ { shardId: "shard2" }, { shardId: "shard1" } ].    
    - 2.1 shard2 更新本地的 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ]。
    - 2.2 shard2 执行 prepareTransaction 命令， prepare 时间为（Timestamp(1720856237, 4)）
    - 2.3 shard2 更新 config.transaction_coordinators 表，设置 participants: [ "shard1", "shard2" ], decision: { decision: "commit", commitTimestamp: Timestamp(1720856237, 4) } }
    - 2.4 shard2 上执行 commitTransaction，指定 commitTimestamp: Timestamp(1720856237, 4)，并在几毫秒后结束。
    - 2.5 shard2 上删除 config.transaction_coordinators 中的内容，过滤条件是 lsid+uid+txnNumber.

### 1 阶段提交
如果只涉及到 1 个 shard 的更新，则mongos 直接透传 commitTransaction 给对应的 shard，走 1 阶段提交流程。    

<p align="center">
  <img src="https://github.com/user-attachments/assets/4c383cae-05f4-4ead-b986-4adc27374084" width=600>
</p>

mongos 侧行为：    
1. client->mongos 发送一条 insert 命令，并指定 lsid,txnNumber, autocommit:false, 以及 startTransaction: true
2. client->mongos 发送 commitTransaction 命令，

shard1 侧的行为：    
1. mongos->shard2 发送第 1 条 insert 命令，并指定 startTransaction: true, coordinator: true.
2. mongos->shard2 发送 commitTransaction 命令。



## 2.3 多副本的一致性保证


## 2.4 关键的数据结构


## 2.5 异常处理


# 3. 副本集事务

# 4. 单机事务
## 4.1 MongoServer 层的封装

## 4.2 WT 引擎的事务处理

# 5. 总结

# 6. 参考资料
