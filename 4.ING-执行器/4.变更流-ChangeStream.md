# 1. 什么是 ChangeStream
ChangeStream是 MongoDB 提供的 CDC 解决方案。
```
Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog.
```

使用传统的 tailing oplog 在 CDC（change data capture）场景下非常不便：    
1. oplog理论上仅供内部 replication 使用：    
    - 比如 noop oplog、migrate 对用户来说没有意义；
    - 比如 5.0 引入了 diff 格式，oplog 格式发生了变化，而且没有文档说明；
    - 比如事务相关的 oplog 有一系列的解析规则；
    - ...
2. 分片集群下每个 shard 都有自己的 oplog，排序和断点续传复杂。
3. 如果只需要关注某个表的数据变更，也要把oplog全部读出来，然后在客户端过滤。
4. 扩展性差，oplog 只是普通的数据，定制化逻辑需要客户端实现，比如post-image、pre-image 等等。

```
Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them.
```

Change stream 支持库表级别的监听，相比 tailing oplog，易用性更强、而且节省了网络传输和客户端的资源消耗。

```
Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will.
```

有着 aggregate 框架的加成，change stream 扩展性更强。比如支持获取 PreImages 和 PostImages，也支持 UpdateLookUp。

Change stream 的缺点：    
- 早期内核版本实现不完善。自3.6版本引入，后续版本对DDL操作支持不够，到6.0版本有所改善。
- 资源开销较大，性能跟不上。   

# 2. 总体流程
如图所示：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/3e0eb9c6-4888-4f2c-8b27-078a4ba2dc06" width=900>
</p>

总体流程为：    
1. 客户端调用driver的watch函数。    
2. driver向mongos发送aggregate请求。    
3. mongos向各个shard 发送aggregate请求。    
4. 各个shard把各自的cursor返回给mongos。    
5. mongos返回一个cursor给driver。    
6. driver返回对象给客户端。    
7. 客户端后续调用driver的next函数。    
8. driver发送getMore给mongos，请求更多数据。    
9. mongos向各个shard发送getMore请求。    
10. 各个shard返回数据给mongos。    
11. mongos返回数据给driver。    
12. driver返回数据给客户端。    

客户端程序如下所示：    
```
func run(cli *mongo.Client, ts primitive.Timestamp) error {
	cs, err := clie.Watch(context.Background(), bson.A{}, &options.ChangeStreamOptions{
		StartAtOperationTime: &ts,
	})
	if err != nil {
		return err
	}

	for cs.Next(context.Background()) {
		var doc bson.D
		err = bson.Unmarshal(cs.Current, &doc)
		if err != nil {
			return err
		}
       // other code to consume doc
	}

    return cs.Err()
}
```

后面主要分享mongodb内核对change stream相关的处理，也上面的步骤2~11。基于 [4.2](https://github.com/mongodb/mongo/tree/v4.2) 代码进行分析。

带着问题看下面的讲解：     
- 如何汇聚、排序多个shard上的事件？    
- 如何做断点续传？不同于tailing oplog，要考虑到长时间没有事件的情况。    
- 如何处理事务相关的事件？    
- 性能差在哪？    

# 3. 详细流程
下面以一个真实的 change stream 使用场景为例，遵循时间顺序依次讲解 mongos 和 mongod 的处理逻辑。

## 3.1 mongos处理driver发来的aggregate请求
请求：    
```
{
  aggregate: 1,
  pipeline: [
    {
      '$changeStream': {
        allChangesForCluster: true,
        startAtOperationTime: Timestamp({ t: 1698997099, i: 0 }),
      }
    }
  ],
  cursor: {},
  lsid: { id: new UUID("14265384-a9d8-4aa5-a60c-f95d499ea833") },
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698997490, i: 1 }),
    signature: {
      hash: Binary("000000000000000000000000000000"),
      keyId: Long("0")
    }
  },
  '$db': 'admin'
}
```

mongos返回一个cursor id给driver:    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
}
```

下面来探究mongos是如何处理这个请求的。

### 代码处理流程
```
ClusterPipelineCommand::run    src/mongo/s/commands/cluster_pipeline_cmd.cpp   
    ClusterAggregate::runAggregate    src/mongo/s/query/cluster_aggregate.cpp  
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        ClusterAggregate::runAggregate
            Pipeline::parse                                     // 构建pipeline
                Pipeline::parseTopLevelOrFacetPipeline
                    DocumentSource::parse
                        DocumentSourceChangeStream::createFromBson
                            buildPipeline
            sharded_agg_helpers::dispatchShardPipeline
                mustRunOnAllShards
                cluster_aggregation_planner::splitPipeline         // 分离出需要在shard上运行的pipeline
                    findSplitPoint
                createCommandForTargetedShards
                    genericTransformForShards
                establishShardCursors                              // 在每个shard上创建cursor
                    establishCursors
            dispatchMergingPipeline
                cluster_aggregation_planner::addMergeCursorsSource // 创建在mongos运行的pipeline
                    DocumentSourceMergeCursors::create
                runPipelineOnMongoS                                // 在mongos上运行pipeline
                    establishMergingMongosCursor                   // 建立一个总的cursor，用来吐出汇聚后的数据
                        cluster_aggregation_planner::buildClusterCursor
                        ccc->next(kInitialFind)
                            RouterStagePipeline::next
                                Pipeline::getNext
                                    DocumentSourceCloseCursor::getNext
                                        DocumentSourceUpdateOnAddShard::getNext
                                            DocumentSourceMergeCursors::getNext          // return EOF
                                                DocumentSourceMergeCursors::populateMerger
                                                    BlockingResultsMerger::BlockingResultsMerger
                                                        AsyncResultsMerger::AsyncResultsMerger
                                                BlockingResultsMerger::next
                                                    BlockingResultsMerger::awaitNextWithTimeout
                        setPostBatchResumeToken(ccc->getPostBatchResumeToken())
                            RouterStagePipeline::getPostBatchResumeToken
                                DocumentSourceMergeCursors::getHighWaterMark
                                    BlockingResultsMerger::getHighWaterMark
                                        AsyncResultsMerger::getHighWaterMark
                        registerCursor                               // 保存cursor信息，等待getMore请求
```

最初构建的pipeline，是由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- DocumentSourceCloseCursor

前 4 个 stage 是 shard stage，不会运行在 mongos 上。对于 mongos 来说这四个 stage 的意义就是生成发送给 shard 的命令，之后会把这几个 stage 丢弃。DocumentSourceCloseCursor 是 merge stage，要在 mongos 上做，merge 的时候以 _id._data 的值排序。      

cluster_aggregation_planner::addMergeCursorsSource 函数中，真正建立了要在mongos运行的pipeline（数据从上往下流动）：    
- DocumentSourceMergeCursors
- DocumentSourceUpdateOnAddShard
- DocumentSourceCloseCursor

建立好 pipeline 之后，会调用 getNext 运行一下，初始化各个stage，从各个 shard 拿到 cursor id 和 post batch resume token，之后返回 EOF。    

## 3.2 mongod 处理 mongos 发来的 aggregate 请求
mongos 向 mongod 发送的 aggregate 请求如下（pipeline 跟driver 发送给 mongos 的 pipeline 一样）：      

<p align="center">
  <img src="https://github.com/user-attachments/assets/56379713-bc62-4840-a195-e0ba21f2cf60" width=550>
</p>

mongod 把 cursor id 返回给 mongos :    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  $_internalLatestOplogTimestamp: Timestamp({ t: 1698997500, i: 1 }),
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $gleStats: {
    lastOpTime: Timestamp({ t: 1234356, i: 0 }),
    electionId: ObjectID("7fffffff0000000000000001")
  },
  lastCommittedOpTime: Timestamp({ t: 1698997500, i: 1 }),
  $configServerState: {
    opTime: { ts: Timestamp({ t: 1698998143, i: 1 }), t: Long("1") }
  },
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
}
```

### 代码处理流程
```
PipelineCommand::Invocation::run
    runAggregate         src/mongo/db/commands/run_aggregate.cpp
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        Pipeline::parse                                      // 构建pipeline，mongos走的也是这个
            Pipeline::parseTopLevelOrFacetPipeline
                DocumentSource::parse
                    DocumentSourceChangeStream::createFromBson
                        buildPipeline
                            DocumentSourceChangeStream::buildMatchFilter
        PipelineD::buildInnerQueryExecutor                  // 创建aggregate框架下层的executor
            PipelineD::buildInnerQueryExecutorGeneric
                PipelineD::prepareExecutor                  // 生成对local.oplog.rs的执行计划
                    attemptToGetExecutor
                        getExecutorFind
                            _getExecutorFind
                                getOplogStartHack
                                    WiredTigerRecordStore::oplogStartHack // 给出collection scan的起始位置
        PipelineD::attachInnerQueryExecutorToPipeline      
            DocumentSourceCursor::create
            PipelineD::addCursorSource
                Pipeline::addInitialSource
        createOuterPipelineProxyExecutor                    // 为change stream加一个proxy stage
            ChangeStreamProxyStage::ChangeStreamProxyStage
                PipelineProxyStage::PipelineProxyStage
        registerCursor                                      // 保存cursor
        handleCursorCommand
            setPostBatchResumeToken(exec->getPostBatchResumeToken())
                PlanExecutorImpl::getPostBatchResumeToken
                    ChangeStreamProxyStage::getPostBatchResumeToken
```

跟 mongos 差不多，最初构建的 pipeline 由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch类混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability

最后经过调整，pipeline 是这样：    
- CollectionScan
- DocumentSourceCursor
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- ChangeStreamProxyStage

## 3.3 mongos 收到 driver 发来的 getMore 请求
请求如下：    
```
{
  getMore: Long("6439458890814903597"),
  collection: '$cmd.aggregate',
  lsid: { id: new UUID("14265384-a9d8-4aa5-a60c-f95d499ea833") },
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698997500, i: 1 }),
    signature: {
      hash: Binary("000000000000000000000000000000000"),
      keyId: Long("0")
    }
  },
  '$db': 'admin'
}
```

mongos 返回数据给 driver:
```
{
  cursor: {
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate,
    nextBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"}
  },
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
}
```

### 代码处理流程
```
ClusterGetMoreCmd::Invocation::run
    ClusterFind::runGetMore
        cursorManager->checkOutCursor
        ClusterCursorManager::PinnedCursor::next
            ClusterClientCursorImpl::next
                RouterStagePipeline::next
                    Pipeline::getNext
                        DocumentSourceCloseCursor::getNext
                            DocumentSourceUpdateOnAddShard::getNext
                                DocumentSourceMergeCursors::getNext
                                    BlockingResultsMerger::next
                                        BlockingResultsMerger::awaitNextWithTimeout
                                            BlockingResultsMerger::getNextEvent
                                                AsyncResultsMerger::nextEvent    
                                                    AsyncResultsMerger::_scheduleGetMores
```

mongos对driver发送来的getMore请求处理比较简单：    
- 取出cursor.
- 调用next.
- 底层stage发送getMore请求给mongod，获取数据。

接下来先看mongod如何处理getMore请求，以及返回给mongos什么数据。然后再看mongos如何返回数据给客户端。

## 3.4 mongod 处理 mongos 发来的 getMore 请求
请求如下：    

<p align="center">
  <img src="https://github.com/user-attachments/assets/1203e489-7c31-4f2e-934f-1cf13da8c0b2" width=550>
</p>

之后，mongod 把数据返回给 mongos:    
```
{
  cursor: {
    nextBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  $_internalLatestOplogTimestamp: Timestamp({ t: 1698997500, i: 1 }),
  ok: 1,
  $gleStats: {
    lastOpTime: Timestamp({ t: 0, i: 0 }),
    electionId: ObjectID("7fffffff0000000000000001")
  },
  lastCommittedOpTime: Timestamp({ t: 1698997500, i: 1 }),
  $configServerState: {
    opTime: { ts: Timestamp({ t: 1698998143, i: 1 }), t: Long("1") }
  },
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698997500, i: 1 })
}
```

这里面有两个重要的东西 **nextBatch** 和 **postBatchResumeToken**，分别是数据和断点续传标记。下面重点看这两个东西怎么来的。

### 代码处理流程
```
GetMoreCmd::Invocation::run
    GetMoreCmd::Invocation::acquireLocksAndIterateCursor
        GetMoreCmd::Invocation::generateBatch
            PlanExecutorImpl::getNext
                PlanExecutorImpl::_getNextImpl
                    PlanStage::work
                        PipelineProxyStage::doWork   （ChangeStreamProxyStage继承自PipelineProxyStage）
                            ChangeStreamProxyStage::getNextBson
                                Pipeline::getNext
                                    DocumentSourceCheckResumability::getNext
                                        DocumentSourceCheckInvalidate::getNext
                                            DocumentSourceChangeStreamTransform::getNext
                                                DocumentSourceCursor::getNext
                                                    DocumentSourceCursor::loadBatch
                                                        ...
                                                            CollectionScan::doWork
            setPostBatchResumeToken(exec->getPostBatchResumeToken())
                PlanExecutorImpl::getPostBatchResumeToken
                    ChangeStreamProxyStage::getPostBatchResumeToken    // read _postBatchResumeToken
```

跟 mongos 一样，取出 cursor 之后，就开始走 pipeline 流程。下面从数据流动的方向开始讲解。    

### CollectionScan    
Query 请求长这样：      

<p align="center">
  <img src="https://github.com/user-attachments/assets/028bd52b-73a1-4f1c-99cb-29e4fc546ff3" width=700>
</p>

过滤条件有很多：    
- oplog 的 ts 大于等于 {11,0} 。这个时间戳是客户端指定的，通过mongos透传到各个shard，用作断点续传。    
    - 考虑一个问题：不同的shard有不同的时间，仅用一个时间戳用作断点续传有没有问题？    
- 捕获 fromMigrate 为 false，非a dmin、config、local 库且 op 不是 n 的 oplog.    
- 捕获 fromMigrate 为 false，op 是 n，且 o2.type 为migrateChunkToNewShard 的 oplog.
- 另外捕获了 drop、dropDatabase、renameCollection、commitTransaction、applyOps 等 DDL 操作。

CollectionScan 不是从头开始的，而是从小于等于客户端指定时间戳，且最近的那条 oplog 开始。如果所有 oplog 的 ts 都比客户端指定时间戳大，collectionScan 就从头开始。然后 collectionScan 会检查这条 oplog，如果不满足下面任一条件，就说明 oplog 被冲了，会抛出ErrorCodes::OplogQueryMinTsMissing 异常：    
- 此 oplog 是类型是 n，o.msg 是 initiating set ，表示这是一个新副本集。
- 此 oplog 的 ts 小于等于给定的 ts.

只要有一个 shard 上的 oplog 被冲了，change stream 命令就会失败。    

另外需要关注，collectionScan 每次返回文档之前都更新一下_latestOplogEntryTimestamp:    
```
Status CollectionScan::setLatestOplogEntryTimestamp(const Record& record) {
    auto tsElem = record.data.toBson()[repl::OpTime::kTimestampFieldName];
    _latestOplogEntryTimestamp = std::max(_latestOplogEntryTimestamp, tsElem.timestamp());
    return Status::OK();
}
```

### DocumentSourceCursor
DocumentSourceCursor 对接 CollectionScan 等普通查询，作为aggregate pipeline 的数据来源。    

这里注意一下 _latestOplogTimestamp 这个私有变量，每次 getNext 都会被更新：    
```
DocumentSource::GetNextResult DocumentSourceCursor::getNext() {
    // ......
    // If we are tracking the oplog timestamp, update our cached latest optime.
    if (_trackOplogTS && _exec)
        _updateOplogTimestamp();
    // ......
}

void DocumentSourceCursor::_updateOplogTimestamp() {
    // If we are about to return a result, set our oplog timestamp to the optime of that result.
    if (!_currentBatch.isEmpty()) {
        const auto& ts = _currentBatch.peekFront().getField(repl::OpTime::kTimestampFieldName);
        invariant(ts.getType() == BSONType::bsonTimestamp);
        _latestOplogTimestamp = ts.getTimestamp();       // 设置为要返回的oplog的ts
        return;
    }

    // If we have no more results to return, advance to the latest oplog timestamp.
    _latestOplogTimestamp = _exec->getLatestOplogTimestamp(); // 从collection scan取时间戳
}

Timestamp CollectionScan::getLatestOplogTimestamp() const {
    return _latestOplogEntryTimestamp;
}
```

### DocumentSourceChangeStreamTransform
这个 stage 负责把 oplog 转换成 **event**。      
```
DocumentSource::GetNextResult DocumentSourceChangeStreamTransform::getNext() {
    while (1) {
        // If we're unwinding an 'applyOps' from a transaction, check if there are any documents we
        // have stored that can be returned.
        if (_txnIterator) {
            if (auto next = _txnIterator->getNextTransactionOp(pExpCtx->opCtx)) {
                return applyTransformation(*next);
            }
            _txnIterator = boost::none;
        }

        // Get the next input document.
        auto input = pSource->getNext();
        if (!input.isAdvanced()) {
            return input;
        }

        auto doc = input.releaseDocument();
        auto op = doc[repl::OplogEntry::kOpTypeFieldName];
        auto opType =
            repl::OpType_parse(IDLParserErrorContext("ChangeStreamEntry.op"), op.getStringData());
        auto commandVal = doc["o"];
        if (opType != repl::OpTypeEnum::kCommand ||
            (commandVal["applyOps"].missing() && commandVal["commitTransaction"].missing())) {
            return applyTransformation(doc);
        }

        // The only two commands we will see here are an applyOps or a commit, which both mean we
        // need to open a "transaction context" representing a group of updates that all occurred at
        // once as part of a transaction. 
        invariant(!_txnIterator);
        _txnIterator.emplace(pExpCtx->opCtx, pExpCtx->mongoProcessInterface, doc, *_nsRegex);
    }
}
```

会把 applyOps 拆开处理。对于普通 oplog，直接进行 transform：    
```
Document DocumentSourceChangeStreamTransform::applyTransformation(const Document& input) {
    MutableDocument doc;

    // Extract the fields we need.
    string op = input[repl::OplogEntry::kOpTypeFieldName].getString();
    Value ts = input[repl::OplogEntry::kTimestampFieldName];
    Value ns = input[repl::OplogEntry::kNssFieldName];
    Value uuid = input[repl::OplogEntry::kUuidFieldName];
    std::vector<FieldPath> documentKeyFields;

    // Deal with CRUD operations and commands.
    auto opType = repl::OpType_parse(IDLParserErrorContext("ChangeStreamEntry.op"), op);

    NamespaceString nss(ns.getString());
    // Ignore commands in the oplog when looking up the document key fields since a command implies
    // that the change stream is about to be invalidated (e.g. collection drop).
    if (!uuid.missing() && opType != repl::OpTypeEnum::kCommand) {
        documentKeyFields = _documentKeyCache.find(uuid.getUuid())->second.documentKeyFields;
    }
    Value id = input.getNestedField("o._id");
    // Non-replace updates have the _id in field "o2".
    StringData operationType;
    Value fullDocument;
    Value updateDescription;
    Value documentKey;

    switch (opType) {
        case repl::OpTypeEnum::kInsert: {
            operationType = DocumentSourceChangeStream::kInsertOpType;
            fullDocument = input[repl::OplogEntry::kObjectFieldName];
            documentKey = Value(document_path_support::extractPathsFromDoc(
                fullDocument.getDocument(), documentKeyFields));
            break;
        }
        case repl::OpTypeEnum::kDelete: {
            operationType = DocumentSourceChangeStream::kDeleteOpType;
            documentKey = input[repl::OplogEntry::kObjectFieldName];
            break;
        }
        case repl::OpTypeEnum::kUpdate: {
            if (id.missing()) {
                // non-replace style update
                operationType = DocumentSourceChangeStream::kUpdateOpType;
                // ......
            } else {
                // replace style update
                operationType = DocumentSourceChangeStream::kReplaceOpType;
                fullDocument = input[repl::OplogEntry::kObjectFieldName];
            }
            documentKey = input[repl::OplogEntry::kObject2FieldName];
            break;
        }
        case repl::OpTypeEnum::kCommand: {
            if (!input.getNestedField("o.drop").missing()) {
                operationType = DocumentSourceChangeStream::kDropCollectionOpType;
                nss = NamespaceString(nss.db(), input.getNestedField("o.drop").getString());
            } else if (!input.getNestedField("o.renameCollection").missing()) {
                operationType = DocumentSourceChangeStream::kRenameCollectionOpType;
                // ......
            } else if (!input.getNestedField("o.dropDatabase").missing()) {
                operationType = DocumentSourceChangeStream::kDropDatabaseOpType;
                nss = NamespaceString(nss.db());
            } else {
                // All other commands will invalidate the stream.
                operationType = DocumentSourceChangeStream::kInvalidateOpType;
            }

            // Make sure the result doesn't have a document key.
            documentKey = Value();
            break;
        }
        case repl::OpTypeEnum::kNoop: {
            operationType = DocumentSourceChangeStream::kNewShardDetectedOpType;
            // Generate a fake document Id for NewShardDetected operation so that we can resume
            // after this operation.
            documentKey = Value(Document{{DocumentSourceChangeStream::kIdField,
                                          input[repl::OplogEntry::kObject2FieldName]}});
            break;
        }
        default: { MONGO_UNREACHABLE; }
    }

    // Note that 'documentKey' and/or 'uuid' might be missing, in which case they will not appear
    // in the output.
    auto resumeTokenData = getResumeToken(ts, uuid, documentKey);
    auto resumeToken = ResumeToken(resumeTokenData).toDocument();

    // Add some additional fields only relevant to transactions.
    if (_txnIterator) {
        doc.addField(DocumentSourceChangeStream::kTxnNumberField,
                     Value(static_cast<long long>(_txnIterator->txnNumber())));
        doc.addField(DocumentSourceChangeStream::kLsidField, Value(_txnIterator->lsid()));
    }

    doc.addField(DocumentSourceChangeStream::kIdField, Value(resumeToken));
    doc.addField(DocumentSourceChangeStream::kOperationTypeField, Value(operationType));
    doc.addField(DocumentSourceChangeStream::kClusterTimeField, Value(resumeTokenData.clusterTime));

    doc.setSortKeyMetaField(resumeToken.toBson());
    // ......
    doc.addField(DocumentSourceChangeStream::kDocumentKeyField, documentKey);
    // ......
}
```

这里就是对CURD操作和一些 DDL 操作进行了转换。需要注意的是：    
- 相比 oplog，event 多了 replace、invalidate 和kNewShardDetected 等很多类型。
- oplog 的 ts 字段被用来生成 resume token，也被用来当作 event 的 clusterTime 字段的值。
- resume token 同时作为 event 的 _id 和 sortKey.     

### DocumentSourceCheckInvalidate
这个 stage 检查 event，如果符合下面条件就生成一个 invalidate 类型的e vent：
- 要监听的 ns、db 被 drop 或者 rename.

### DocumentSourceCheckResumability
这个 stage 做断点续传的恢复工作。    
```
DocumentSource::GetNextResult DocumentSourceCheckResumability::getNext() {
    if (_resumeStatus == ResumeStatus::kSurpassedToken) {
        return pSource->getNext();
    }

    while (_resumeStatus != ResumeStatus::kSurpassedToken) {
        // The underlying oplog scan will throw OplogQueryMinTsMissing if the minTs in the change
        // stream filter has fallen off the oplog. Catch this and throw a more explanatory error.
        auto nextInput = [this]() {
            try {
                return pSource->getNext();
            } catch (const ExceptionFor<ErrorCodes::OplogQueryMinTsMissing>&) {
                uasserted(ErrorCodes::ChangeStreamHistoryLost,
                          "Resume of change stream was not possible, as the resume point may no "
                          "longer be in the oplog.");
            }
        }();

        // If we hit EOF, return it immediately.
        if (!nextInput.isAdvanced()) {
            return nextInput;
        }

        // Determine whether the current event sorts before, equal to or after the resume token.
        _resumeStatus =
            compareAgainstClientResumeToken(pExpCtx, nextInput.getDocument(), _tokenFromClient);
        switch (_resumeStatus) {
            case ResumeStatus::kCheckNextDoc:
                // If the result was kCheckNextDoc, we are resumable but must swallow this event.
                continue;
            case ResumeStatus::kSurpassedToken:
                // In this case the resume token wasn't found; it may be on another shard. However,
                // since the oplog scan did not throw, we know that we are resumable. Fall through
                // into the following case and return the document.
            case ResumeStatus::kFoundToken:
                // We found the actual token! Return the doc so DSEnsureResumeTokenPresent sees it.
                return nextInput;
        }
    }
    MONGO_UNREACHABLE;
}
```

这个 getNext 函数的意思是：     
- oplog被冲，不可能恢复时，抛出ErrorCodes::ChangeStreamHistoryLost异常。    
- 如果可以恢复:
    - 能找到这个token，就从这个token开始吐出事件；
    - 找不到这个token，说明这个token很可能是其他shard生成的，就从大于这个token的地方开始吐出事件。

不返回重复事件。

### ChangeStreamProxyStage    
```
boost::optional<BSONObj> ChangeStreamProxyStage::getNextBson() {
    if (auto next = _pipeline->getNext()) {
        auto nextBSON = _validateAndConvertToBSON(*next);
        _latestOplogTimestamp = PipelineD::getLatestOplogTimestamp(_pipeline.get());
        _postBatchResumeToken = next->getSortKeyMetaField();
        _setSpeculativeReadTimestamp();
        return nextBSON;
    }

    // We ran out of results to return. 
    auto highWaterMark = PipelineD::getLatestOplogTimestamp(_pipeline.get());
    if (highWaterMark > _latestOplogTimestamp) {
        auto token = ResumeToken::makeHighWaterMarkToken(highWaterMark);
        _postBatchResumeToken = token.toDocument().toBson();
        _latestOplogTimestamp = highWaterMark;
        _setSpeculativeReadTimestamp();
    }
    return boost::none;
}
```
PipelineD::getLatestOplogTimestamp() 返回的就是 DocumentSourceCursor::_latestOplogTimestamp。

这个 stage 做的主要工作就是维护 postBatchResumeToken 变量。postBatchResumeToken 就是用作断点续传的 resume token。
- 返回每个文档之前，都把postBatchResumeToken更新成这个文档的resume token.
- 没有文档要返回时，就把 postBatchResumeToken 设置成一个high-water-mark resume token，它的 ts 是 collectionScan 里面的 latest oplog timestamp。

为什么要有 high-water-mark resume token？    
想象一个场景——客户端 watch 一个 collection，但是这个 collection 一直没有写入，如果 postBatchResumeToken 一直不推进，那等客户端拿着很旧的 resume token 续传时，发现无法恢复。high-water-mark resume token 的意义就是即使没有客户端关注的事件产生，也能推进同步点。    

## 3.5 mongos汇聚数据返回给 driver


# 4. ChangeStream 的问题以及应对方案


# 5. 总结


