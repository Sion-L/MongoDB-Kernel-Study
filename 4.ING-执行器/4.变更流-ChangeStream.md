# 1. 什么是 ChangeStream
ChangeStream是 MongoDB 提供的 CDC 解决方案。
```
Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog.
```

使用传统的 tailing oplog 在 CDC（change data capture）场景下非常不便：    
1. oplog理论上仅供内部 replication 使用：    
    - 比如 noop oplog、migrate 对用户来说没有意义；
    - 比如 5.0 引入了 diff 格式，oplog 格式发生了变化，而且没有文档说明；
    - 比如事务相关的 oplog 有一系列的解析规则；
    - ...
2. 分片集群下每个 shard 都有自己的 oplog，排序和断点续传复杂。
3. 如果只需要关注某个表的数据变更，也要把oplog全部读出来，然后在客户端过滤。
4. 扩展性差，oplog 只是普通的数据，定制化逻辑需要客户端实现，比如post-image、pre-image 等等。

```
Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them.
```

Change stream 支持库表级别的监听，相比 tailing oplog，易用性更强、而且节省了网络传输和客户端的资源消耗。

```
Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will.
```

有着 aggregate 框架的加成，change stream 扩展性更强。比如支持获取 PreImages 和 PostImages，也支持 UpdateLookUp。

Change stream 的缺点：    
- 早期内核版本实现不完善。自3.6版本引入，后续版本对DDL操作支持不够，到6.0版本有所改善。
- 资源开销较大，性能跟不上。   

# 2. 总体流程
如图所示：    

<TODO  总体流程图>

总体流程为：    
1. 客户端调用driver的watch函数。    
2. driver向mongos发送aggregate请求。    
3. mongos向各个shard 发送aggregate请求。    
4. 各个shard把各自的cursor返回给mongos。    
5. mongos返回一个cursor给driver。    
6. driver返回对象给客户端。    
7. 客户端后续调用driver的next函数。    
8. driver发送getMore给mongos，请求更多数据。    
9. mongos向各个shard发送getMore请求。    
10. 各个shard返回数据给mongos。    
11. mongos返回数据给driver。    
12. driver返回数据给客户端。    

客户端程序如下所示：    
```
func run(cli *mongo.Client, ts primitive.Timestamp) error {
	cs, err := clie.Watch(context.Background(), bson.A{}, &options.ChangeStreamOptions{
		StartAtOperationTime: &ts,
	})
	if err != nil {
		return err
	}

	for cs.Next(context.Background()) {
		var doc bson.D
		err = bson.Unmarshal(cs.Current, &doc)
		if err != nil {
			return err
		}
       // other code to consume doc
	}

    return cs.Err()
}
```

后面主要分享mongodb内核对change stream相关的处理，也上面的步骤2~11。基于 [4.2](https://github.com/mongodb/mongo/tree/v4.2) 代码进行分析。

带着问题看下面的讲解：     
- 如何汇聚、排序多个shard上的事件？    
- 如何做断点续传？不同于tailing oplog，要考虑到长时间没有事件的情况。    
- 如何处理事务相关的事件？    
- 性能差在哪？    

# 3. 详细流程
下面以一个真实的 change stream 使用场景为例，遵循时间顺序依次讲解 mongos 和 mongod 的处理逻辑。

## 3.1 mongos处理driver发来的aggregate请求
请求：    
```
{
  aggregate: 1,
  pipeline: [
    {
      '$changeStream': {
        allChangesForCluster: true,
        startAtOperationTime: Timestamp({ t: 1698997099, i: 0 }),
      }
    }
  ],
  cursor: {},
  lsid: { id: new UUID("14265384-a9d8-4aa5-a60c-f95d499ea833") },
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698997490, i: 1 }),
    signature: {
      hash: Binary("000000000000000000000000000000"),
      keyId: Long("0")
    }
  },
  '$db': 'admin'
}
```

mongos返回一个cursor id给driver:    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
}
```

下面来探究mongos是如何处理这个请求的。

### 代码处理流程
```
ClusterPipelineCommand::run    src/mongo/s/commands/cluster_pipeline_cmd.cpp   
    ClusterAggregate::runAggregate    src/mongo/s/query/cluster_aggregate.cpp  
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        ClusterAggregate::runAggregate
            Pipeline::parse                                     // 构建pipeline
                Pipeline::parseTopLevelOrFacetPipeline
                    DocumentSource::parse
                        DocumentSourceChangeStream::createFromBson
                            buildPipeline
            sharded_agg_helpers::dispatchShardPipeline
                mustRunOnAllShards
                cluster_aggregation_planner::splitPipeline         // 分离出需要在shard上运行的pipeline
                    findSplitPoint
                createCommandForTargetedShards
                    genericTransformForShards
                establishShardCursors                              // 在每个shard上创建cursor
                    establishCursors
            dispatchMergingPipeline
                cluster_aggregation_planner::addMergeCursorsSource // 创建在mongos运行的pipeline
                    DocumentSourceMergeCursors::create
                runPipelineOnMongoS                                // 在mongos上运行pipeline
                    establishMergingMongosCursor                   // 建立一个总的cursor，用来吐出汇聚后的数据
                        cluster_aggregation_planner::buildClusterCursor
                        ccc->next(kInitialFind)
                            RouterStagePipeline::next
                                Pipeline::getNext
                                    DocumentSourceCloseCursor::getNext
                                        DocumentSourceUpdateOnAddShard::getNext
                                            DocumentSourceMergeCursors::getNext          // return EOF
                                                DocumentSourceMergeCursors::populateMerger
                                                    BlockingResultsMerger::BlockingResultsMerger
                                                        AsyncResultsMerger::AsyncResultsMerger
                                                BlockingResultsMerger::next
                                                    BlockingResultsMerger::awaitNextWithTimeout
                        setPostBatchResumeToken(ccc->getPostBatchResumeToken())
                            RouterStagePipeline::getPostBatchResumeToken
                                DocumentSourceMergeCursors::getHighWaterMark
                                    BlockingResultsMerger::getHighWaterMark
                                        AsyncResultsMerger::getHighWaterMark
                        registerCursor                               // 保存cursor信息，等待getMore请求
```

最初构建的pipeline，是由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- DocumentSourceCloseCursor

前 4 个 stage 是 shard stage，不会运行在 mongos 上。对于 mongos 来说这四个 stage 的意义就是生成发送给 shard 的命令，之后会把这几个 stage 丢弃。DocumentSourceCloseCursor 是 merge stage，要在 mongos 上做，merge 的时候以 _id._data 的值排序。      

cluster_aggregation_planner::addMergeCursorsSource 函数中，真正建立了要在mongos运行的pipeline（数据从上往下流动）：    
- DocumentSourceMergeCursors
- DocumentSourceUpdateOnAddShard
- DocumentSourceCloseCursor

建立好 pipeline 之后，会调用 getNext 运行一下，初始化各个stage，从各个 shard 拿到 cursor id 和 post batch resume token，之后返回 EOF。    

## 3.2 mongod 处理 mongos 发来的 aggregate 请求
mongos 向 mongod 发送的 aggregate 请求如下（pipeline 跟driver 发送给 mongos 的 pipeline 一样）：      
<TODO 补充图片>

mongod 把 cursor id 返回给 mongos :    
```
{
  cursor: {
    firstBatch: {},
    postBatchResumeToken: {_data: "8200000001000000002B0229296E04"},
    id: 9208559579072114181,
    ns: admin.$cmd.aggregate
  },
  $_internalLatestOplogTimestamp: Timestamp({ t: 1698997500, i: 1 }),
  ok: 1,
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
  $gleStats: {
    lastOpTime: Timestamp({ t: 1234356, i: 0 }),
    electionId: ObjectID("7fffffff0000000000000001")
  },
  lastCommittedOpTime: Timestamp({ t: 1698997500, i: 1 }),
  $configServerState: {
    opTime: { ts: Timestamp({ t: 1698998143, i: 1 }), t: Long("1") }
  },
  $clusterTime: {
    clusterTime: Timestamp({ t: 1698998143, i: 1 }),
    signature: {
      hash: Binary("00000000000000000000000"),
      keyId: Long("0")
    }
  }
  operationTime: Timestamp({ t: 1698997500, i: 1 }),
}
```

### 代码处理流程
```
PipelineCommand::Invocation::run
    runAggregate         src/mongo/db/commands/run_aggregate.cpp
        LiteParsedPipeline::LiteParsedPipeline
            LiteParsedDocumentSource::parse
                DocumentSourceChangeStream::LiteParsed::parse
        Pipeline::parse                                      // 构建pipeline，mongos走的也是这个
            Pipeline::parseTopLevelOrFacetPipeline
                DocumentSource::parse
                    DocumentSourceChangeStream::createFromBson
                        buildPipeline
                            DocumentSourceChangeStream::buildMatchFilter
        PipelineD::buildInnerQueryExecutor                  // 创建aggregate框架下层的executor
            PipelineD::buildInnerQueryExecutorGeneric
                PipelineD::prepareExecutor                  // 生成对local.oplog.rs的执行计划
                    attemptToGetExecutor
                        getExecutorFind
                            _getExecutorFind
                                getOplogStartHack
                                    WiredTigerRecordStore::oplogStartHack // 给出collection scan的起始位置
        PipelineD::attachInnerQueryExecutorToPipeline      
            DocumentSourceCursor::create
            PipelineD::addCursorSource
                Pipeline::addInitialSource
        createOuterPipelineProxyExecutor                    // 为change stream加一个proxy stage
            ChangeStreamProxyStage::ChangeStreamProxyStage
                PipelineProxyStage::PipelineProxyStage
        registerCursor                                      // 保存cursor
        handleCursorCommand
            setPostBatchResumeToken(exec->getPostBatchResumeToken())
                PlanExecutorImpl::getPostBatchResumeToken
                    ChangeStreamProxyStage::getPostBatchResumeToken
```

跟 mongos 差不多，最初构建的 pipeline 由如下几个 stage 组成：    
- DocumentSourceOplogMatch  （不要与DocumentSourceMatch类混淆）
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability

最后经过调整，pipeline 是这样：    
- CollectionScan
- DocumentSourceCursor
- DocumentSourceChangeStreamTransform
- DocumentSourceCheckInvalidate
- DocumentSourceCheckResumability
- ChangeStreamProxyStage

# 4. ChangeStream 的问题以及应对方案


# 5. 总结


