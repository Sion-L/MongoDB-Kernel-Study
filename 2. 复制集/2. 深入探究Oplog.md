# 1. 导语
Oplog 是 MongoDB 中一个特殊的、存储了变更记录的表，在主从数据复制、数据持久化保障、按时间点备份回档、changeStream 变更流、操作追溯等各方面都有着举足轻重的作用。因此，其实现的好不好，直接关系到了 MongoDB 的产品竞争力。   

在上文介绍 MongoRaft 时，已经说明了 oplog 的复制流程以及在数据持久化中发挥的作用，本文不再赘述。但是关于 oplog仍有很多其他方面的细节并未做分析。因此，本文会聚焦 oplog 实现中的核心问题进行分析，包括以下内容：   
1. Oplog 格式。分析 oplog 中每条文档会包含哪些字段，以及各自发挥的作用。   
2. 幂等性保证。存储系统中日志对于幂等性的需求并不罕见，那么MongoDB 在支持了各种操作算子的背景下，如何保证 oplog 的幂等性呢？   
3. 可见性判断。主从节点上都是并发写 oplog，都存在临时的 oplog 空洞，如何保证存在空洞的 oplog 不会被拉取到呢？   
4. 性能。分析 oplog 表如何在无索引的情况下保证高效的查询性能，以及如何优化oplog 表的删除机制，来避免对用户写入性能的影响。   
5. 回放。分析 oplog 的回放流程，探讨 MongoDB 内核以及外部工具如何使用 oplog。   

不同内核版本之间可能在 oplog 格式上存在差异，比如 4.2 版本引入了分布式事务相关的 oplog，取消了 hash 值作为每条 oplog 的 ID。   
本文的讨论基于 4.2.24 版本内核代码。

# 2. 格式
Oplog 是 local 库中一个特殊的固定大小的表，逻辑上可以看做是一个环形队列。里面的文档按照时间戳进行组织，当空间不足时，会自动删除最老的文档。和普通的用户表一样， oplog 表也支持各种过滤条件的查询、排序、映射等操作。   
Oplog 中每一条文档（oplog entry）记录了一次变更操作，文档中包含的字段相对比较固定，可以参考 [oplog_entry.idl](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_entry.idl) 代码中的定义，列举如下：    
|字段名|类型|是否可选|备注|
|:--|:--|:--|:--|
|ts|timestamp|必选|时间戳，由 int32 的 unix 秒级时钟和 int32 的秒内自增计数器共同组成。可以当做一个 int64 整数，是 **oplog 表的主键**。|
|t|long|必选|MongoRaft 协议中的 term。未引入 MongoRaft 协议的老版本没有这个字段。|
|h|long|4.2 版本开始逐步废弃|这条 oplog 文档的摘要。兼容4.0以及更老的版本，新版本这个字段为 0。|
|v|int64|必选|Oplog entry 格式的版本，默认为1。|
|wall|date|必选|机器的毫秒级时间。|
|op|string|必选|表示命令的类型，包括："i" -> insert 操作，"u" -> update 操作，"d" -> delete 操作， “c” -> 其他命令，比如建表，"n" -> noop, 没有变更操作时进行时钟推进。|
|ns|string|必选|表空间，一般是 “库名.表名”。如果只涉及库，不涉及具体的表，则为“库名.$cmd”。|
|ui|uuid|可选|表的 UUID 信息，如果对表进行了重命名，其 UUID 不变。|
|o|object|必选|执行的操作。比如对于 insert 操作，这个字段就包含了插入的文档。|
|o2|object|可选|操作的附加信息。比如对于 update 操作，除了执行的操作之外，还需要查询条件（对哪条文档执行的操作）。|
|b|bool|可选|是否为 upsert（update 自动转 insert），兼容 3.6 及更老的版本。|
|fromMigrate|bool|可选|是否为 chunk 迁移产生的，存在于分片集群中。|
|stmtId|int32|可选|产生这个oplog entry的 statementId, 一个事务可能对应多个变更操作，每个操作有独立的 statementId。|
|prevOpTime|optime|可选|同一个事务中，上一条 oplog 的 optime。通过这项信息，可以把事务的多个 oplog 串联起来。|
|preImageOpTime|optime|可选|prevImage 信息存储在另外一条 oplog entry 中，这个字段存的是其“指针”信息。<br>**一般存在于 retryWrites 模式下， findAndModify 命令返回旧值的场景。普通的读写操作不会有这条记录。**|
|postImageOpTime|optime|可选|本次 update 操作执行之后，文档是啥样（postImage）。postImage 信息存储在另外一条 oplog entry 中，这个字段存的是其“指针”信息。<br>**一般存在于 retryWrites 模式下， findAndModify 命令返回新值的场景。普通的读写操作不会有这条记录。**|
|needsRetryImage|string|可选|可以是 "preImage" 或者 "postImage"。|

# 3. 幂等性
## 3.1 为什么要保证幂等性
当我们使用 mongdump 工具或者 find+getmore 命令进行全表扫描时，默认情况下读到的并非某个时间点的一致性数据（除非在高版本内核主动指定 readConcern 为 snapshot， 不过这种方式会带来资源消耗风险）。为了防止一个批量扫描请求长时间锁住太多资源（维护老版本的开销），MongoDB 的执行模式为定期 yield 释放老 snapshot，然后创建新 snapshot 继续扫描。参考《[Tunable Consistency in MongoDB](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)》论文中的说明：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/67f71ba0-e233-4866-aeb8-f990483a96b1" width=350>
</p>

定期释放snapshot 能够降低长期维护老数据版本造成的内存压力，但是带来的数据一致性问题需要通过其他方式来解决。对于 mongodump 来说，可以通过[指定 --oplog 参数](https://www.mongodb.com/docs/v4.2/tutorial/backup-and-restore-tools/#create-backups-using-oplogs)来备份从扫描开始到结束这段时间的所有 oplog，来达到一致性备份的目的，示意图如下：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/903a9cec-59e8-4e12-8823-ec7b3bf48f64" width=450>
</p>

1. t1时刻，mongodump 开始全量扫描，将第 1 条文档 {a:1} 备份下来。   
2. t2 时刻，扫描到第2 条文档，刚好在此之前用户将这条文档的 b 字段进行了加 1 操作，全量备份得到 {b: 2}，对这条文档的操作会记录 oplog。   
3. t3 时刻，扫描到第 3 条文档，刚好在此之前用户将这条文档的 c 字段进行了加 2 操作，全量备份得到 {c: 3}，但与此同时对第1 条文档{a: 1}的修改不会再扫描一次。t3 时刻对第 1 条和第 3 条文档的操作都会记录 oplog。   

可以看到，全量备份结束后，全量备份数据和 t3 时刻数据库的状态并不一致。但是在回放时，如果结合 oplog 的增量回放，就能准确地将数据库回档到 t3 时刻的一致性状态。   
细心的读者可能已经发现，在回放 oplog 时可能会有操作重复执行的情况。比如上例的全量备份中，第 2 条文档已经是修改过的 {b:2}，但是在 oplog 回放时可是会执行一次这条文档的修改操作。**如果 oplog 本身不能保证幂等性，则上述操作重复执行的情况可能导致数据不正确。**  

**特别说明**   
MongoDB 还支持以下 2 种一致性备份方式：   
1. 使用 fsyncLock 命令将节点锁住，不接收任何写入。等备份完成后，使用fsyncUnlock 命令解锁。   
2. 高内核版本使用  snapshot一致性读，等备份完成后释放snapshot。但是如果备份时间比较长，会带来较大的系统压力。   


除了 mongodump，还有很多流程依赖 oplog 的幂等性，比如跨集群的 DTS 数据同步工具，内核中的 [initial sync](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md#initial-sync) 逻辑同步流程等。因此，oplog 对于幂等性的要求是毋庸置疑的。

## 3.2 如何保证幂等性
Oplog 实现幂等性的方法，就是存储变更操作后的结果，而不仅是具体的变更动作。比如对文档  {a: 1} 的 "a" 字段做加 1  操作，则 oplog 中记录 {$set: {a: 2}} 。    

对于 insert操作，在 "o" 字段中记录插入的完成文档，例如插入一条文档 {"_id" :10, "a": 5}，得到的 oplog 为：   
```
{ "ts" : Timestamp(1702090192, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "wall" : ISODate("2023-12-09T02:49:52.960Z"), "o" : { "_id" : 10, "a" : 5 } }
```

对于 delete操作，只需要记录文档的 "_id" 即可，例如删除上一步插入的文档，得到的 oplog 为：    
```
{ "ts" : Timestamp(1702090210, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "d", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "wall" : ISODate("2023-12-09T02:50:10.013Z"), "o" : { "_id" : 10 } }
```

对于 update 操作，则情况要复杂一点。 MongoDB 支持多种 update 方式：   
- [Replace](https://www.mongodb.com/docs/v4.2/tutorial/update-documents/) 方式，客户端将要更新的文档准备好，然后使用**整文档替换**的方式进行 update，有点把  MongoDB 当作 KV 存储使用的感觉。比如将文档 {"_id":0, "a":1, "b":2, "c":3} 替换为 {"_id":0, "a":1, "b":2, "c":4}，则执行 replace 更新的方式是 db.coll1.update({"_id":0}, {"_id":0, "a":1, "b":2, "c":4})， 得到的 oplog 如下， "o" 字段中包含了整条文档，而不只是修改过的 "c" 字段：
```
{ "ts" : Timestamp(1702090827, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "o2" : { "_id" : 0 }, "wall" : ISODate("2023-12-09T03:00:27.142Z"), "o" : { "_id" : 0, "a" : 1, "b" : 2, "c" : 4 } }
```
- [Pipeline](https://www.mongodb.com/docs/v4.2/tutorial/update-documents-with-aggregation-pipeline/) 方式，4.2 内核版本开始支持，支持更高级的语法在MongoDB 服务侧对文档进行更新。这种方式下，oplog 中存储的也是整条新文档，和 Replace 方式相同。
- [Operator](https://www.mongodb.com/docs/v4.2/reference/operator/update/) 方式。MongoDB 支持十几个算子，在服务侧对指定文档的一个或多个字段进行**局部更新**。这种方式下，oplog 中只需要存储涉及到的字段的操作结果。相比 Replace 方式，Operator 方式的网络传输流量更小、占用的 oplog 和 journal 空间更小。比如将文档 {"_id":0, "a":1, "b":2, "c":3} 中的 "c" 字段改成 5，则执行局部更新的命令是  db.coll1.update({"_id":0}, {$set:{"c":5}})，得到的 oplog 如下，只包含修改过的字段：   
```
{ "ts" : Timestamp(1702091364, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "o2" : { "_id" : 0 }, "wall" : ISODate("2023-12-09T03:09:24.932Z"), "o" : { "$v" : 1, "$set" : { "c" : 5 } } }
```

Operator 方式下的 oplog 构造流程相对较复杂。为了弄清其 oplog 构造流程，我们需要先看看 operator 更新条件是如何执行的。   
由于 MongoDB 的文档模型中存在 Object 和 Array 嵌套，MongoDB 在请求解析阶段会先将更新语句解析成一个前缀树，比如对嵌套字段 "a.b.c" 和 "a.b.d" 的更新具有相同的前缀 "a.b"。前缀树的根节点就是对某个字段的具体变更操作，比如 $inc $mul 算数操作等。    
参考[内核代码](https://github.com/mongodb/mongo/tree/r4.2.24/src/mongo/db/update)，前缀树的节点定义关系如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1418abba-22f9-45fd-a11c-0bcffe23efdc" width=800>
</p>

以 update(<queryFilter>, {$set:{"a.b": 5, "c":6}, $inc:{"a.c":1}}) 这条语句的执行流程为例，MongoDB 首先会根据解析更新操作生成如下前缀树（updateTree）：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/3e37c3ed-4c57-4a9a-9c4a-9fab9708f670" width=400>
</p>

执行计划的构造流程为：先使用 queryFilter 构造 [queryStage](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L1086-L1094)，然后基于此构造 [updateStage](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L1099-L1103)。在 queryStage 阶段查找出需要修改的文档，然后在 updateStage 阶段对文档进行修改。    
而 updateStage 的执行流程，本质上就是对上述 updateTree 中各个叶子节点执行 apply 的流程。在执行 apply 操作时，会传入 logBuilder 进行 oplog 的流式构造。以 {$inc: {"a.c": 1}} 为例，会在 ArithmeticNode 的 apply 流程中对 "a.c" 字段进行 “加1” 操作，然后将操作后的 newValue 传递给 logBuilder 转换为 {$set : {"a.c": newValue}} 合并到最终的 oplog 中。   

# 4. 可见性
设想在 primary节点上并发执行了 2 个插入请求的场景：   
1. 请求A 先执行，此时获得的时间戳版本是 t1。   
2. 请求 B 后执行，此时获得的时间戳版本是 t2。   
3. B 先提交，提交时会按 t2 时间戳写 oplog 和 journal。   
4. A 后提交，提交时会按 t1 时间戳写 oplog 和 journal。      

假设有另外一个 secondary 节点一直从 primary 节点拉取 oplog 进行同步，在第 3 步看到了时间戳为 t2 的 oplog 并拉过去回放。然后又按照（时间戳 >= t2）拉取下一批日志，此时第 4 步时间戳为 t1的 oplog 再也不会被拉取到。**此时主从复制就出现了错误**。   

为了弄明白为什么会有上述风险，需要先说明以下背景。   
1. **写请求为什么要伴随一个时间戳？**    
MongoDB 通过时间戳来作为数据版本进行可见性判断。包括事务以及普通的写请求，都会分配一个时间戳作为该请求的数据版本。   
和事务 ID 一样，时间戳通过[算法](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98-L142)保证即使出现物理时钟回调、主从切换等问题也能严格单调递增。   
2. **为什么写请求的时间戳要和 oplog 中的时间戳绑定？**   
主从复制除了保证数据一样，也要保证数据版本信息相同。否则如果出现主从切换，或者客户端选择到 secondary 节点读取数据，就会遇到数据版本混乱的问题。   
显然，将时间戳信息放在 oplog 中传递到 secondary 节点，是最优的办法。   

那么，如何避免日志空洞造成的复制问题呢？   
能想到的做法有：   
1. 在 primary 节点上保证不将有空洞的日志“提前泄露”给其他节点。   
2. 在 secondary 节点回放日志前，先检查日志不存在空洞。如果存在空洞，要等待日志中的空洞被填补后再回放。   

以上 2 种解决方案各有优劣。第 1 种方案能够保证 secondary 节点拉倒的日志就是能回放的，但是 primary 节点上如果有个请求执行太慢，导致空洞的时间存在太长，则空洞后面的日志迟迟得不到复制。第 2 种方案能够第一时间拉取到最新的日志，但是需要 secondary 节点上执行空洞检查流程，而且需要日志具备空洞检查的能力（比如每条日志都有上一条日志的“指针”）。

MongoDB 实现的是第 1 种方案。由于 oplog 在设计上也没有“指针”将前后日志串联起来，因此也不具备实现第 2 种方案的能力。   
另外需要注意的是，MongoDB 支持链式复制，secondary 节点也能作为其他节点的同步源。因此，primary 和 secondary 节点都需要考虑 oplog 的可见性问题。   

下面结合代码，分别说明 primary 和 secondary 上 oplog 可见性的实现逻辑。   

## 4.1 Primary 节点的 oplog 可见性
依旧使用上面并发插入的例子，分析用户线程并发写 primary 节点时，如何处理 oplog 可见性。执行流程为：   
1. 写请求 A 开始执行，调用 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98-L142) 分配时钟（[全局锁](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/local_oplog_info.cpp#L124)内）。   
2. Logical Clock 模块给请求 A 分配时钟 t1, 并[注册](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1916)到 WiredTiger 存储引擎，声明有一个 t1 时钟的事务开始执行（全局锁内）。   
3. 写请求 A 继续执行。   
4. 写请求 B 开始执行，调用 LogicalClock::reserveTicks 分配时钟（全局锁内）。   
5. Logical Clock 模块给请求 B 分配时钟 t2, 并注册到 WiredTiger 存储引擎，声明有一个 t2 时钟的事务开始执行（全局锁内）。   
6. 写请求 B 继续执行。   
7. 写请求 B 提交，提交的时候会使用 t2 写 oplog。   
8. 写请求 A 提交，提交的时候会使用 t1 写 oplog。   

<TODO 架构图>

与此同时，存在一个 [WTOplogJournalThread](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.cpp#L152-L230) 守护线程。在请求提交时，会通知这个线程刷 journal 并更新oplog可见性。   
WTOplogJournalThread 线程工作时，会调用存储引擎的 [query_timestamp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L2052) 接口获取 "all_durable" 时间戳，然后将这个时间戳作为当前 oplog 可见的最大时间戳。   
对应到上图的 t3 时刻，虽然请求 B 提交并以 t2 写入了 oplog，但是请求 A 还没有提交。因此，存储引擎的 query_timestamp 接口返回的 "all_durable" 还是 t0。所以请求 B的 oplog 还是不可见，直到请求 A 也完成了提交。通过这种方式，避免了存在空洞的 oplog 被错误读取。   

另外需要说明的是，4.0 版本采用 "[all_committed](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.h#L46)" 时间来设置 oplog 可见性。 而在 4.2 版本开始引入了 2 阶段事务后，为了避免 prepare 阶段的 oplog 同步到 secondary 节点后又 abort，采用了 "[all_durable](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.h#L44)" 时间来设置可见性。   
关于 2 者的区别可以参考[代码注释](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/storage_engine.h#L580-L589)：
```
The all_durable timestamp only includes non-prepared transactions that have been given a commit_timestamp and prepared transactions that have been given a durable_timestamp. Previously, the deprecated all_committed timestamp would also include prepared transactions that were prepared but not committed which could make the stable timestamp briefly jump back.
```

当其他节点到 primary 上读取 oplog 时，会[获取当前的 oplog 可见时间](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L536)，只[读取可见范围](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1978)内的 oplog：   
```
// WiredTigerRecoveryUnit::_txnOpen 获取 oplog 可见时间  
if (_isOplogReader) {
    _oplogVisibleTs = static_cast<std::int64_t>(_oplogManager->getOplogReadTimestamp());
}

// WiredTigerRecordStoreCursorBase::next 只读取可见范围内的 oplog
// 这里的 id 就是 oplog 的 recordId，也就是 ts 字段
if (_oplogVisibleTs && id.repr() > *_oplogVisibleTs) { 
    _eof = true;
    return {};
}
```

## 4.2 Secondary 节点的 oplog 可见性
Secondary 的 oplog 可见性处理要更加复杂，我们可以从以下几方面来看：   
1. MongoRaft 一致性协议保证了每个副本的数据都是一致的。这里的一致性除了要保证每条文档的内容一致之外，还要保证每条文档的版本一致。   
2. 每条文档的版本通过时间戳来标识。所以 secondary 节点在写数据时，不能和 primary 一样在自己节点上维护一个逻辑时钟模块来单独推进数据版本，而是必须使用和 primary 节点一样的时间戳。而这个时间戳，就是 oplog 中的 “ts” 字段。所有的副本都统一使用 primary 节点写到 oplog 中的时间戳来执行写入。   
3. 在上一篇文章，我们提到，secondary通过 batch 内乱序回放来保证 oplog 的回放性能。因此，如果还是按照 primary 节点上的那套获取存储引擎 "all_durable" 时间戳的机制，无法正确的处理 oplog 空洞问题。   

因此，除了包含 primary 节点中使用的存储引擎 "all_durable" 时间戳推进机制之外，Secondary 节点上还提供了额外的“校准”机制：   
- 当一个 oplog batch 整体回放完成之后，[主动推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L876) oplog 可见性。推进的依据就是 oplog batch 最后一条的时间戳。
- 其他节点过来读 oplog 时，严格遵循 snapshot 读。而 snapshot 使用的时间戳，是当前 oplog batch 回放之前的时间戳（也就是每次 oplog batch整体回放完成后，才推进的 [lastAppliedTimestamp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L550)）。

关于上述问题，可以参考[代码注释](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L663-L669)：   
```
On secondaries, however, the all_durable time, as computed by the storage engine, can advance before oplog application completes a batch. This is because the all_durable time is only computed correctly if the storage engine is informed of commit timestamps in increasing order. Because oplog application processes a batch of oplog entries out of order, the timestamping requirement is not satisfied. Secondaries, however, only update the last applied time after a batch completes. Thus last applied is a valid no-overlap point on secondaries.
```

# 5. 性能优化

# 6. 回放

# 7. 总结

# 参考文档
