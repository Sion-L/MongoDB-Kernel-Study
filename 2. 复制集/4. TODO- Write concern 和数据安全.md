# 1. 导语
当应用程序向分布式存储系统执行写入请求时，常关注的问题有：   
1. 请求执行成功时，复制到了多少个节点？
2. 如果出现了断电重启，执行成功的操作是否会丢失？
3. 请求执行需要花多长时间？超时之后服务端是否主动终止？

不同的业务场景对应不同的需求。比如有些业务更看重延迟，希望尽快返回结果，即使异常情况出现一些数据丢失也能接受。但是有些业务更看重数据持久化，比如涉及到金钱的业务，即使耗时久一些，也不能出现数据丢失的问题。   

MongoDB 通过 writeConcern 实现了可调持久性，能够满足各类业务场景。   

# 2. WriteConcern 配置项
WriteConcern 包含 3 个配置参数，列举如下：   
|参数|类型|使用方法|
|:--|:--|:--|
|w|int/string|如果指定整数，则表示请求在多少个节点上执行成功后再返回（只统计有投票权的节点，包括 primary 自己）。<br>如果指定字符串，可以指定 majority 表示请求在大多数有投票权的节点上执行完成。也可以指定 tags 来自定义复制策略，比如 3 节点的副本集，其中 2 个节点在北京机房，tags 分别是  { beijing: 'haidian' } 和 { beijing: 'chaoyang' }，另外 1 个节点在上海，tags 是 { shanghai: 'jingan' }。<br>可以在settings.getLastErrorModes 中自定义一种模式为 { multiAZ: { beijing: 1, shanghai: 1 } }.<br>然后在请求中指定 writeConcern 为 {w:"multiAZ", j:true} 则保证请求成功返回时，北京和上海至少都有 1 个节点完成了数据写入。|
|wtimeoutMS|int|表示 primary 节点请求执行完成后，等待（需要统计的）其他节点完成的超时时间，以毫秒为单位。|
|j|bool|表示每个节点写入成功是否以为这 journal 已经做好了持久化。如果 w=majority, 则 j 默认为 true， 可以通过 writeConcernMajorityJournalDefault 参数来控制|

WriteConcern 可以配置在服务端（server）、客户端连接(client)、会话(session，如果有)、事务（transaction，如果有）以及每个请求(operation)内部。如果在每一级都有配置，则生效的优先级为：请求 > 连接 > 服务端默认配置。如果在某一级没有明确地进行配置，则自动继承上一级的配置。

## 2.1 服务端默认配置
使用 mongo 客户端连上副本集后，通过 rs.conf() 可以查看 writeConcern 的默认配置：      
```
> rs.conf().settings.getLastErrorDefaults
```
可以通过 reconfig 方式修改默认配置。   

从 4.4 版本开始，可以使用 [setDefaultRWConcern](https://www.mongodb.com/docs/v4.4/reference/command/setDefaultRWConcern/) 命令进行服务端的默认配置，并使用 [getDefaultRWConcern](https://www.mongodb.com/docs/v4.4/reference/command/getDefaultRWConcern/) 获取配置。    

## 2.2 连接时设置
在使用 mongo url 连接实例时，指定 writeConcern 参数。例如：     
```
mongosh "mongodb://<user>:<passwd>@localhost:27017/admin?w=majority&wtimeoutMS=5000&journal=true"
```

配置参数参考[官方文档](https://www.mongodb.com/docs/manual/reference/connection-string/#write-concern-options)。   

## 2.3 请求侧设置
可以直接在写请求中指定 writeConcern, 常见的 insert、update、delete、findAndModify 操作都支持在请求内部指定。   
例如在 insert 请求中指定：   
```
db.runCommand({insert:"coll1", documents:[{b:9}], writeConcern:{w:"majority", wtimeout:20000, j:true}})
```

或者在 mongosh 中，执行：   
```
db.coll.insert({b:9},{writeConcern:{w:"majority", wtimeout: 20000, j: true}})
```


## 2.4 创建 session 或者 transaction 时设置
如果使用了 logical session, 可以在创建 session 时通过 [sessionOptions](https://github.com/mongodb/mongo-go-driver/blob/v1/mongo/options/sessionoptions.go) 设置 writeConcern。    
如果使用了事务操作，可以在开启事务时通过 [transactionOptions](https://github.com/mongodb/mongo-go-driver/blob/v1/mongo/options/transactionoptions.go) 设置 writeConcern。     

## 2.5 使用注意事项
虽然 MongoDB 提供了非常灵活的多级配置，但是个人建议在工程时间中，最好是在请求级别给出明确的 writeConcern 设置。这样可以避免一些不必要的困扰：     
- 服务端默认配置可能随着版本的不同而发生变化。比如 5.0 内核版本将 writeConcern 的默认配置从 w=1 调整为了 w=majority，如果你对服务端的内核版本进行了升级，可能会感觉数据库怎么突然变慢了。   
- 在 url 中进行默认配置也有可能不符合预期。以官方的 mongo 客户端为例，最新的 mongosh 是能使用 url 中的 writeConcern 配置的，但是老版本的 mongo shell 并不支持（实测 4.2 版本）。比如，同样是 mongosh，执行下面 2 条看似一样的语句，效果也是不一样的：  
```
1. 使用带 writeConcern 配置的 url 连接数据库
mongosh "mongodb://<user>:<passwd>@localhost:27017/admin?w=majority&wtimeoutMS=15000&journal=true"

2. 使用 mongosh 封装的命令执行 insert， 是能够继承 url 中的 writeConcern 配置的
>db.coll1.insert({b:1})

3. 使用原生命令，则不能继承 url 中的 writeConcern 配置
> db.runCommand({"insert":"coll1", documents:[{b:1}]})
```
    
另外需要特别说明的是，对于多文档事务，应该将 writeConcern 设置在事务侧，而不是对事务中的每个子请求单独设置。    

# 3. 执行流程分析
WriteConcern 的等待和判断逻辑由 primary 节点来执行。如果使用的是分片集群，客户端设置的 writeConcern 也会通过 mongos 传递到对应 shard 的 primary 节点上执行。    
Primary 节点执行写操作的核心[代码](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/service_entry_point_common.cpp#L490)如下：    
```
bool runCommandImpl(...) {
   ...
   // 解析请求中的 writeConcern. 如果没设置，就用服务端默认配置
   extractWriteConcern(opCtx, request.body);
   ...
   // 执行写命令，包括对表、索引和 oplog的写操作
   invocation->run(opCtx, replyBuilder);
   ...
   // 等待请求复制，直到满足 writeConcern 设置的条件
   waitForWriteConcern(replyBuilder->getBodyBuilder());
}
```

如果设置了 journal 为 true, 则 waitForWriteConcern 中会调用引擎层的接口进行  journal 的 fsync 操作。然后，waitForWriteConcern 会等待其他节点的复制进度满足要求。

如果 writeConcern 要等待的节点数是 0 或者1，则 waitForWriteConcern 可以直接返回。如果要等待的节点数大于1，则会进入 [ReplicationCoordinatorImpl::_awaitReplication_inlock](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_coordinator_impl.cpp#L1722) 流程。

ReplicationCoordinatorImpl::_awaitReplication_inlock 会将当前执行的线程放到 _replicationWaiterList中并等待被唤醒。如果有复制进度更新、进程退出、stepDown或者状态配置变更等事件，则会通过条件变量唤醒等待的线程，并判断当前的日志复制进度是否满足 writeConcern 设定的条件。如果满足，则正常返回；如果不满足，则继续等待下一次被唤醒；如果出现了错误，比如切主、进程退出、等待超时等，则返回报错。核心代码如下：   
```
// 如果不满足 writeConcern 的条件，则循环等待
while (!_doneWaitingForReplication_inlock(opTime, writeConcern)) {
        // 进程退出中，返回错误
        if (_inShutdown) {
            return {ErrorCodes::ShutdownInProgress, "Replication is being shut down"};
        }

        try {
            // 等待被唤醒
            if (!opCtx->waitForConditionOrInterruptUntil(
                    condVar, *lock, wTimeoutDate, waiter.makePredicate())) {
                // 等待超时
                return {ErrorCodes::WriteConcernFailed, "waiting for replication timed out"};
            }
        } catch (const DBException& e) {
            return e.toStatus();
        }

        // 等待过程中出现了切主，返回错误
        stepdownStatus = checkForStepDown();
        if (!stepdownStatus.isOK()) {
            return stepdownStatus;
        }
    }
    // 满足条件，返回成功
    return Status::OK();
```

# 4. 常见问题
## 4.1 WriteConcern 超时后如何处理
如果在等待复制时出现了 wtimeout 超时，则服务端的最终状态是未知的：    
- 没有复制到其他节点，然后  primary 节点宕机后选出了新主。之前的 primary 重启后，数据被回滚。   
- 其他节点只是复制慢了点，并没有出现任何节点异常或者切主。写入操作会成功，服务端各个节点的数据最终一致。   

因此，客户端在超时重试时，应该尽量保证重试操作的幂等性。比如使用 upsert 、$set 等操作进行重试。   

## 4.2 Wtimeout 和 maxTimeMS 的关系
相信 MongoDB 的重度用户对 maxTimeMS 服务端超时设置都不陌生，通过设置这个参数，能够相对精确地控制请求在 MongoDB 服务端侧的执行时间，避免请求挤压。特别时对于 MongoDB 低版本（4.0及以下），如果不设置这个参数，可能客户端已经超时断开，而请求可能还在服务端执行，最终导致各种问题。    

那么 writeConcern 中的 wtimeout 参数和 maxTimeMS 又有什么关系呢？    

总结如下：
- Wtimeout 是等待复制的超时时间，并不包括请求在 primary 节点上执行的耗时。
- MaxTimeMS 是请求整体的执行时间，既包括请求在 primary 节点上的执行耗时，也包括等待复制的耗时。
- 如果 2 个参数都设置，就看那个超时时间先到达。

在 writeConcern 等待阶段，等待唤醒的超时时间会综合考虑 wtimeout 和 maxTimeMS，哪个参数的 deadline 更靠前，就用哪个。具体可以参考[OperationContext::waitForConditionOrInterruptNoAssertUntil](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/operation_context.cpp#L280) 的代码实现：    
```
if (opHasDeadline) { // 如果设置了 maxTimeMS
    // 确定最终的 deadline
    // 第 1个参数 deadline 是 wtimeout
    // 第 2 个参数 getDeadline() 是 maxTimeMS
    deadline = std::min(deadline, getDeadline());
}

const auto waitStatus = [&] {
    if (Date_t::max() == deadline) {
        Waitable::wait(_baton.get(), getServiceContext()->getPreciseClockSource(), cv, m);
        return stdx::cv_status::no_timeout;
    }
    // 使用新 deadline 等待
    return getServiceContext()->getPreciseClockSource()->waitForConditionUntil(
        cv, m, deadline, _baton.get());
}();
```

为了方便理解，我们使用 4.2 副本集做个实验。将 3 节点副本集中的 2 个 secondary节点的 slaveDelay 设置为 60 秒，然后执行以下 2 个命令，看看返回结果。   

**命令1：wtimeout 为30 秒，maxTimeMS 为 10 秒**
```
mymongo:PRIMARY> db.runCommand({insert:"coll1", documents:[{b:10}], writeConcern:{w:"majority", wtimeout:30000}, maxTimeMS:10000})
{
        "n" : 1,
        "opTime" : {
                "ts" : Timestamp(1705972878, 1),
                "t" : NumberLong(33)
        },
        "electionId" : ObjectId("7fffffff0000000000000021"),
        "ok" : 1,
        "writeConcernError" : {
                "code" : 50,
                "codeName" : "MaxTimeMSExpired",
                "errmsg" : "operation exceeded time limit"
        },
        "$clusterTime" : {
                "clusterTime" : Timestamp(1705972878, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1705972878, 1)
}
```
命令执行 10 秒后超时，并返回错误码 MaxTimeMSExpired。显然，是 maxTimeMS 的设置提前生效了。    

**命令2：wtimeout 为10 秒，maxTimeMS 为 30 秒**
```
mymongo:PRIMARY> db.runCommand({insert:"coll1", documents:[{b:10}], writeConcern:{w:"majority", wtimeout:10000}, maxTimeMS:30000})
{
        "n" : 1,
        "opTime" : {
                "ts" : Timestamp(1705973043, 1),
                "t" : NumberLong(33)
        },
        "electionId" : ObjectId("7fffffff0000000000000021"),
        "ok" : 1,
        "writeConcernError" : {
                "code" : 64,
                "codeName" : "WriteConcernFailed",
                "errmsg" : "waiting for replication timed out",
                "errInfo" : {
                        "wtimeout" : true
                }
        },
        "$clusterTime" : {
                "clusterTime" : Timestamp(1705973043, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1705973043, 1)
}
```
命令也是执行 10 秒后超时，但是这次返回的错误码是 WriteConcernFailed。显然是 wtimeout 的设置提前生效了。  

## 4.3 等待阶段会加锁吗？
在 writeConcern等待阶段不会继续持有 Global/DB/Collection 的任何锁，也不会占用 ticket（控制读写并发）、RSTL（控制副本集状态切换） 等锁资源。因为在等待阶段，已经完成了对 primary 节点的写入操作，没有继续加锁的必要。另外，writeConcern 等待阶段可能会持续较长时间，如果持有锁的话，可能会影响后续的读写请求。    
关于这一点，在[代码](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_coordinator_impl.cpp#L1728-L1733)中也有详细说明：    
```
// We should never wait for replication if we are holding any locks, because this can
// potentially block for long time while doing network activity.
if (opCtx->lockState()->isLocked()) {
    return {ErrorCodes::IllegalOperation,
            "Waiting for replication not allowed while holding a lock"};
}
```

当然，不持有锁也会有其他问题。比如在等待期间可能出现 stepDown 或者 shutDown。对于这些场景，会主动唤醒等待中的线程，然后给客户端返回对应的错误码。    

## 4.4 W = 0 不需要等 primary 写完就提前返回？
既然 w=1 表示写完primary 就返回，那 w=0 岂不是请求包丢给 primary 节点就能返回了，不需要等 primary 节点写完？    
相信很多人和我一样，初次看到 w=0 这个配置时，都会抱有上面的观点。    
然而通过代码和日志跟踪，我们会发现 w=0 和 w=1 的执行过程其实大体相同。区别主要体现在生成响应时， w=0 会[跳过](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/commands/write_commands/write_commands.cpp#L92)一些状态信息，比如插入了多少行，修改了多少行等。    


# 5. 参考文档
1. https://www.mongodb.com/docs/v6.0/reference/write-concern/
2. https://www.mongodb.com/docs/v4.4/reference/command/setDefaultRWConcern/
3. https://www.mongodb.com/docs/v4.4/reference/command/getDefaultRWConcern/
4. https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/
5. https://github.com/mongodb/mongo-go-driver/tree/v1