# 1. 导语
MongoDB 使用多个节点组成副本集保证数据高可靠以及服务高可用。对于自动容错的分布式系统来说，如何保证HA、数据一致性、高吞吐、低延迟、协议易理解（易运维）是系统设计的关键。   
一致性协议（consensus algorithm， 共识算法）就是上述问题的解决方案。

# 2. Raft 和 MongoRaft
目前业界比较流行的一致性协议有 Paxos 和 Raft。[《共识协议的技术变迁》](https://mp.weixin.qq.com/s/UY9TPMcuf0O7xS0kuXTcVw)一文中对一致性算法的发展历程进行了非常通俗易懂的描述。   
Paxos 诞生时间比较早，并使用在 Chubby，ZooKeeper 等系统中，但是协议理解起来比较复杂，学习路径有些陡峭。   
Raft 的诞生实践稍晚，相对 Paxos来说非常好理解。从 [Raft 论文](https://raft.github.io/raft.pdf)也可以看出，从设计之初就考虑了协议的通俗易懂，以及如何指导工程落地。在 etcd，redis 等开源项目中都能看到它的身影。   
正如 Raft 论文中所说，Raft 本身还有很多值得优化的地方，比如日志复制的性能问题等。另外，在实际的成功落地过程中，也有很多边界情况需要考虑。所以，业界很多系统使用的 Raft 算法时都是根据实际情况进行优化后的变种。截止目前，在 [raft.github.io](https://raft.github.io) 中登记的使用了Raft算法的开源项目已经有几十个（比如 TiKV, etcd 等），如果算上没有登记的项目（比如 MongoDB） 应该有上百个。

MongoDB 使用的一致性协议也可以看作是 Raft 协议的一个变种，同样使用了强主模式来定序，使用日志复制到大多数节点来提交请求。但是 MongoDB 在落地过程中，相比于原生 Raft 协议还是有不少改进，比如更丰富的节点状态、乱序提交、Pull-Based 复制模型、链式复制、可调一致性、Logless Reconfig 等。    
正如 MongoDB 官方论文[《Fault-Tolerant Replication with Pull-Based Consensus in MongoDB》 ](https://www.usenix.org/conference/nsdi21/presentation/zhou)中所说，MongoDB 选取 Raft 而没有基于 Paxos 很大一定程度上也是基于工程落地方面的考虑。理论上来说，如果基于 Paxos 实现也是没有问题的。   
MongoDB 一致性协议的[发展历史](https://www.usenix.org/system/files/nsdi21_slides_zhou-siyuan.pdf)如下：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/3b514626-b141-45ed-8499-f372c944489f" width=800>   
- MongoDB 1.0 是刀耕火种的时代，需要手动进行 failover.   
- MongoDB 1.6 引入了自动 failover 机制，但是是基于没有证明的私有协议。
- MongoDB 3.2 基于 Raft 重新改造了一致性协议，进行了 TLA+ 证明。同时在改造过程中保留了 MongoDB自身 的 Pull-Based 日志复制模型、链式复制等诸多优秀特性，使得MongoDB 的一致性协议在正确性、效率、性能等各方面都达到了非常高的标准。   

由于 MongoDB 官方已明确说明一致性协议基于 Raft，而且在 [《Design and Analysis of a Logless Dynamic Reconfiguration Protocol》](https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf)论文中更是将配置变更算法起名为 MongoRaftReconfig. 因此，为了描述方便，本文统一将 MongoDB 的一致性协议简称为 MongoRaft。

类比是非常好的学习方法。本文首先从原生 Raft 入手，从 Raft 论文简要了解其设计思想和流程。然后结合 MongoDB内核源码、官方论文以及github wiki，重点介绍 MongoDB 的一致性协议，并说明相比原生 Raft 所作的优化及效果。   
结合 Raft 论文对协议做的模块划分，以及我个人的理解。本文的描述将分如下几个模块阐述：   
1. 节点状态及其变迁规则。
2. 日志复制和提交。
3. 如何选主。
4. 配置变更。
5. 持久化保证。
6. 一致性保证。

# 3. 深入分析 MongoRaft
## 3.1 节点角色
Raft 论文中描述了 3 种节点角色：   
- Leader： 主节点，接收读写请求，承担日志复制，请求提交，心跳探活等任务。
- Follower：从节点，接收日志复制和应用，作为一个完整的数据副本。  
- Candidate：Follower长时间没有感知到主节点时，可以变为 Candidate 并发起选举。   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/33003281-72a3-44b5-8b3e-e826c17490e1" width=400>

此外，论文中也提到了 Follower 节点可以设置 non-voting 属性，不计入大多数提交时的副本数（Learner）。一般在新加入节点时，可以使用这个设置避免数据提交卡顿。  

MongoDB 中包含的节点角色也有主从状态。 其中主节点（Primary）支持读写，从节点（Secondary）支持读（这是和 Raft 不同之处）。另外从节点又[细分](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/member_state.h#L58-L70)为：  
- Secondary：正常状态下的从节点，支持用户读。  
- Rollback：回滚状态。常见的场景是主节点异常重启后变为从节点，需要将没有复制到其他节点的日志回滚掉。  
- Recovering：正在恢复的状态。节点启动后会短暂处于该状态，然后变成 Secondary。如果长期处于该状态，对应的场景是该节点的日志太就，无法找到合适的节点同步日志，此时一般需要清空数据后重新做全量同步。  
- Startup：刚启动时的状态。  
- Startup2：初始化全量同步的状态。对应的场景是新加入一个空节点（或者手动将某个节点的数据清空），该节点需要通过 Initial Sync 流程全量同步数据。  
- Arbiter：投票节点，不存储数据。  
此外，如果节点异常，会显示 Unknown、Down、Removed(Other) 状态。

另外 MongoDB 中的每个节点可以配置多种属性：   
- Votes：是否有投票权。所谓的“多数派提交”和“多数派选举”中的“多数”，指的就是包含有 votes 属性的节点。一个 MongoDB 副本集中最多只有7 个包含 votes 属性的节点。通过将 votes 属性设置为 0 ，可以加更多的从节点，因此这个属性一般会用在各大云厂商的“只读实例”产品中。  
- Priority：节点被选为主的优先级。riority 设置的数值越高，越会被选举为主。通过将 priority 设置为0，可以避免主节点切换到某些节点。  
- Hidden：是否将从节点隐藏起来不对外提供服务。设置为隐藏后，客户端驱动无法通过 isMaster/Hello 探测命令感知这个节点，因此无法向其发读请求，但是通过 rs.conf() 和 rs.Status() 命令还是能看到其配置。Hidden 节点一般作为备份节点。由于主节点不能被隐藏，因此官方规定 hidden=true 的节点需要 priority=0。  
- BuildIndexes：从节点是否建索引。如果设置为 false， 则不会同步创建索引。理论上可以节省索引占用的存储空间以及维护索引的 CPU 和内存消耗。但是如果从节点被选举为主，此时由于索引缺失可能会导致一些问题，因此官方规定 buildIndexes=false 的节点需要 priority=0，而且不能对已经加入副本集的节点进行动态变更。  
- Tags：为节点打标签，一般用户从节点的请求分流。比如副本集中有 4 个节点，其中 2 个节点硬件配置高，可用于在线服务，则设置这 2 个节点的 tag 为 {"usage":"online"}；另外 2 个节点硬件配置较低，只用户离线特征分析，则设置这 2 个节点的 tag 为 {"usage":"offline"}。用户程序可以根据自身请求特点设置readPreference 说明希望将请求发往 "online" 还是 "offline" 节点。  
- ArbiterOnly：轻量级从节点，不同步和存储数据，只负责选主投票。一般用于解决偶数副本数的场景。比如副本集中只有 2 个节点，必须全部存活才能选出主。加入一个 arbiter 节点之后，副本个数变为 3，可以容忍一个节点失效。  
- ●SlaveDelay：强行设置某个从节点距离主节点的主从延迟。比如设置 60 秒，则该从节点的最新日志同步时间比主节点要延迟 60 秒或以上。一般用于业务数据快速回滚（比如误删）和测试场景。  

通过状态区分和灵活的配置，MongoDB 能够更好地支持多样化的业务场景。

## 3.2 日志复制
### 3.2.1 Raft 原理
Raft 采用推日志模型，由 leader 节点将日志并发推送到 follower 节点，并应用状态机。整体流程如下图所示：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/6796bf31-33ab-45c2-af62-b194d8cc9ba6" width=400>
   
（1）Client 往 leader 节点发写请求。   
（2）Leader 节点在本地生成日志，并通过 RPC 将日志并发推送到其他 follower 节点。   
（3）确保当前大多数节点（包括自己）已经完成了日志复制，则将日志应用到自身的状态机（存储引擎）中。   
（4）给 Client 返回提交成功。   
注意 follower 节点的日志应用是异步的。在上述流程结束后，leader 节点会推进自身的 commitIndex，在下一次日志推送以及定期的心跳请求中会携带上 commitIndex 发送给 follower，使得 follower 也将这部分日志应用到状态机。   

日志中包含了 index, term，执行命令（幂等形式）等信息，如下所示：   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/a8b3e351-9e92-4240-87d6-4ed335759409" width=400>

每个节点的日志顺序要保证完全一致且不能由空洞，不支持乱序提交。   

### 3.2.2 MongoRaft 原理
MongoDB 使用 oplog 完成主从节点之间的数据同步， oplog 采用递增的混合逻辑时钟作为“序号”（类似 Raft 中的 logIndex），并且会携带 term、库表名、操作类型和命令信息。   
以 4.2.24 内核版本为例，往 db1.coll1 表中插入一条文档 {a:1} 后生成的 oplog 如下：   
```
{ "ts" : Timestamp(1698199568, 1), "t" : NumberLong(37), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.coll1", "ui" : UUID("a3e5d8ac-f4b3-4a40-88c9-8b0b6c47b4c4"), "wall" : ISODate("2023-10-25T02:06:08.268Z"), "o" : { "_id" : ObjectId("6538780f07311975afc52751"), "a" : 1 } }
```
#### 日志同步模型
MongoRaft 中的日志同步采用了从节点拉日志的模型（Pull-Based)，和 Raft 的主节点推日志模型显著区别。    
以4.2.24内核版本为例，MongoDB节点启动后会加载配置，并最终调用[ReplicationCoordinatorExternalStateImpl::startSteadyStateReplication()](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/replication_coordinator_external_state_impl.cpp#L207) 启动复制流程，其中会涉及如下线程：   
1. **BackgroundSync**：一直向源节点发起 find+getmore 请求，并将获取的 oplog 数组存放到 **oplogBuffer** （本质上是一个 OplogBufferBlockingQueue, 即 std::queue\<BSONObj\>）。   
2. **ReplBatcher**(参考 SyncTail::OpQueueBatcher) ：由 OplogApplier 启动的后台线程，这个线程不断消费 oplogBuffer 中的 oplog 日志，并传递到 **OpQueue**（本质上是一个 std::vector）。在消费 oplogBuffer 的过程中，会对每一条 oplog 进行版本检查，并通过类型设置栅栏。比如对一个表进行了 ”文档插入“后又进行了“删表”，则“删表”操作不能和前面的“文档插入”放在一个 OpQueue 中。也就是要保证 OpQueue 是可以并发回放的。   
3. **OplogApplier**：管理  syncTail 对象，并在启动时调用 syncTail::oplogApplication 方法一直获取 **OpQueue** 中的 oplog 数组进行回放（**MultiApply**），回放流程首先按照{表，文档_id} 将 oplog 数组再哈希到多个数组中（参考 fillWriterVectors），然后提交给 writerPool 线程池进行并发回放(_applyOps)。   
4. **WriterPool 线程池**：真正执行 oplog 回放的线程，默认的最大线程个数为min([16](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/repl_server_parameters.idl#L220) ，[机器的核数*2](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L61) ），线程名字为 repl-writer-worker-*.   
5. **ApplyBatchFinalizerForJournal**:  oplogApplier 创建出来的守护线程，主要的作用是在应用完一批 oplog 并更新了 lastAppliedOpTime 之后，通知 syncSourceFeedback 主动给上游节点上报。另外还会等待这批 oplog 的请求刷完 journal 更新了 lastDurableOpTime 之后，通知 syncSourceFeedback 给上游节点上报。   
6. **SyncSourceFeedback**：给上游节点上报同步进度的守护线程。一般由 oplog 回放流程结束后通过条件变量触发，如果长时间没有oplog 应用，也会在 5秒后主动上报一次（起到探活的作用）。   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d1edbe03-0eb9-48fe-aaba-a7233679ac53" width=1000>

从架构图中，可以看到 MongoDB 复制模型具有以下明显特征和优化点：   
- **主节点先应用状态机（写数据）并同时生成 oplog**，然后再将日志复制到其他节点。对于原生 Raft 来说，是先复制日志到其他节点，然后再提交。   
- 从节点**拉日志**。而原生 Raft 是主节点推日志到从节点，从节点被动接收。拉日志模型带来一个明显的好处就是可以形成链式复制结构，即从节点不一定非要到主节点上复制，使得复制链路更加灵活，容错性更高。另外对于一些跨地域的复制场景，链式复制能够明显降低网络带宽成本。比如 5 个节点的副本集，S1 是主节点， S1 和 S2 在北京，S3/S4/S5 在广州，则可能的复制链路为 S1->S2->S3->S4/S5，即 S4 和 S5 不需要跨地域复制。   
- 从节点**异步通知同步进度**。而原生 Raft 是主节点在推送日志的 RPC 响应中感知日志的复制进度。而在 MongoDB 中，由于链式复制的存在，syncSourceFeedback 流程除了上报自己的进度之外，还会上报其他已知节点的进度。比如 3  节点的副本集，复制链路是 S1->S2->S3, S3 给 S2 上报进度，S2 给 S1 上报进度时也会带上 S3 的进度信息。如果不这样做，S1 只能通过心跳感知 S3 的复制进度，效率会大打折扣。另外，即使S2 一段时间没有日志推进，但是 S3 有日志推进，S3 给 S2 上报进度后，也会触发  S2 给 S1 进行一次上报。syncSourceFeedback 还有另外一个作用就是探活，比如 S1 和 S3 之间的网络有问题，心跳不通，但是 S1 能够通过 S2 的主动上报来感知 S3 的存活状态。   
- 使用**多线程**进行**流程解耦**。日志的拉取、回放和进度反馈都是不同的线程完成的，并通过内存队列和条件变量等数据结构进行多线程通信。解耦后各个子流程之间不会直接相互影响，尽可能提升拉日志、回放核心流程的效率。   
- **并发乱序回放oplog**。原生 Raft 规定 Follower 节点严格按照日志顺序进行回放，但是在 MongoDB 中日志是严格按照日志时间分 Batch进行拉取的，但是在一个 Batch 内部的回放是哈希打散之后分发给线程池并发完成。并发回放时必须的，因为 MongoDB 主节点提供并发写入，如果从节点只能严格按照时间顺序串行回放，性能肯定跟不上，导致主从延迟越来越大。但是并发又会带来一些问题，比如是否所有的操作都支持并发？按照什么规则进行拆分能保证正确性？并发操作是否会和唯一索引等约束条件产生冲突？并发带来的乱序是否会导致用户读到不一致的数据，MongoDB 是如何解决的呢？   
- **更灵活的复制状态**。原生 Raft 中 Follower 节点在日志持久化后才认为复制成功，但是在 MongoDB 中使用 lastAppliedOpTime 和 lastDurableOpTime 进行了区分。其中 lastAppliedOpTime 表示日志已经应用并在本地生成了 oplog，但是还没有异步写 jounal(WAL)，而 lastDurableOpTime 表示已经写  journal 的持久化状态。用户可以自定义写入级别，主节点将根据用户定义的写入级别依据 lastAppliedOpTime 或者 lastDurableOpTime 来更新提交状态。总结来说，MongoDB 将选择权交给用户，可以在请求延迟和数据持久性上做权衡。   
 
#### 同步源节点的选择
如果用户关闭了副本集的链式复制功能，则所有节点只能到主节点同步。   
如果用户打开了链式复制功能（默认打开），则：   
- （默认）**自动选择**同步源节点。从节点根据自己的拓扑信息，选择一个 opTime 比自己新的，而且网络距离最近的节点作为同步源。   
- **手动选择**同步源节点。用户可以通过 replSetSyncFrom 命令，手动给一个从节点指定同步源。通过手动指定同步源，用户可以明确的控制整个复制链路。   

链式复制距离如下，B 从 A 同步，C和 D 从 B 同步。复制链路包括 拉oplog 流程（粗实线）和进度反馈流程（细实线），心跳流程是虚线部分。因此，最终副本集中各个节点的通信是**复制链路+心跳链路相结合**的形式。复制链路通信更频繁，是节点状态同步的核心路径（oplogFetch 和 updatePosition 命令的元数据中会携带节点的状态信息）而且由于链式的存在，能够很好容忍局部网络故障。心跳通信频率更低（默认 2 秒 1 次），能够作为节点状态同步非常好的补充，并且为网络故障时的同步源切换流程提供参考。两者相辅相成。

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/95fb398d-7558-48bc-a3bc-4190c3abda62" width=400>

#### 并发回放如何保证正确性
**如何并发：按照什么规则进行 Hash?**   
MongoDB 内部采用了 2 级 Hash 的策略，参考[SyncTail::_fillWriterVectors](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1163):   
1. 首先按照表（db.collection）进行 Hash，因为不同表之间的修改没有直接关联。   
2. 其次再按照文档的 _id 进行 Hash，因为不同文档之间的修改也没有直接关联。   

同 1 个文档（_id 相同）的 oplog 会Hash 到一个桶中，并保持原始的时间顺序。   
当然也有一些特殊情况不会进行 2 级 hash。比如固定集合（capped collection）要满足“先插入先删除”的特性，对不同文档的插入顺序有严格要求，因此不能按 _id Hash 后乱序插入，也[不能进行 groupInsert](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/applier_helpers.cpp#L91) 的回放优化。

**DDL 操作如何处理？**   
Oplog 中除了常见的 CRUD 操作之外，还有例如删库删表等 DDL 操作。这些 DDL 操作无法和 CRUD 操作并行回放，例如对表 A 先插入一条数据，然后再删表。如果将这 2 条 oplog 并发回放的话，可能会出现先删表再插入数据，显然是不合适的。   
从节点的 ReplBatcher 线程调用 [OplogApplier::getNextApplierBatch](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L244) 方法将 oplogBuffer 队列中的 oplog 传递到 opQueue 中，opQueue 可以认为是一个批量并发回放的 batch。getNextApplierBatch 方法会甄别每条 oplog 的类型，如果是 DDL 操作，会将这些操作单独作为一个 batch 回放，具体可以参考 [mustProcessStandalone](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_applier.cpp#L211) 的处理逻辑。

**回放过程中的不一致数据能读到么？**   
假设在主节点上依次插入 3 条数据 {x:1} -> {y:2} -> {z:3}，在从节点上并发回放的先后顺序可能是 {z:3} -> {y:2} -> {x:1}。如果用户在从节点上读取数据，显然是无法满足一致性要求的。   
因此，MongoDB 通过一些机制避免了用户读取正在回放中的数据：   
- 在 3.6 及之前的版本，用户对从节点的读取操作需要先获得 PBWM (Parallel Batch Writer Mode)锁。由于 oplog 的批量回放也需要先获取 PBWM 锁，因此能够互斥。但是这种方式效率很低，导致用户到从节点的请求经常卡顿，而且也会影响从节点的同步速度。   
- 在 4.0 及之后的版本，用户到从节点的读取操作变为直接读一致性快照（不依赖 PBWM锁）。所谓快照，就是读取不超过 lastAppliedOpTime 之后修改的数据版本。而每次 oplog 批量回放完成之后，都[会主动更新 lastAppliedOpTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L865)。**这样保证了用户在从节点上能够尽快获取尽可能新的一致性数据。**   

示意图如下：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/409df3b3-3b55-4800-b6dd-b8e4823d9234" width=800>


**乱序回放是否会打乱从节点的  oplog 顺序，导致主节点和从节点的 oplog 顺序不一致？**   
假设下图的场景：  
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/177d4c62-66d1-492f-93c6-f0c513dbed03" width=600>
   
(a) S1 是主节点，最新写入日志 3、4、5。S2 是从 S1 同步，日志进度为 1、2。S3 从 S2 同步，日志进度为 1。   
(b) S2 从 S1 拉取日志 3、4、5，然后批量乱序回放，本地形成日志 5、4、3 。   
(c) S3 从 S2 拉取日志 2、5、4，然后批量乱序回放，本地形成日志 4、2、5。批量回放结束后，4、2、5 的修改变为对用户可见，**此时用户会读取到不包含 3 的不一致数据！**  

上述假设场景的根本问题是日志回放顺序和生成顺序绑定，导致的日志乱序。MongoDB 通过以下机制保证了乱序回放情况下的日志严格有序：
- **每条 oplog 都会携带时间戳**（int64），而且这个时间戳只能是主节点生成，从节点只能遵守不能修改。这样保证 oplog 在副本集中每个节点的时间顺序都是一样的。   
- **Oplog 在 MongoDB 中严格按照时间戳存储和查询**，并通过可见性判断机制（只有 oplog batch 回放完，[才对外可见](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L876)）来保证从节点拉取的 oplog 不会出现空洞。   
- 另外，**从节点上写 oplog 和回放 oplog 的流程是分开的**，不像主节点将写用户库表和oplog放在一起提交。在从节点上，线程池[先并发写 oplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1369)，再[并发回放 oplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L1410)。   

上述机制保证了从节点上的日志回放顺序不影响日志生成顺序，副本集中每个节点的日志顺序完全相同。所以，MongoDB 中真正的复制流程如下所示：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9d257a15-5d08-405c-89ac-9787a15abb22" width=800>

(a) S1 是主节点，最新写入日志 3、4、5。S2 是从 S1 同步，日志进度为 1、2。S3 从 S2 同步，日志进度为 1。
(b) S2 从 S1 拉取日志 3、4、5，虽然乱序回放时数据写入的先后顺序时 5->4->3， 但是日志的顺序仍然保持 3->4->5，和主节点一样。
(c) S3 从 S2 拉取日志 2，3，4，乱序回放插入数据 4->2->3。回放完之后用户可以读取 S3 的数据为 1，2，3，4，虽然数据不是最新的，但是不会读到有空洞的非一致性数据。


**如何处理乱序打破唯一性约束的问题？**   
乱序回放可能会破坏唯一索引的约束。以下图为例，在某个表上按照 "a" 字段创建了唯一索引。在主节点上对这表依次进行了 插入 {_id:1, a:1}、删除{_id:1, a:1} 和 插入{_id:2, a:1} 的操作。随后生成的 oplog 在从节点上按照 _id 哈希之后并发回放，则有可能 {_id:1, a:1} 和 {_id:2, a:1} 的插入操作先执行，此时就破坏了唯一索引的约束，导致错误。    
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/f023bf96-0ec6-4362-bcba-c12472792cde" width=600>
   
为了解决索引唯一性的问题，MongoDB 在从节点 oplog 回放阶段**临时放开了唯一性约束**，具体可以参考 [IndexCatalogImpl::_indexFilteredRecords](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1334)、[IndexCatalogImpl::_updateRecord](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1395) 和[IndexCatalogImpl::_unindexKeys](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1441) 中如何调用 [prepareInsertDeleteOptions](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/catalog/index_catalog_impl.cpp#L1649) 生成操作参数。在 prepareInsertDeleteOptions 的逻辑中，对于从节点的索引操作放开了约束，即使是唯一索引也允许 duplicateKey 的存在，即设置 dupsAllowed = true。这个设置会使得操作存储引擎时[跳过索引的唯一性检查](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_index.cpp#L1519-L1549)。   
跳过唯一性检查，意味着 oplog 回放阶段的唯一索引可能存在多个相同 key 的情况，但是在回放完成后能仍然能满足唯一性。基于前面的分析，在 **oplog 回放期间这部分不一致数据是用户读不到的**，因此不会对正确性产生影响。

>备注：  
业界广泛使用的 DTS(数据传输服务，将一个 MongoDB 集群的数据同步到另一个 MongoDB 集群) 也会遇到索引的唯一性约束问题。  
一些云产商为了实现方便，在检测到表有唯一索引后，会将这个表的数据同步变为串行（不再按_id 并发），单表的同步性能会下降。  
也有一些云产商采用了一些冲突检测机制尽量保留并发，比如 MongoShake 在 oplog 日志中增加了 uk 字段来说明表有哪些唯一索引以及本次改动的 key。如果有 2 条 oplog 改动了相同的 key，则不能放在一个 batch 中回放。如果改的 key 不同，则还是能放在同一个 batch 中并发回放。  
总体来说，MongoDB 内核中主从同步的冲突处理策略是最易理解，也是性能最强的，当然这也得益于内核代码本身就能进行更加精细化的控制。  

#### 日志同步性能探究
**GetMore 拉日志的困境**       
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/59ffbd58-6f8e-4936-bb97-e6512346a7e4" width=300>
  
虽然MongoDB 采用了将**日志拉取和回放操作分离**的设计，尽可能提升拉取日志的速度，但是本质上还是一个单连接模型，网络延迟仍然不能忽视。   
以上图为例，假设主节点和从节点之间的网络距离是 50 ms（跨地域部署的场景，图中虚线框所示），硬件配置无限高使得请求和响应的处理几乎不耗时，已知一批 oplog 的最大载荷是 16MB，则可以推算该场景下的极限同步速度：   
a) 0->50ms 时刻, 拉日志请求（find/getmore）由从节点发给主节点。   
b) 主节点准备好日志，封装好 response，极限情况下足够快，不耗时。   
c) 50->100ms 时刻, Response 由主节点发给从节点，极限情况下带宽无限。   
d) 从节点处理响应，将 oplog 放到内存队列，然后开始拉下一批，极限情况下不耗时。   
这样主从的极限同步速度为 16MB/100ms = 160MB/s。如果主节点上有大量写入，生成oplog日志的速度超过了 160MB/s，则主从延迟只会越拉越大。   

**MongoDB 的解决方案**
1. FlowControl 机制，主动限流   
从 4.2 内核版本，开始引入了 [Flow Control 机制](https://www.mongodb.com/docs/v4.4/tutorial/troubleshoot-replica-sets/#flow-control)，主节点在主从延迟较大时，会对写入请求进行限流，保证副本集的大多数提交时间点（majority committed point）不会超过设置的阈值（默认 10 秒），具体实现为主节点根据延迟情况计算每秒能够发出多少 ticket供写入，如果 ticket 消耗完毕，则写入请求会阻塞到下一秒有空余的 ticket 为止。具体的计算规则可以参考 [Github Wiki](https://github.com/mongodb/mongo/blob/r5.0.16/src/mongo/db/catalog/README.md#flow-control)，这里不作展开。   

2. 主节点流式推送日志，推拉结合   
从 4.4 版本开始，MongoDB 默认支持了 [Streaming Replication](https://www.mongodb.com/docs/v4.4/core/replica-set-sync/#streaming-replication), 如果节点 B 到节点 A 同步数据，会发起带 exhaust cursor 属性的拉日志请求，然后接收节点A 的日志推送(推拉结合)。后续节点 A 产生新的 oplog 日志后，可以直接推送给节点B，这样可以减少网络 round-trip 带来的延迟。提升了拉 oplog 日志的速度后，在以下方面会带来显著提升：   
    - 提升到 Secondary 节点读数据的新鲜度。   
    - 对于 {w:1} 只写主节点就确认成功的请求，能够降低主节点 crash 时数据回滚的风险。   
    - 对于{w:"majority"} 写多数节点的请求，能够有效降低处理延迟。   

以节点 A -> 节点 B 的同步为例。在 MongoDB 4.4 版本中，为了实现 streaming 复制流程，节点B的 OplogFetcher在通过 find/getmore 请求到节点A 上拉日志时，会[携带 exhaust 标记](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/oplog_fetcher.cpp#L638-L656)。后续处理流程如下：   
- 节点A 在返回一批oplog后，**如果发现处理的是 exhuast 请求，则[自动生成一个新请求](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L511-L517)（相当于在同一个连接上，自己给自己发了一个 getmore 命令）。然后将 ServiceStateMachine 的[状态置为 State::Process](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L440) 方便后续直接处理这个请求**，非 exhaust 场景下会[置为 State::Source](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/transport/service_state_machine.cpp#L446) 接收节点 B 发下一个 getmore 命令。   
- 节点B  不断通过 [OplogFetcher::_getNextBatch()](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/oplog_fetcher.cpp#L663) 获取新日志，底层调用 [DBClientCursor::requestMore()](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L261) **在判断请求是 exhaust 类型时，会直接[读取连接中的buffer数据](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L305)**，如果不是 exhuast 类型则[重新发起 getmore 命令](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/client/dbclient_cursor.cpp#L277-L284)去源节点拉日志。   

**业界横向对比：ParallelRaft  的并行复制**
不论是MongoRaft 的拉日志模型还是推拉结合模型，都是使用了单连接有序传输日志。日志的提交方式都是顺序提交，且不能有空洞。同理，原生 Raft 协议由于其顺序提交的特性，也不能很好地驾驭多连接日志同步的场景。   
[PolarFS](https://www.vldb.org/pvldb/vol11/p1849-cao.pdf) 的 ParallelRaft 算法提供了一个新思路：通过多个连接传输日志，并且支持乱序确认和乱序提交。为了解决乱序提交带来的冲突检测，每条日志上都有 look-behind buffer 记录前 N 条日志修改的 LBA （逻辑地址，可以类比为 MongoDB 中的 _id），在日志存在少量空洞的情况下也能完成冲突检测并顺利提交。


### 3.2.3 小结
MongoRaft 和 Raft 虽然都是强主模式，通过大多数节点的日志复制来完成提交，但是在具体实现上却有很大区别。主要体现在：  
1. Raft 采用**推日志模型**，leader 通过 RPC 将日志推送给 follower。而 MongoRaft 采用**拉日志模型**，并通过 streaming 推拉结合的方式提升速度。拉日志模型带来一个明显的好处就是能更好的支持链式复制。  
2. Raft 先复制日志到大多数节点，leader 节点才应用日志。而 MongoRaft 中每个节点的日志复制和日志应用是”一体“的。  
3. Raft 中的日志采用递增的序号来标记顺序。MongoRaft 中的 oplog 采用混合时钟来标记顺序，可以更灵活应对各种业务需求，比如按时间戳读、回档到某个时间点等。  

另外，MongoDB 在内核实现中，使用了大量的架构优化来提升性能，比较重要的优化点有：  
1. 采用多线程 + 内存队列，将日志的拉取和回放流程**解耦**，使得整个复制过程**流水线化**，充分利用多核硬件。  
2. 采用最极致的并发策略，充分利用多线程提升日志持久化和回放的效率。在并发乱序回放过程中保证了读取数据的一致性，并且严格按顺序提交日志。  
3. 通过 flow control 限流机制和 streaming replication 机制尽可能提升日志传输的效率，避免主从延迟带来的风险。  



## 3.3 选主协议
Raft 和 MongoRaft 都是 “强主”协议，所有的写操作都在主节点上完成，并复制到从节点。因此，如何快速安全的选出主节点非常关键。简单来说，主节点的选举由副本集中大多数节点（包括candidate节点自己）投赞成票而来，然而实际生产环境中还需要解决一些异常场景，包括：   
1. 触发选主的条件是什么？
2. 如何避免网络分区导致同时出现多个主节点？
3. 如何避免在配置变更时出现多个主节点？
4. 如何在出现投票冲突时快速达成一致？
5. 如何主动进行主节点切换？
6. 如何保证新主节点一定包含已提交的数据，即不会出现已提交的数据由于切主而丢失？

下面我们带着这些问题，分析一下 Raft 和 MongoRaft 的处理流程。其中配置变更时的选主安全性问题，我们单独在配置变更章节描述。

### 3.3.1 Raft 原理
#### 何时触发   
Leader 定期给 follower 发送心跳。如果 follower 节点在一段时间内（Election timeout）没有接收到心跳，则状态变为 candidate 并发起选主。   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/c849c661-4c9a-4eef-ae0a-8bd31b6fbb69" width=300>

#### 选主流程   
Candidate 首先将 term 加 1，然后并发给副本集中的其他节点发 RequestVote 请求。Candidate 节点接下来可能会遇到 3 种场景：   
1. 副本集中大多数节点（包括 candidate 自己）投了赞成票，此时 candidate 成功变为 leader. 投票节点在每个 term 最多只会投出 1 张赞成票，而且为了防止出现已提交数据被回滚的情况，candidate 的日志必须足够新才能得到赞成票。   
2. 收到另外一个节点声明自己是 leader 的心跳信息。如果那个新 leader 的 term 大于等于自己当前的 term，则 candidate 变为 follower，否则忽略那个心跳。   
3. 没有收到大多数节点的赞成票，则 candidate 等待一段时间后 term 加1，重新发起选举。出现这种场景，可能是副本集中同时有多个 candidate，每个 candidate 都只得到了少数票。也有可能是副本集当前只有少数节点存活。   

对于上述第 3 种场景，raft 采用的解决方式是 candidate 等待随机一段时间后再重新发起选举，这样下次选举的冲突几率就非常低了。   
最初 raft 打算采用给 candidate 派优先级的方式来解决冲突（ranking system），优先级低的 candidate 如果发现有更高优先级的 candidate 存在，则出动退回到 follower 状态。这个方案看起来合理，但是实际测试中发现系统的可用性（无主时间）会收到较大影响，而且有很多极端场景导致的bug. 最终，还是选择了随机回退的方案。   
从 raft论文 给出的测试结果来看，随机回退能够有效避免选举冲突（split vote）的情况。   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/40a79a7b-7ee7-4e09-82d8-44a4cdc41beb" width=400>
   
在上面的半张图中，150-150ms 的场景是没有任何随机性的，可能选主需要长达 10 秒才收敛。如果增加 5 ms 的随机性，即选主超时在 150-155ms 中的随机值，平均 287 ms 即收敛，可以说效果非常明显了。   
在下面的半张图中，raft 分析了将 election timeout 设置为多长更合适的问题。一般来说设置的短一点，follower 能很快发现无主并发起选择，降低系统无主的时间，比如实验中设置 12-24ms 的 election timeout, 平均只需要 35ms 就能选出新主。   
但是设置的太短也有问题，如果出现一些网络延迟抖动，可能导致一些不必要的选主和切主。比如 3 副本分别在中国、北美和南美，网络 RTT 就在百 ms 级别，而且网络不稳定，如果设置 election timeout 为 200 ms, 可能会导致选主非常频繁。**因此，election timeout 的设置要依据网络距离，网络质量等因素进行综合评估。**   

#### 选主成功后的流程
如果 candidate 成功变为 leader，则立刻向其他节点发送心跳昭告自己的状态，并避免新的选举发生。

### 3.3.2 MongoRaft 原理
#### 何时触发
mongo 的选主支持主动和被动 2 种触发方式。
主动触发的方法有：   
- 主节点主动 stepDown。在主节点上主动执行 replsetStepDown 命令，主节点会马上转为 secondary, 并很快选举出新主节点。相比重启主节点来触发选主，replsetStepDown 方式触发选主的时间更短，从节点不需要等待 election timeout （默认 10 秒）就发起选主。   
- Priority takeover。如果某个节点发现自己配置了比主节点更高的 priority，则会在等待一段时间后（参考 getPriorityTakeoverDelay）发起选举。   

被动触发一般对应节点不通的场景：   
- 主节点一段时间内（election timeout, 默认 10 秒）感知不到大多数节点的存在，则主动 stepDown。  
- 从节点一段时间内（election timeout, 默认 10 秒）感知不到主节点的存在，则发起选举。  

Mongo 也使用了心跳机制来感知各个节点的状态，但是与 raft 不同的是，Mongo 中的任意 2 个节点都能相互发心跳请求：   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/607a6fd1-6969-4a41-a042-c1f2f0a8a9cf" width=500>

由于任意 2 个节点之间都能互发心跳，对于 N 个节点的副本集，平均每个心跳周期内，副本集内的总心跳请求有 N(N-1) 个，如果节点太多会出现爆发式增长。这也是 MongoDB 限制一个副本集最多 50 个节点的原因之一。   
这种心跳机制的好处是，副本集中每个节点都能知道全局的节点状态信息，因为心跳信息中包含了节点的 opTime 同步进度，replsetConfig 版本，主节点的状态和 term 等信息。    
基于上述机制，副本集中的从节点可以根据一定的规则选择合适的同步源节点，并形成链式复制结构（生成无环的 spanning tree）。   
另外，在 MongoDB 的复制模型中，拉取 oplog 的请求和更新oplog 复制位置的请求会携带包含节点状态的元数据。   
以下图为例，副本集中有 5 个节点，在某个时刻主节点 A 和 C, D, E 网络不通，但是 B 和所有节点都能正常通信：   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/f5450ee0-bcec-40ce-a376-cfad9a2c4bd4" width=600>

对于左侧 Raft 来说，C, D, E 在一段时间内接收不到主 A 的心跳，会发起选主。   
对于右侧的 MongoRaft 来说，C, D, E 可以从 B 节点同步，链式复制结构会让 A 通过 B 节点感知到 C、D、E的状态，而不用重新发起选主。   
不过需要注意的是，如果节点 B 是 arbiter 或者数据非常落后，则不会形成上述链式复制结构。   

#### 选主流程
选主流程分为 2 步：   
1. Dry-run election, 即试探性发起选举。其目的在于试探集群中有多少节点会支持自己，增加后续真正选举的成功率。发起 dry-run election 时不会增加自己的 term，所以不会造成 term 无意义的递增，而且也不会导致当前的 Primary 节点 stepDown.   
2. Real election，candidate将 term 加 1，然后真正发起选举。首先 candidate 会投票给自己，然后并发给集群中有投票权的节点发起投票请求。如果获得了大多数赞成票，则选举成功。   

从 voter 的视角来看，它在收到 candidate 发过来的 requestVotes 命令时，先判断 term 是否比自己的新，并更新自己的 term. 然后判断自己是否应该投赞成票，如果满足以下条件，会投反对票：  
- Candidate 的 term 更低。   
- 配置不匹配，replSetName 不匹配。   
- Candidate 的数据更旧（lastAppliedOptime更小）。   
- 对于这个 term，voter 已经投过一次票了（当然 dry-run 流程的不算数）。   
- Voter 是一个 arbiter, 而且它能感知到当前存活了另一个主，并且这个主的 priority 不比 candidate 低。这个策略主要是应对 Primary-Secondary-Arbiter 架构下，primary 和 secondary 不能通信，arbiter 能够和它们都正常通信的场景。如果 arbiter 没有这样的投票策略，可能同时会出现 2 个主。   

一旦节点给自己或者其他节点投票，则会将 “lastVote” 信息持久化到本地的 local.replset.election 表中，避免节点重启之后这些信息丢失，导致给同一个 term 多次投票的情况。   

#### 如何保证选主流程的安全
一个节点只给一个 term 投 一次票的机制，避免了一个 term 出现多个主节点的情况，但在某些时刻，集群中可能出现 2 个主的情况，分属于不同的 term。    
举例如下：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b13756c9-9c85-47e5-9c0c-406ad4acf66d" width=400>

(a). 所有节点运行在 term 1, 其中S1 为主节点。    
(b). S1和S5 出现网络隔离，S5 发起选举并获取 S3、S4、S5的投票成为了新主。S1、S2 运行在 term 1, S3、S4、S5运行在 term 2。    
(c). S1 和 S5 都能接收客户端的写请求，S5 的写请求能提交（到大多数节点）。S1 的写请求只能复制到 S1 和 S2，其他节点由于网络不通，因此会卡住不能提交。    
(d). S1 的日志复制到 S3、S4、S5, 但是term 太低并拒绝。    

对于 Raft 来说，S1 推送给 S3、S4、S5 的复制请求会由于自身 term 太低被拒绝。    
而对于 MongoRaft 来说，由于采用了从节点拉日志的方式，情况要更复杂一些。在上述例子中，S3 和 S4 的日志复制取决为当时的同步源节点是否切换为 S5，如果切换为 S5 则压根不会从 S1 节点复制日志。如果同步源还是 S1 ，则会在应用日志后，通过 updatePosition 命令给 S1 反馈日志同步情况，该命令中会携带 S3/S4 自身的 term 为 2。 S1 在收到这个命令后，明白自身的 term 太低，主动 stepDown，并不会将该写请求置为提交。   
MongoDB 论文中列举了不一样的例子：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/d3583214-26e2-4bf1-a84e-cf24fe277f20" width=400>

(a). A、B运行在 term 2, A 作为主节点提交了日志 “2”；C、D、E 运行在 term 3， E 作为主节点提交了日志 “3”.   
(b). Raft 协议：A的日志只能复制到节点B，C、D、E 节点由于 term 更高会明确拒绝。   
(c). MongoRaft 协议：A 的日志可以复制到 C 和 D.   
(d). MongoRaft 协议：C 和 D 在updatePosition 反馈时表名自己是 term 3，A 收到后主动 stepDown，不会提交日志 “2”。   
(e). MongoRaft 协议：E 节点的日志“3”复制到所有节点，并 overwrite A节点的日志“2”。   
整体来看，虽然由于复制模型导致处理过程不一样，但是 MongoRaft 和 Raft 殊途同归。   

引入 term 机制后确实避免了同时出现多主并同时提交日志的问题。但是也引入了另外一个问题：主节点是否能提交之前 term （older term）的日志？   
对于 Raft 和 MongoRaft 来说，答案都是否定的。   
首先引用 Raft 论文中的一个例子，看看提交主节点如果提交之前 term 的日志会有什么问题：   
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/bf8eb6f9-4b64-4b96-a2fd-a73a39d83c60" width=400>

(a). S1 是主节点，并将最新的日志 "2" 同步到了 S1 和 S2，还没有提交。    
(b). S1 挂了，S5 被 S3、S4、S5 选举为新主，运行在 term 3，并写入了日志 “3” 但没有复制到其他节点。    
(c). S5 挂，S1 重启并成为新主。然后继续将之前的日志 “2” 复制到 S3, 此时已经复制到了大多数节点（已经复制到了S1、S2、S3），但还没有提交。    
(d). S1 没有提交日志 “2” 就挂掉，则 S5 可能重新被选为主，并在选主之后会将之前已经复制到大多数的日志“2” 抹掉（overwrite）。   
(e). S1 提交了当前 term 的日志“4” 才挂掉，则 S5 不可能被选为主。日志 “2”在日志“4”之前，才会保持提交状态。  

>需要特别说明的是，为什么提交了日志 “4”， S5 就不会被选举为新主，而之前“提交”了日志 “2”， S5 却能被选举为新主呢？   
>Raft 选举时，voter 给 candidate 是否投票，有一项主要的依据就是比较日志的新旧：如果 voter 的日志更新，就不会给 candidate 投票。而比较日志的新旧，首先就是要比较日志中的 term 大小。
在上述例子中，日志“2” 的 term小于日志“3” 的 term，日志 “3” 的 term 小于日志“4” 的 term。

从上述场景(d) 中可以看到，即使older term 的日志复制到了大多数节点，还是可能被抹掉。因此，Raft 中的主节点不会通过计算日志是否复制到了大多数节点来提交日志，而是**通过提交当前 term 的日志来间接提交之前 term 的日志**（场景(e))。   
Raft 论文中也提到，采用上述“一刀切”的机制还是方便协议的理解落地，降低复杂性。理论上来说，如果在上述步骤 (c) S1 挂掉之前，日志 “2” 复制到了所有节点（并 overwrite 了 S3 的日志“3”），其实也能肯定日志“2” 的提交状态。   

MongoDB 也采用了一样的机制，如果从节点反馈的 updatePosition 中的 term 较低，主节点不会真正提交这条日志。只有收到的反馈中 term 和主节点当前的 term 相同，才会真正提交这条日志。在某一条日志被提交之后，比其更早（opTime 更小、term 更小）的日志也间接提交了。   

**这也是为啥 MongoDB 在选举新主节点之后，会马上生成一条当前 term 的 oplog 日志，其实就是为了提交更早 term 的日志。**   

#### 选主成功后的流程
新主节点在选举成功之后会进入如下流程：   
1. 停止当前的同步逻辑，并将自己选举成功的消息通过心跳告知其他节点。   
2. 检查自己是否需要 catch-up. 比如副本集中有 5 个节点，数据从新到旧的排序为 A>B>C>D>E，B得到了 BCDE 的投票成为新主。A 节点上存在一些更新的日志，虽然没有提交，但是 B 节点也会同步过去。具体实现流程为步骤1 中的心跳响应中包含了各个节点的 lastAppliedOpTime 信息，如果有节点比自己新，则这个新主节点就会进入 catch-up 阶段尽可能多同步一些日志。在进入 catch-up 节点之前，新主节点会启动一个定时器，防止catch-up 时间不可控，集群长期无主影响可用性。如果 catch-up 超时也没有关系，因为这只是一个尽力而为的操作。   
3. Catch-up 不论成功与否，接下来会进入 drain-mode. 新主节点将拉过来的日志进行回放（apply）。   
4. 新主节点写一条 “new primary” 的 oplog，方便提交之前 term 的日志。   
5. 新主节点删除上述临时表、终止残留的事务。此时选主操作才真正完成，新主节点可以对外提供写服务。   

从上述流程可以看出，catch-up 阶段时选主成功后 MongoRaft 和 Raft 最大的区别。MongoRaft 通过这个机制可以尽量保留已经写入到主节点但是还没有提交（到大多数节点）的数据。主要是因为MongoDB 支持非 majority的写入方式。比如客户端为了性能考虑，可以在只写主成功后即返回。原则上来说，这些数据是可能被回滚的，MongoDB 也不保证这种写入方式的持久性。但是，能 MongoDB 在设计上尽量避免发生回滚。

### 3.3.3 小结
纵观 Raft 和 MongoRaft 的选主流程，在心跳探活、term、大多数选举等机制上大致相同。但是 MongoRaft 相比 Raft，还是在不少方面做了改进，包括：   
- 探活机制方面，MongoRaft 任意 2 个节点之间都有心跳，容错性更高。另外，MongoRaft 支持灵活的链式复制架构生成 spanning tree 并传递节点状态信息。使得集群对网络的容错性也更高，避免发生不必要的选举。
- 选主流程上，MongoRaft 引入了 dry-run 机制，提升了真正选举时的成功率，避免不必要的term 递增以及由此造成的主节点 stepDown。
- 选主成功后，MongoRaft 引入了 catch-up 机制，尽可能地保留已经写到主节点但是还没有提交（到大多数节点）的数据，增强了数据持久性，减少回滚操作。


## 3.4 配置变更
在副本集运行过程中会遇到配置变更需求，比如新增加一个节点提升读能力，或者删除一个机器故障的节点等。    
一种比较原始的方法，是将配置写在配置文件中。如果需要修改配置，则停掉所有节点，统一修改配置之后再重启所有节点。但是这种简单粗暴的方法并不适用于线上系统，因为很大程度上影响了系统的可用性，并增加了误操作风险。    
因此，非常有必要支持不停服变更配置。    
下面先从 [Raft 论文](https://raft.github.io/raft.pdf)中描述的算法入手，了解配置变更的问题以及解决方案。然后分析 MongoRaft 的实现方法，以及相对 Raft 有哪些改进。

### 4.4.1 Raft 原理
为了做到不停服，就需要解决变更配置期间系统运行的安全性问题。我们知道新配置的同步是需要时间的，在这段时间内集群中新、老配置共存，可能引发同时存在多个主节点的问题。论文中列举了一个例子如下：    
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/b6dd40f6-8ce7-4798-bf48-5f820a3a4142" width=400>

起始时刻，集群中有 Server 1,2,3 共 3个节点按照 C(old) 配置正常运行，后来加入了 2 个节点 Server 4 和 Server 5。在中间某一时刻，Server 1, 2 还在使用 3 节点的旧配置，但是 Server 3,4,5 已经使用了 5 节点的新配置。如果 Server 1,2 与 Server 3,4,5 出现了网络隔离，则 Server 1,2 之间会选出一个主节点（满足总节点数为 3 的选举条件），Server 3,4,5 之间也会选出一个主节点（满足总节点数为 5 的选举条件）。 2 个主节点都能接收并提交写请求。

为了解决上述问题，Raft 采用 2 阶段流程来实现不停服配置变更，具体流程为：       
1. 生成 C(old, new) 日志，并提交到 C(old) 的大多数节点和 C(new) 的大多数节点。
2. 生成  C(new) 日志并提交到集群的大多数节点，如果有节点发现自己不在新配置中，则主动退出。     

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/5212f6fd-350e-4d41-9a4b-946ee77469f5" width=400>
  
需要说明的是，C(old, new) 不只是简单的将 C(old) 和 C(new) 的节点做并集。C(old, new) 的真正的含义是：每项决议（选主和日志提交）都需要 C(old) 的大多数节点通过**而且** C(new) 的大多数节点通过。     
通过这种方式，避免了集群在同一时刻同时存在 C(old) 和 C(new) 并且各自独立进行决议的问题。但是又引入了 3 个新问题：
1. C(old) 和 C(old, new) 能否共存？    
如果 C(old,new) 还没有完成提交就出现了主节点宕机或者网络分区，此时可能会选举新主。不妨假设假设新主节点在 C(old) 配置下，则其需要获得 C(old) 中大多数节点的赞成票；假设新主节点在 C(old, new) 配置下，则其同时需要 C(old) 和 C(new) 中大多数节点的赞成票。**也就是说，无论如何都需要收到 C(old) 中大多数节点的赞成票**。由于C(old) 节点中大多数节点的赞成票只会投给一个节点（Election Safety），因此不会再同一时刻出现 2 个主节点。    
至于新主节点可能运行在  C(old) 配置下，也有可能运行在 C(old, new) 配置下，具体取决于故障之前 C(old, new) 的同步情况。    
2. C(old, new) 是否为稳定状态?      
如果 C(old, new) 状态被提交，说明 C(old, new) 配置已经同时提交到了 C(old) 和 C(new) 中的大多数节点。如果此时发生主节点宕机或者网络分区，新主节点肯定会运行在 C(old, new) 配置下，其选举要同时获得 C(old) 大多数节点的赞成票以及 C(new) 中大多数节点的赞成票。因此，不会出现 2 主节点。   
3. C(old, new) 到 C(new) 能否平滑过渡？       
C(old, new) 被提交之后，主节点接着会提交 C(new)。如果在提交 C(new) 的过程中出现了主节点宕机或者网络分区导致重新选主。则新主节点要么运行在 C(old, new) 下，要么运行在 C(old) 下，绝不可能运行在 C(old) 下，因为前面 C(old, new) 的成功提交已经保证了即使还有节点运行在 C(old) 下，也构不成大多数。      
不妨假设新主节点运行在 C(old, new) 下，则其需要同时获得 C(old) 和 C(new) 中大多数节点的赞成票；如果新主节点运行在 C(new) 下，则其需要获得 C(new) 中大多数节点的赞成票。也就是说不论怎么选举，新主节点都需要 C(new) 中大多数节点的赞成 。因此，不会出现 2 个主节点。      

除了新旧配置的安全过渡之外，Raft 论文中还描述了一些工程实践中常见的异常场景：       
- **新加入节点的延迟过大问题**。新加入的节点需要花较长的时间进行数据同步（catch-up），一般是全量加日志增量的方式。由于 Raft 中日志顺序提交的特性，配置变更日志和数据变更日志糅合在一起，会导致配置变更日志需要较长的时间才复制到新加节点上，整个配置变更的时间也随之变长。为了解决这个问题，Raft 建议按照 non-voting 模式添加新节点，这种模式的节点不会算在 majority 范围，因此不会阻塞配置变更的提交。     
- **主节点不在 C(new) 新配置中**。如果主节点发现自己不在 C(new) 中，就立刻 step down 并退出集群，则 C(new) 日志无法在集群中提交。这种情况下，意味着已经要被剔除的节点还要承担一段时间的集群管理任务。Raft 建议的做法是该主节点要等到 C(new) 在集群中提交之后再退出，当然这里提交时的大多数节点是不包含它自己的。
- **已删除的节点发起选主请求**。第一次看到这问题比较诧异，因为论文中明确描述了 C(new) 提交之后，不在 C(new) 中的节点主动停服并退出集群，为啥还有已删除节点发起选举的问题呢？后来想了一下，个人认为这里说的应该是配置提交过程中因为各种原因没有收到新配置的节点。这些节点因为已经被剔除，无法收到主节点的心跳，所以会尝试通过RequestVote RPC 选主，导致主节点降为 follower 状态。然后集群重新选主，选出的新主又不给这些已删除 的节点发心跳，然后这些已删除的节点又要闹着选主。Raft 建议的做法是从节点如果在 election timeout 内成功收到过主节点的心跳，或者主节点在 election timeout 内成功收到过大多数节点的心跳，则认为主节点正常。如果这段时间有节点闹着选主，一概拒绝之。

Raft 的配置变更流程整体来看比较完善，但是实际工程实践中还有一些场景有待优化。个人认为有 2 点：     
1. 没有充分说明主从延迟很大时的效率问题。     
Raft 将配置和数据变更都放在日志中提交，这样导致主从延迟大时，配置变更操作很可能卡住。比如线上 1主 2 从的副本集，2 个从节点延迟 1 个小时，我希望将 1 个从节点剔除后换个高性能节点，那这个变更操作可能会卡 1 个小时。显然是无法接受的。     
可能有人会想到，可以参考加节点的方法，先将 2 个从节点变成 non-voting 模式，不算在 majority，配置变更就能很快提交了。然而问题是，调整 non-voting 本身就是配置变更操作，这项操作首先就会卡住。
2. 对主节点强依赖。
Raft 论文中描述的配置变更算法都依赖主节点执行，然而实际场景中我们可能遇到无主节点的情况。比如 3 节点副本集，有 2 个节点的机器永久性损坏了，不可能自己选出主节点，此时该如何恢复呢？      
一种简单粗暴的方法，就是将仅剩的节点停掉，然后改配置，再作为单节点的方式重启。     

### 4.4.2 MongoRaft 原理
MongoRaft 的实现原理可以参考作者发布的[论文](https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf)，MongoDB 4.4 版本的代码实现可以参考源码中的 [README.md](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md)。结合论文和代码实现，我认为 MongoRaft 和 Raft 在配置变更流程上有 3 点重大区别：   
1. **Logless**. 结合前面的分析我们看到 Raft 的配置变更流程还是要依赖日志同步，日志同步的快慢或多或少会对配置流程的时长会有影响。而 MongoRaft 采用了“无”日志的方式（也不是整个过程完全没日志，所以要加个引号），使用基于心跳的 Gossip-like 协议进行配置变更同步。在流程上将配置变更和数据同步分开，再保证数据安全的前提下尽量快速进行变更。    
2. **没有照搬 Raft 的 2阶段算法进行配置的安全过渡**。MongoRaft 使用了更加简单巧妙的方式。具体来说，新配置和旧配置必须保证 Quorum 节点有重叠，每次只能有一个 voting节点加入或者删除（对于 non-voting 成员，由于它们不属于 majority(quorum) 的计算范围，因此没有这项限制）。通过这个限制，可以使得协议实现更加简单，但是如果要添加或者删除很多节点，则可能需要发起多次配置。我们可以简单论证一下这种方法的安全性：当前配置C1 和新配置 C2 如果要同时选举产生不同的主节点，则必须通过获得 C1 的主节点需要获得 C1 中大多数节点的同意，C2 的主节点也需要获得 C2 中大多数节点的同意，但是现在 C1 中大多数节点和 C2 中大多数节点必定是有重叠的（QuorumOverlap），这些重叠的节点不会在同一个选举周期内给 2 个不一样的节点投赞成票。因此，通过反证法能够证明不可能同一时刻在 2 个配置中出现 2 个不一样的主节点。
3. **支持 unsafe(force) 模式**。真实业务场景中可能会出现节点损坏较多无法选主的情况，MongoRaft 支持在非主节点发起强制配置变更。Unsafe 模式会跳过很多安全性检查，有数据回滚、丢失的风险。因此，这种模式一般用来应急使用，而不是常规手段。

下面以 MongoDB 4.4 版本的实现进行详细分析。由于 unsafe 模式同 safe 相比主要是跳过了一些检查，因此这里只分析 safe 模式。

#### 配置构成
MongoDB 的配置方法和配置选项可以直接参考[官方文档](https://www.mongodb.com/docs/v4.4/reference/method/rs.reconfig/)，这里不再赘述。

#### 核心流程
配置信息持久化存储在每个 mongod 节点自己的 local.local.system.replset 表中。我们知道 local 库是每个 mongod 本地私有的，不会通过 oplog 同步到其他节点。因此，配置信息不是通过 oplog 同步，而是使用心跳进行同步。    
为了保证变更流程的安全性，除了上述每次添加和删除的节点个数限制之外，执行 reconfig 的主节点还要做 2 项检查（假设当前的配置是 C1,上一个配置是 C0, 即将要变更的配置是 C2）：    
1. 当前配置 C1，已经被复制到了副本集的大多数 voting 成员。
2. 上一个配置 C0 已经提交的数据（复制到大多数 C0 的 voting 成员），在当前配置C1 中也是提交状态，才能进行下一步的状态变更。

其中第 1 项检查保证了 C0 不再有形成大多数的机会。否则在 C1 到 C2 的变更过程中，C0 配置如果也选举了一个主节点，则整个副本集中将同时有  2 个主节点接受写入。    
第 2 项检查保证了已经提交的数据不会因为配置变更而回滚丢失，为了方便理解，参考下面的示意图：     

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9b10f846-0cf5-480a-9801-c8493846e9fc" width=700>

    
图中所有节点都是 voting 节点，最开始有一条数据在 A, C,E 上提交。节点 E 被删除后，副本集中只有 4 个节点，数据存在于 A 和 C 中，此时不满足大多数条件，但是数据还没有回滚丢失的风险。      
下一步，在没有等待数据同步到 B 时就又进行了一次配置变更，这次增加了一个节点 F。这时问题来了，如果 A 和 C 同时挂掉，会选举 B 或者 D 成为主节点继续接受写入，而 A 和 C 中的数据丢失了。即使 A 和C 重启了恢复正常，也会成为从节点，并将最近提交的数据回滚掉。    
可以看到，如果在第 2 次变更之前，先等待数据同步到 B ，使得数据变更满足大多数节点的提交状态，才能安全的进行下一步配置变更，示意图如下：      
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1769cb27-1435-461d-851c-b18873e10ff0" width=900>
    
检查完成，满足安全配置变更条件之后，主节点就执行真正的配置变更流程了，示意图如下：     
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/cbb17664-6e75-407e-afba-be7e512fac95" width=900>
    
主要流程为：    
1. 主节点确保当前配置同步到了大多数 voting 节点，awaitConfigCommitment(currentConfig).     
2. 主节点检查新配置是否合法，以及新配置中节点的状态情况，configValidation & checkQuorumForReconfig.   
3. 主节点将新配置持久化到本地的 local 库中，并更新内存中维护的配置信息。storeLocalConfigDocument & setCurrentRSConfig.   
4. 主节点等待配置同步到大多数节点（提交成功），这里同步是采用心跳的方式，因此最终也是由心跳处理流程唤醒。   
5. 2 个从节点给主节点发（周期性）心跳请求，心跳请求中会携带从节点自己的配置版本号。主节点发现自己的配置版本更高，因此会在心跳响应中塞入新配置，传给从节点。   
6. 2 个从节点接受到更高版本的配置信息，应用到自身节点。即持久化到本地的 local 库，并更新内存中维护的配置信息。storeLocalConfigDocument & setCurrentRSConfig.   
7. 主节点（周期性）给从节点发送心跳请求，发现 2 个从节点都使用了最新配置。认为新配置已经复制到了大多数 voting 节点，已经提交。此时可以给客户端返回 success.    

Mongod 节点内部维护了一个状态机，确保同一时刻主节点上最多只有 1 个 reconfig 操作处于执行中，避免并发操作相互干扰。   
如果在变更流程中出现了节点故障，或者新配置一直没有提交到大多数 voting 节点，则 reconfig 命令可能会失败或者超时。然而在后台新配置可能还是会通过 Gossip-like 协议完成提交。所以客户端在重试之前可以先做一些检查。   

整体变更流程中，心跳发挥了非常重要的作用，包括前期的配置检查 checkQuorumForReconfig, 新配置同步，以及同步情况检查。[心跳请求](https://github.com/mongodb/mongo/blob/r4.4.13/src/mongo/db/repl/repl_set_heartbeat_args_v1.h#L48)和[心跳响应](https://github.com/mongodb/mongo/blob/r4.4.13/src/mongo/db/repl/repl_set_heartbeat_response.h#L50)包含的信息如下：   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/a322ba73-5dfa-43be-9b5e-1d015c65106c" width=400>

   
副本集内任意2 个节点之间都会有周期性心跳（默认 2 秒），每个节点也可以在一些流程中主动触发心跳，比如节点拓扑发生改变，或者上述变更流程中的 checkQuorumForReconfig 逻辑等。心跳除了探活之外，还会携带 term，configVersion 等信息。如果节点在收到心跳请求时，发现对方的配置版本比自己低，则会在心跳响应中附带上自己完整的配置信息。    

#### 测试分析
我们可以通过下面的测试，验证一下 MongoDB 在主从延迟很大的时候能否快速完成配置变更。   
测试环境为：4.4.13 副本集，3 个 voting 节点，为了方便测试观察，心跳周期设置为 10s（默认 2s）, 选主超时配置为 60s（默认 10s）, 2 个 从节点 slaveDelay 10min（默认没有 slaveDelay, 设置为 hidden 是为了防止主节点切换）. 后台 10 秒 1 次写请求。   
配置变更操作为将一个从节点的 slaveDelay 从 600s 改成 601s.   

首先我们验证正常情况下的配置变更耗时。在执行变更前当前配置已经运行了很长时间，处于提交状态。而且没有切主及 term 变更。   
测试得到的总耗时为 10.902s, 通过在代码中添加日志，看看具体耗时的分布情况：   
1. 第1阶段，确认当前配置复制到了大多数 voting 节点，并且当前 term 的第一条oplog 同步到了大多数，耗时忽略不计（小于 1ms）。   
2. 第 2 阶段，配置检查+大多数节点状态检查+本地配置应用，耗时 811ms。   
3. 第 3 阶段，确认新配置同步到了大多数 voting 节点，耗时 10.091s。   

可见，正常情况下，配置变更流程的耗时主要集中在心跳检查阶段。由于 MongoDB 默认的心跳周期是 2s, 所以正常情况下 2s 左右就能完成配置变更，即使主从延迟有 10分钟也没关系。

再来看看异常情况，主节点重启并继续成为主(term++), 由于 slaveDelay 很大，会在第一阶段阻塞很久。总耗时 10min.   
通过加日志发现，这里的耗时基本都在第 1 阶段，等待当前 term 的第一条日志复制到大多数节点。由于我们测试环境的 slaveDelay 配置为 10 分钟，因此这里也卡了 10 分钟。通过代码我们可以看到第 1 阶段确认 oplog 最小提交时间的判断条件为 std::max(_lastCommittedInPrevConfig, _firstOpTimeOfMyTerm); 

另外，通过测试发现， reconfig 流程也会生成 oplog，这也是为什么要对 logless 的“无”日志变更加上引号的原因：   

<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/31511f6f-671b-46f1-abcd-9c06275bcecb" width=700>

 
同样，出现了选主及 term 变更，也会生成一条 oplog, 也就是上述 _firstOpTimeOfMyTerm 的依据。   

#### 小结
1. Raft 协议通过动态变更算法保证了配置变更流程中的可用性，并通过 2 阶段的方式保证安全。   
2. MongoRaft 在 Raft 基础上，借助心跳机制实现配置变更和数据复制分离，能够很大程度上缓解主从延迟大带来的配置变更耗时长的问题。并通过 QuorumCheck 的机制在保证安全性的同时简化了配置变更算法。   
3. MongoRaft 提供了 unsafe(force) 模式，能够在非主节点上实现强制变更，能够在极端情况下快速完成服务恢复。   


## 3.5 一致性讨论

# 4. 总结

# 5. 参考文档
1. Design and Analysis of a Logless Dynamic Reconfiguration Protocol， https://drops.dagstuhl.de/opus/volltexte/2022/15801/pdf/LIPIcs-OPODIS-2021-26.pdf
2. In Search of an Understandable Consensus Algorithm (Extended Version)，https://raft.github.io/raft.pdf
3. https://raft.github.io/
4. https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md
5. 共识协议的技术变迁，https://mp.weixin.qq.com/s/UY9TPMcuf0O7xS0kuXTcVw


