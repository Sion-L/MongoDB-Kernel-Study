# 1. 导语    
相信大家对数据库事务的  ACID 特性都不陌生，其中的 "I" 代表了读数据的隔离级别（Isolation），常见的有：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）、快照读（Snapshot Read）、事务串行化（Serializable）等。不同隔离级别下，读取的数据版本以及资源消耗是存在差异的，用户需要根据自己的业务场景做出正确选择。

如果将隔离级别放在多副本的分布式数据库中，则情况会更加复杂。主要的区别有：    
- 本地提交（local commit） 和大多数提交（majority commit）。如果请求只在 primary 节点上完成了提交（并持久化），还是可能在节点重启之后数据回滚。    
- 主从延迟带来的数据不一致。即使请求满足了 majority commit 的条件，避免数据回滚，也可能会在读取到 secondary 节点时，由于主从延迟的原因读取不到刚才的提交。    
- 内部数据均衡带来的脏数据问题。分布式数据库一般采用 Sharding 的方式来搞定海量数据，为了避免不同 shard 之间存在数据倾斜，内部会有数据迁移机制来保证数据均匀。在数据迁移过程中，可能出现 1份数据同时存储在 2 个shard 中的情况。    

同样，分布式数据库中不同隔离级别带来的数据版本保证以及资源消耗都是不一样的。有些业务场景需要避免读取到脏数据和可能回滚的数据，可以通过设置更高的隔离级别来做到这一点，但是无可厚非的需要更多的机器资源。而有些业务正好相反，更加注重读取性能。    

为了满足不同业务场景的数据读取要求，MongoDB 提供了可调隔离级别 ReadConcern，用户可以通过自定义 ReadConcern 配置来达到想要的效果。    

# 2. ReadConcern 的定义
MongoDB  中的 [ReadConcern](https://www.mongodb.com/docs/v5.0/reference/read-concern/#read-concern-levels) 有 5 种级别，按照由低到高的顺序列举如下：    
|级别|对应的场景|使用事项|
|:--|:--|:--|
|available|返回本地提交的数据。由于不保证数据已经复制到大多数节点，因此读到的数据存在回滚风险。<br>如果是分片集群，可能读取到还没有来得及删除的孤儿文档（内部迁移时临时存在的脏数据）。<br>提供了最低的处理时延。|3.6 内核版本开始引入。<br>不能在 logical session 和 transaction 中使用。|
|local|返回本地提交的数据。由于不保证数据已经复制到大多数节点，因此读到的数据存在回滚风险。<br>如果是分片集群，不会读取到孤儿文档（这也是包括  local 在内的其他级别和 available 的重要区别）。|默认的使用方式。|
|majority|返回完成大多数提交的数据。需要注意的是，副本集中各个节点看到的大多数提交点（majority committed point）可能不一样，secondary 节点的 majority 数据版本可能更旧。|需要使用 WiredTiger 存储引擎。<br>保持 enableMajorityReadConcern 服务端的参数默认开启。<br>在 transaction 操作中， 需要和 writeConcern : majority 配合使用。|
|snapshot|读取当前时间点完成大多数提交的数据。如果包含多个子请求，能保证每次读取的数据版本都一样，point in time read。|4.0 内核版本开始引入，5.0版本开始开放给事务外的普通读请求（find, aggregate, distinct）。<br>需要使用 WireddTiger 存储引擎。<br>保持 enableMajorityReadConcern 服务端的参数默认开启。<br>在 transaction 操作中， 需要和 writeConcern : majority 配合使用。|
|linearizable|线性一致性。<br>保证本次读请求之前发生的所有写请求都完成大多数提交，并且均对本次读操作可见。<br>和 writeConcern : majority配合使用，能够实现线性一致性（[Real Time Order](https://www.mongodb.com/docs/v5.0/reference/read-concern-linearizable/#real-time-order)）。<br>|3.4 内核版本开始引入。<br>**只能读 primary  节点。**<br>不能使用在 logical session 和 transaction中。<br>读取请求需要携带过滤条件，保证只读取  1 条文档，才能发挥预期的效果。|

为了方便直观的理解 majority 和  local/available 的区别，我们借用官方文档的[例子](https://www.mongodb.com/docs/v5.0/reference/read-concern-local/#example)来进行说明。为了方便描述，当前的写请求是 write_0，上一个写请求 write_prev 已经完成提交并复制到所有节点。除此之外没有其他写请求。    
<TODO readConcern 图>

上述例子对应的时序状态如下：    
|时间|操作|local commit 进度（读 local/available）|majority commit 进度|读 majority|
|:--|:--|:--|:--|:--|
|t0|write_0 在主节点 P 上提交|P : write_0<br>S1 : write_prev<br>S2 : write_prev|P : write_prev<br>S1 : write_prev<br>S2 : write_prev|P : write_prev<br>S1 : write_prev<br>S2 : write_prev|
|t1|write_0 复制到 S1|P : write_0<br>S1 : write_0<br>S2 : write_prev|P : write_prev<br>S1 : write_prev<br>S2 : write_prev|P : write_prev<br>S1 : write_prev<br>S2 : write_prev|
|t2|write_0 复制到 S2|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_prev<br>S1 : write_prev<br>S2 : write_prev| : write_prev<br>S1 : write_prev<br>S2 : write_prev|
|t3|P 收到 S1 的复制进度反馈，并给 Client返回成功|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_0<br>S1 : write_prev<br>S2 : write_prev|P : write_0<br>S1 : write_prev<br>S2 : write_prev|
|t4|P 收到 S2 的复制进度反馈|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_0<br>S1 : write_prev<br>S2 : write_prev|P : write_0<br>S1 : write_prev<br>S2 : write_prev|
|t5|S1 收到 P 的 mcp(majority committed point) 通知|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_0<br>S1 : write_0<br>S2 : write_prev|P : write_0<br>S1 : write_0<br>S2 : write_prev|
|t6|S2 收到 P 的 mcp 通知|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_0<br>S1 : write_0<br>S2 : write_0|P : write_0<br>S1 : write_0<br>S2 : write_0|

关于 available/local/majority 的读取机制，总结如下：    
1. available/local 方式：只要节点状态机更新了，就能马上读到。
2. majority方式：读取截止 mcp （majority committed point）的尽可能新的数据，而 mcp 由primary 复制推进，并且传播到副本集的其他节点。

# 3. ReadConcern 的实现
在分析 readConcern 的实现之前， 先推荐一些关键的内核代码位置（4.2 版本），方便进行对照查阅：    
1. [src/mongo/db/read_concern_mongod.cpp](https://github.com/mongodb/mongo/tree/r4.2.24/src/mongo/db/read_concern_mongod.cpp) 里面有 readConcern 的 wait 逻辑，以及如何设置对应的 readConcern level 到 RecoveryUnit 中。
2. [src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L519) 中的 WiredTigerRecoveryUnit::_txnOpen 方法，里面说明了如何根据 level 选择合适的时间戳并调用 WT 引擎的接口去设置。
3. [src/mongo/db/db_raii.cpp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/db_raii.cpp#L101) 中的 AutoGetCollectionForRead 构造函数，里面说明了在使用默认 readConcern 读取从节点时，如何转换为 kLastApplied 方式进行“快照”读。

## 3.1 Local 的实现
Local 级别能够读取到节点上最新的数据。如果是 primary 节点，则读取引擎层的最新快照（storage engine's most recent snapshot）。如果是 secondary 节点，则读取到最近批量回放完成的时间点（lastApplied）。    
由于不存在等待 oplog 推进的过程，因此 local 级别能够很快返回结果，性能很好。    

需要指出的是，这里所说的“引擎层的最新快照” 和后面要说的 snapshot readConcern 并不相同。如果使用 local 级别扫描大量的数据，会在执行时不断 yield 并更新快照。Yield 可能会由执行器主动触发（比如写冲突），也有可能是定期触发（默认累积调用 shouldYiled 方法达到 [128](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/query_knobs.idl#L262) 次，或者距离上次 yield 过去了 [10](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/query_knobs.idl#L269) 毫秒）。    

整体流程可以参考 《[Tunable Consistency in MongoDB](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)》 论文中的描述：    
<TODO 图，扫描的时候定期更新 snapshot 的伪代码>

## 3.2 Available 的实现
Available 级别和 local 级别唯一的不同，就是在分片集群场景下，不会过滤内部数据均衡导致暂时残留的孤儿文档。这也是该级别和其他所有隔离级别最显著的差别。    
体现在代码中则是：    
1. mongod 侧在执行请求时，会判断 readConcern 级别，如果[不是 available](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/service_entry_point_common.cpp#L873)，会初始化这个请求的 OprationShardingState。    
2. OprationShardingState 初始化流程会将 mongos 请求中携带的 shardVersion 和 dbVersion 元数据解析出来，并保存在自己的对象中。    
3. 在生成执行计划时，如果 OperationShardingState::isOperationVersioned 为 true, 则在生成的执行计划中有一个 SHARD_FILTER 阶段。参考 [_getExecutorFind](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L754) 的实现。    
4. 请求在执行时，进入到 ShardFilterStage 阶段，会按照文档中的 shardKey 对比路由表，确认文档是否应该属于这个 shard。如果文档是脏数据，不属于这个 shard，会直接过滤掉，不会进入下一步执行计划。参考 [ShardFilterStage::doWork](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/exec/shard_filter.cpp#L67) 的实现。    

由于不存在等待 oplog 推进的过程，因此 available 级别能够很快返回结果。另外在分片集群场景下，available 级别没有 SHARD_FILTER 阶段，理论上性能最佳。

>很多MongoDB 资深用户在使用分片集群时，会选取合适的 shardKey 并进行 Hash 分片。在业务上保证数据均衡的情况下，会选择关闭数据均衡功能。对于这类使用场景，个人认为非常适合使用 available 级别去读数据。

## 3.3 Majority 的实现
顾名思义，majority 隔离级别读到的都是已经完成大多数提交的数据。这种方式能够保证读取到的数据不会因为主从切换发生回滚，但是数据新鲜度会有所降低。    
在请求开始执行时，如果是 majority 级别的 readConcern，会先获取当前的 "committedSnapshot"，然后调用存储引擎的 timestamp_transaction 接口设置时间戳，后续的读操作按照这个时间戳去读取。    

由于主从延迟的原因，"committedSnapshot" 在副本集中各个节点的推进速度是不一样的。    
比如 3 节点副本集，其中 P 和 S1 的 opTime 时间戳推进到了 t2，但是 S2 节点的 时间戳才推进到 t1。从前面文章的分析，我们知道主节点 P 会将 committed 时间戳推进到 t2，并且传播给 S1 和 S2。如果客户端读取 P 和 S1 节点，会读取 t2 的数据。如果读取 S2 节点，S2 节点是否会等待 opTime 复制到 t2，然后再返回 t2 时刻的数据呢？    
答案是否定的，S2 节点在计算 committedSnapshot 时，会综合考虑副本集中 mcp 的推进情况以及自身节点的复制进度（lastApplied 时间戳），对于上述场景，S2 节点会**不作等待地**返回 t1 时刻的数据。   

由于 majority 级别不需要等待复制进度，因此能够很快返回结果，性能较好。 

## 3.4 Snapshot 的实现
如果希望事务中的多个请求都是访问的同一个快照，即同一个时刻的数据，则可以使用 snapshot readConcern。    
从 5.0 内核版本开始，snapshot 级别也能使用在非事务场景（find, aggregate 和 distinct）。    
MongoDB 使用 [atClusterTime](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/read_concern_mongod.cpp#L321) 来标识希望读取哪个时刻的数据。如果客户端指定了 snapshot readConcern 但是没有指定 atClusterTime，服务端会生成 atClusterTime 并返回给客户端，客户端可以将这个时间戳用户后续的请求中。    

使用 atClusterTime 能够读取老版本数据，如果 atClusterTime 时间戳太小，则存储引擎需要维护非常多版本的数据，这对存储引擎来说是不小的压力。如果要维护的数据版本太多，会导致 WiredTiger 引擎的 cache 压力增大，并可能导致很多的硬盘换入换出（cache 存不下的数据版本会换出到硬盘上存储）。    
MongoDB 默认只维护最近 [5 分钟](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.minSnapshotHistoryWindowInSeconds)的数据版本，如果读取的 atClusterTime 太老，会报 SnapshotTooOld 错误。    

## 3.5 Majority/snapshot 在事务中的speculative isolation 机制
当 majority/snapshot 使用在事务中时，情况有所不同。对于事务来说，一般会涉及到多个读写操作，而且从事务从开始到提交会有较长一段时间。    
前面说到，majority级别会导致数据的新鲜度受到影响。如果仍然使用之前的 majority/snapshot 处理方式应用到事务中，会导致事务都是基于比较老版本的数据进行读写操作，这样在提交阶段会有很大的概率出现写冲突，然后进入失败重试逻辑。    
MongoDB 为了解决上述问题，引入了 speculative 机制。事务中的读写操作不再是根据大多数提交的时间戳去读，而是使用 "[no overlap](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/read_concern_mongod.cpp#L338)" 时间：存储引擎 [all_durable](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L650) 时间戳和 [lastApplied](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L676) 时间戳（secondary 节点）取小值。    

通过上述策略，事务在提交之前能够保证读取到更加新鲜的数据，降低冲突回滚的概率，但是不保证读取到的数据在未来是不会回滚的。在同时指定事务的writeConcern 也是 majority 时，就能保证在事务提交成功时，之前读取的数据也是 majority 提交的。引用官方文档中的描述：    
>This means a transaction speculatively executes without ensuring that the data read won't be rolled back until it commits. No matter the read concern, when a node goes to commit a transaction, it waits for the data that it read to be majority committed *as long as the transaction was run with write concern majority*. Because of speculative behavior, this means that the transaction can only provide the guarantees of majority read concern, that data that it read won't roll back, if it is run with write concern majority.

《[Tunable Consistency in MongoDB](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)》 论文中列举了一个例子来说明 speculative 机制的效果：    
<TODO 图 speculative majority>

C1 和 C2 是2 个客户端，它们几乎同时修改了同一条文档。P 是 primary 节点， S 是 secondary 节点。时间线从上到下。    
上图左侧是采用 speculative 机制的情况。update1 在 P 节点上执行成功后， update2 就能基于这个版本成功执行事务，不需要等待 update1 完成大多数提交。    
上图右侧是不采用 speculative 机制的情况。update1 在 P 上执行完成后， update2 还读不到它，而update2如果基于老版本的数据去执行之后会由于冲突检测判定失败。因此，update2 要等待 update1 复制到 S 节点后才能成功执行事务。可以看到，由于**图中红圈部分的等待，update2 的执行时间会大大增加。**

## 3.6 Linearizable 的实现
如果客户端 A 先提交了一条数据（复制到大多数节点），如何保证客户端 B 一定能够看到呢？MongoDB 中的 linearizable readConcern 能够解决这个问题。    
影响客户端 B 看到新版本数据的常见问题有 2 个：    
1. 客户端B 读到了一个延迟较大的 secondary 节点。Linearizable 规定了读操作只能在 primary 节点上执行，绕过了这个问题。    
2. 网络隔离导致临时出现了 2 个 primary，其中一个是少数派 primary ，另一个是多数派 primary。在较短的时间内，这2 个节点各自都认为自己是主节点，如果读到的是少数派 primary，则有可能读取到过时的数据。Linearizable 在读数据之前，会先[写一条 noop oplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/read_concern_mongod.cpp#L408)，并等待提交到大多数节点。如果能够提交成功，则说明当前节点是多数派 primary， 然后才执行读。    

由于 linearizable 级别会等待主从复制，因此性能较差。

## 3.7 因果一致性中的 afterClusterTime
因果一致性能够保证在同一个客户端内，上一个请求的执行结果，对下一个请求可见。为了实现这个特性，客户端会将前一个请求的执行时间 T，作为后一个请求的执行参数 {afterClusterTime: T } 传输到服务端。服务端节点会将 T 和自身的复制进度进行比较，确保自身复制进度不小于 T 之后再处理请求。    

由于 afterClusterTime 可能要等待主从复制，因此延迟可能较大。    

对于分片集群，可能存在不同 shard 之间写入不均的情况。比如 shard1 执行了写入，时间戳为 T，然后客户端使用 {afterClusterTime: T} 到 shard2 上去读取一条数据。但是 shard2 上可能长时间没有写入，oplog 中的时间戳非常老，此时请求就会出现卡顿。    
MongoDB 中每个 primary 节点在长时间没有数据写入时，默认会每隔 10 秒产生一个  noop 操作来推进时间戳。但是这个时间粒度还是非常粗的，比绝大多数请求的超时时间都长。    
为了解决上述可能出现的卡顿问题，MongoDB 中引入了 [waitForSecondaryBeforeNoopWriteMS](https://www.mongodb.com/docs/v5.0/reference/parameters/#mongodb-parameter-param.waitForSecondaryBeforeNoopWriteMS) 参数，secondary 节点在等待 10 毫秒后，如果还没有满足 afterClusterTime 的需求，则主动给 primary 节点发起一个 noop 请求去推进时钟。    

# 4. 使用方法和注意事项
在理解了各个级别 readConcern 的含义之后，我们还需要关注如何在代码中使用它，常见的关注点如下：    
1. **如何设置**。和 writeConcern 的设置方法一样，用户可以在创建链接的 URL 中设置，创建 mongoClient、session、transaction 多个阶段设置。从 4.4 内核版本开始，用户可以通过 [setDefaultRWConcern](https://www.mongodb.com/docs/v4.4/reference/command/setDefaultRWConcern/) 命令在服务端进行全局默认配置，通过 [getDefaultRWConcern](https://www.mongodb.com/docs/v4.4/reference/command/getDefaultRWConcern/) 命令获取服务端的全局配置。    
2. 命令、驱动、版本的**兼容性**。首先要确认使用的内核版本，然后遵循对应版本的官方文档，确认支持哪些级别的 readConcern。其次，确认使用的命令和对应级别的 readConcern 是否兼容，比如 5.0 版本虽然开放 snapshot 级别普通请求，但也只限于有限的几个命令。最后，要测试使用的驱动版本是否能兼容。    
3. **延迟问题**。Linearizable、atClusterTime 和 afterClusterTime 都有可能由于主从延迟过大导致请求卡住，建议在请求中携带 maxTimeMS 参数，避免请求一直阻塞。    

# 5. 参考文档
1. https://www.mongodb.com/docs/v5.0/reference/read-concern/. 
2. Tunable Consistency in MongoDB: https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf
3. https://github.com/mongodb/mongo/tree/r4.2.24
4. https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md
5. https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.rst
6. https://www.mongodb.com/docs/v5.0/reference/parameters/
7. https://www.mongodb.com/docs/v4.4/reference/command/setDefaultRWConcern/

