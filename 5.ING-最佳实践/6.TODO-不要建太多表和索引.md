>基于作者公开发布的文章进行补充整理：https://mp.weixin.qq.com/s/VBWGi-yL4zfygQokCEhZwg

# 1. 导语
MongoDB 以 "schema-free" 的 NoSQL 数据库著称，因此使用者可能有会一种**错觉** : ~~不需要进行表结构设计，可以任意创建表和索引。~~      

MongoDB 确实足够灵活：可以任意变更数据字段和类型，每个表最多可以定义 [64](https://www.mongodb.com/docs/v4.4/reference/limits/#mongodb-limit-Number-of-Indexes-per-Collection) 个索引，并支持嵌套索引，并且没有硬性的表个数限制。

然而，很多建表随意的用户会发现，在表个数越来越多之后，系统也越来越卡。即使数据库没有多少请求，请求的延迟仍然很大。正如 MongoDB [官方博客](https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-massive-number-collections/)中描述的场景。

对此，官方的建议是通过 schema-design 避免创建太多表，并给出了 1万 的建议值：
```
In general, we recommend limiting collections to 10,000 per replica set. When users begin exceeding 10,000 collections, they typically see decreases in performance.
```

本文从内核实现原理出发，说明表太多为什么会带来性能危害，以及如何避免此性能问题。     
本文基于的内核版本为 4.0。

# 2. 为什么不能太多库表
通过监控观察发现，原生MongoDB在库表和索引数量达到百万量级场景下， MongoDB实例在CPU、磁盘等资源远没有达到瓶颈时，也会出现操作卡顿性能下降的问题。    

从我们的运营观察来看，至少有以下 3 个非常严重的问题：    
- 性能严重下降，慢查询变多；
- 内存消耗增大，频繁出现OOM；
- 实例启动时间明显变长，可能达到小时级。


通过打点和日志调试，发现所有问题都指向存储引擎层，因此，WiredTiger 引擎成为了我们重点分析的对象。
## 2.1 WiredTiger 如何处理建表
MongoDB采用WiredTiger(简称WT) 作为默认存储引擎，整体架构如下图所示：    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/08bac29e-9954-4e04-937b-9d956325b448" width=300>
</p>

用户在 MongoDB 层创建的每个表和索引，都对应各自独立的 WT 表。    

数据读写经过以下 3 层：    
1. **WT Cache**：通过B+树缓存未压缩的库表数据，并通过自定义的淘汰算法确保内存占用在合理范围。    
2. **OS Cache**：由操作系统管理，缓存压缩后的库表数据。    
3. **数据库文件**：存储压缩后的库表数据。每个 WT 表对应一个独立的磁盘文件。磁盘文件划分成多个按照4KB对齐的extent(offset+length)，并通过 3 个链表来管理：available list(可分配的extent列表)，discard list(废弃的 extent 列表，但是还不能马上重用，可能其他 checkpoint 还在引用)和 allocate list (当前已分配的 extent 列表)。

## 2.2 内存消耗分析
**空闲表问题**    

我们先思考一个问题：如果用户不会在短时间内访问所有的表，必然有表长时间空闲，那为何非活跃表的数据长时间停留在内存呢？    
如果非活跃表占用的内存能够及时换出，那将有效提高一个普通规格的集群能够支持的最大表数量，从而避免频繁OOM。    

**测试空闲表的资源消耗**    

我们在云上创建一个2核4G的副本集，不断创建表（每个表2个索引），每个表在插入数据创建完成之后不再访问。**测试过程中发现 current active dhandle 一直在上升，而 connection sweep dhandles closed 指标却很平缓**。最终实例占用的内存也一直上升，在创建的表不足 1 万时，就触发了实例 OOM。

**Data handle & sweep thread**    

Data handle(简称 dhandle) 可以简单理解为 WT 资源的专属句柄，类似系统的 fd。    

在全局 WT_CONNECTION 对象中维护着全局的 dhandle list, 每个 WT_SESSION 对象维护着指向 dhandle list 的 dhandle cache。第一次访问 WT 表时，在全局 dhandle list 和 session dhandle cache 中没有对应的 dhandle，会为此表创建dhandle, 并放到全局 dhandle list 中。    

Sweep thread 后台线程每 10 秒扫描 WT_CONNECTION 中的 dhandle list，标记当前没有使用的 dhandle。如果打开的 btree 数量超过了close_handle_minimum(默认值250)，则检查有哪些 dhandle 在 close_idle_time 内一直处于 idle 状态，则关闭与此 dhandle 相关的 btree，释放一些资源（非全部资源），标记 dhandle 为 dead 以便引用它的 session 能够发现此 dhandle 已经不能再访问。接下来还得判断是否有 session 引用这个dead dhandle，如果没有则从全局list中移除。    

基于以上分析，我们有理由怀疑为何清理的效率这么差。深入分析 MongoDB 代码可知，在初始化 WT 引擎时将 close_idle_time 设置成了 100000s(~28h)。 这样设置导致的后果是 sweep 不够及时，不再访问的表仍然会长时间消耗内存。    

**验证猜想**
为了快速验证我们的分析，将代码中close_idle_time从原先的1万秒调成600秒，运行相同测试程序，发现节点没有OOM，1万个 collection成功插入，内存使用率也维持在较低值：    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/471a0794-0205-4692-9dd9-18b5fea63888" width=600>
</p>

Connection data handles currently active 在一开始就不再直线上升，而是有升有降，最终程序结束后，回归到 250，符合预期。所以，**对于线上业务表多、频繁 OOM、实例规格又小的集群，可以临时采取调整配置的方式来缓解**。但是这样又会导致dhandle的缓存命中率非常低，带来比较严重的性能下降。    

## 2.3 预热前性能分析
预热指顺序读取每个 collection 至少一条数据，以便让所有 collection 的 cursor 都进入到 session cursor cache。    

在整个测试过程中，每个线程随机选择 collection 进行测试，可以发现持续有慢查询。通过下面的监控展示可以看到，在 wait time 窗口，随着蓝色曲线（schema lock wait）飙高，read latency 也飙高。因此，有必要分析 schema lock 的使用逻辑。    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/fe44c336-ff2b-4795-8eaf-98a3a92f42cd" width=800>
</p>

**为什么简单的CRUD请求要直接或间接依赖schema lock呢？**

在测试过程中生成火焰图，得到函数调用栈如下：    

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/9807a480-2f39-4003-826a-80bab598b462" width=800>
</p>

在 "all" 上方还有很多像 "conn10091" 线程一样的火焰柱形，表明客户端处理线程都在抢 schema lock，而它是属于 WT_CONNECTION 对象的全局锁。这样也就解释了为何会产生那么大的 schema lock wait。    

**那为何线程都在执行open_cursor呢？**    

在执行CRUD操作时，MongoServer 线程需要选择一个 WT_SESSION, 再从 WT_SESSION 中获取表的 wtcursor，然后执行 findRecord, insertRecord, updateRecord 等操作。    

为了保证性能，在 MongoServer 侧和 WT 引擎侧都会缓存 wtcursor。但如果表是首次访问，此时 dhandle 和 wtcursor 还未生成，需要执行代价昂贵的创建操作，包括打开 file，建立 btree 结构等。    

结合代码和火焰图可知，open_cursor 获取 schema lock 之后在 get dhandle 阶段消耗了很多 CPU 时间。当系统刚启动未预热时，很显然会有大量的 open_cursor 调用，从而造成 spinlock 长时间等待。**可见，dhandle 越多，竞争也越大。若想减小这种 spinlock 竞争，就需减少 dhandle 数量，这样就能大大地加快预热速度。**

## 2.4 预热后性能分析
持续读写的场景下，在经历了“数据预热”阶段之后，每个 WT 表的 data handle 都完成了 open 操作，此时前文描述 schema lock 已经不再成为性能瓶颈。但是我们发现和表少的场景相比，性能仍然相对低下。    

为了进一步分析性能瓶颈，我们对读写请求的全链路各个阶段进行了分段统计和分析，发现在 data handle 缓存访问阶段耗时很长。   

WT_CONNECTION 和 WT_SESSION 会缓存 data handle，并采用哈希链表加速查找，默认的 hash bucket 的个数为 **512**。 在百万级库表场景下，每个 list 会变得很长，查找效率剧烈下降，从而导致用户的读写请求变慢。    

通过在WT代码中增加 data handle 缓存的访问性能统计，发现用户侧慢请求的个数和 data handle 访问慢操作的个数相关，而且时延分布也基本一致。如下所示：    
|慢操作次数统计|MongoDB用户侧慢请求个数|遍历 dhandle list 慢操作次数|
|:--|:--|:--|
|10-50ms|13432|19684|
|50-100ms|81840|75371|
|>100ms|12473|6905|

为了快速验证我们的分析，可以在压测时尝试将 bucket 个数提升 100 倍，使每个 hash 链表的长度大幅变短，发现性能会有几倍甚至数量级的提升。    

通过以上分析，可以得到的启示是：如果能够将 MongoDB 表共享到少量 WT 表空间中，能够降低 data handle 个数，提升 data handle 缓存的访问效率，从而提升性能。

## 2.5 启动速度分析

原生MongoDB在百万级库表场景下，启动 mongod 实例会耗时长达几十分钟，甚至超过 1 个小时。如果有多个节点在 OOM 之后不能被很快被拉起提供服务，则整体服务可用性将受到很大影响。    

**问题分析**    

为了观察启动期间 MongoServer 和 WT 引擎的流程和耗时分布，我们对 MongoDB 和 WT 引擎日志进行了分析统计。    

通过日志分析发现，耗时最长的部分为 WT 表的 reconfig 阶段。    

具体来说，在 mongod 启动时 mongoDbMain 的初始化阶段会初始化存储引擎，并执行 loadCatalog 初始化所有表的 WiredTigerRecordStore。在构造WiredTigerRecordStore 时，会根据表的 uri 执行 setTableLogging 配置底层 WT 表是否开启 WAL。最后调用底层 WT 引擎的 schema_alter 流程执行配置，这里会涉及到多次 IO 操作（获取原有配置，再和新配置进行比较，最后可能执行配置更新）。同理，mongoDbMain 也会初始化所有表的索引（WiredTigerIndex），也会执行对应的 setTableLogging 操作。    

另外，**以上流程都是串行操作，在库表索引变多时，整体耗时也会线性增长**。实测发现在百万库表场景下超过99%的耗时都在这个阶段，成为了启动速度的瓶颈。    

**为什么启动时要对WT表进行reconfig呢？**    

MongoDB 会根据表的用途以及当前的配置，决定是否对某个表开启 WAL，具体可以参考 WiredTigerUtil::useTableLogging 和 WiredTigerUtil::setTableLogging 的实现逻辑。在 mongod 实例启动以及用户建表时都会进行相关配置。仔细分析这里的逻辑，可以得到以下规律：**在表创建完成，对应的WT uri确定之后，这个表是否开启WAL是可以确定的，不会随着实例的运行发生改变。**

## 2.6 性能分析小结

WT 引擎在百万级库表场景下，会对应生成大量的 data handle 并导致性能下降，包括锁竞争、关键数据结构的缓存命中率低、部分串行化流程在百万级库表下带来的耗时放大等问题。    

# 3. 解决方案
从前面的分析可知，表多的性能瓶颈集中的 WT 引擎侧。可以从 2 个方面尝试去解决这个问题：    
- **MongoServer 侧**。尝试将用户的海量库表，映射为 WT 中的少量库表。
- **用户侧**。进行良好的表结构设计，用少量的库表支撑起海量业务。

## 3.1 MongoDB 架构优化
业界常用的方式为：通过前缀映射的方式，实现上层库表**共享**底层存储引擎的**表空间**。比如 [MongoRocks](https://zhuanlan.zhihu.com/p/32148445) 引擎就是如此，InnoDB 引擎也通过 [innodb_file_per_table](https://dev.mysql.com/doc/refman/8.0/en/innodb-file-per-table-tablespaces.html) 参数配置支持共享表空间。

MongoDB 也有 GroupCollections 结构来通过前缀映射共享表空间，不过官方只预留了接口，并没有实际实现。如果要使用这项功能，需要数据库开发者[自己实现](https://mp.weixin.qq.com/s/VBWGi-yL4zfygQokCEhZwg)。

共享表空间的架构优化确实能提升海量库表下的 MongoDB 性能，不过也带来了一些麻烦：    

- 删表和索引之后不能立马释放空间。
- 有些监控统计会收到影响，比如表的 storageSize、indexSize 无法直接再根据文件大小进行精确统计。
- 官方代码既不支持 Mongorocks 引擎，也没有支持 GroupCollections。因此，版本升级会是一个问题。

## 3.2 推荐：优化用户侧的表结构设计
正如本文开头所说， schema-free 不意味着 no schema design。如果能在使用时进行优秀的表结构设计，就不需要在 MongoDB 服务端进行各种性能调优，而且应用程序的健壮性和可移植性也要强很多。    

当业务侧在进行分表时，需要考虑是否会造成海量库表导致性能降低。一般常见的分表策略有：    
- 按照唯一属性（ID）分表。比如有些社交类业务会给每个聊天室单独创建一张表，或者给每个注册的用户单独创建一张表。这种是非常不可取的，表数量完全不可控。解决的方式也很简单，就是在每条文档中加入 ID 字段，然后塞到一张表中，每次查询时根据 ID 的索引进行过滤就好了。    
- 按照时间属性分表。这种分表方式确实带来了一些好处，对于过期的数据，直接 drop 表，速度极快。但是需要考虑一下分表的粒度，如果分的太细可能导致表很多。另外也可以考虑一下 bucket 方式，5.0 版本引入的时序表是否能满足自己的需求。

另外需要注意的是，在 MongoDB 中每创建一个索引，也会在 WT 中创建对应的表（table）。因此，可以通过 indexStats 命令统计没有使用或者重复的索引进行删除：    
```
db.collection.aggregate([{$indexStats:{}}])
```

# 4. 参考资料
1. https://mp.weixin.qq.com/s/VBWGi-yL4zfygQokCEhZwg
2. https://www.mongodb.com/docs/v4.4/reference/limits/#mongodb-limit-Number-of-Indexes-per-Collection
3. https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-massive-number-collections/
4. https://zhuanlan.zhihu.com/p/32148445
