# 1. 导语
Oplog 是 MongoDB 中一个特殊的、存储了变更记录的表，在主从数据复制、数据持久化保障、按时间点备份回档、changeStream 变更流、操作追溯等各方面都有着举足轻重的作用。因此，其实现的好不好，直接关系到了 MongoDB 的产品竞争力。   

在上文介绍 MongoRaft 时，已经说明了 oplog 的复制流程以及在数据持久化中发挥的作用，本文不再赘述。但是关于 oplog仍有很多其他方面的细节并未做分析。因此，本文会聚焦 oplog 实现中的核心问题进行分析，包括以下内容：   
1. Oplog 格式。分析 oplog 中每条文档会包含哪些字段，以及各自发挥的作用。   
2. 幂等性保证。存储系统中日志对于幂等性的需求并不罕见，那么MongoDB 在支持了各种操作算子的背景下，如何保证 oplog 的幂等性呢？   
3. 可见性判断。主从节点上都是并发写 oplog，都存在临时的 oplog 空洞，如何保证存在空洞的 oplog 不会被拉取到呢？   
4. 性能。分析 oplog 表如何在无索引的情况下保证高效的查询性能，以及如何优化oplog 表的删除机制，来避免对用户写入性能的影响。   
5. 回放。分析 oplog 的回放流程，探讨 MongoDB 内核以及外部工具如何使用 oplog。   

不同内核版本之间可能在 oplog 格式上存在差异，比如 4.2 版本引入了分布式事务相关的 oplog，取消了 hash 值作为每条 oplog 的 ID。   
本文的讨论基于 4.2.24 版本内核代码。

# 2. 格式
Oplog 是 local 库中一个特殊的固定大小的表，逻辑上可以看做是一个环形队列。里面的文档按照时间戳进行组织，当空间不足时，会自动删除最老的文档。和普通的用户表一样， oplog 表也支持各种过滤条件的查询、排序、映射等操作。   
Oplog 中每一条文档（oplog entry）记录了一次变更操作，文档中包含的字段相对比较固定，可以参考 [oplog_entry.idl](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_entry.idl) 代码中的定义，列举如下：    
|字段名|类型|是否可选|备注|
|:--|:--|:--|:--|
|ts|timestamp|必选|时间戳，由 int32 的 unix 秒级时钟和 int32 的秒内自增计数器共同组成。可以当做一个 int64 整数，是 **oplog 表的主键**。|
|t|long|必选|MongoRaft 协议中的 term。未引入 MongoRaft 协议的老版本没有这个字段。|
|h|long|4.2 版本开始逐步废弃|这条 oplog 文档的摘要。兼容4.0以及更老的版本，新版本这个字段为 0。|
|v|int64|必选|Oplog entry 格式的版本，默认为1。|
|wall|date|必选|机器的毫秒级时间。|
|op|string|必选|表示命令的类型，包括："i" -> insert 操作，"u" -> update 操作，"d" -> delete 操作， “c” -> 其他命令，比如建表，"n" -> noop, 没有变更操作时进行时钟推进。|
|ns|string|必选|表空间，一般是 “库名.表名”。如果只涉及库，不涉及具体的表，则为“库名.$cmd”。|
|ui|uuid|可选|表的 UUID 信息，如果对表进行了重命名，其 UUID 不变。|
|o|object|必选|执行的操作。比如对于 insert 操作，这个字段就包含了插入的文档。<br>多文档事务也会将操作打包成数组放在“o.applyOps” 字段中，数组中的每一项是一个 BSON Object，包含了事务中的每个子操作，比如一次 insert。<br>另外引入分布式事务之后，还对 “o” 增加了 3 个字段：<br>1."o.prepare"：表示是否为 prepareTransaction 的事务。<br>2."o.partialTxn": 表示这条 oplog 是否只是一部分。如果为 true，则需要通过 "prevOpTime" 将多个 oplog 串联起来。除了最后一条 oplog ，前面的 oplog 都需要设置这个标志位。<br>3."o.count"：记录事务中所有子操作（statement， 比如 1 次 insert）的个数，count 记录在最后一条 oplog 中，如果事务只有  1 条  oplog，则不需要记录这个字段。|
|o2|object|可选|操作的附加信息。比如对于 update 操作，除了执行的操作放在 o字段中之外，还需要将查询条件（对哪条文档执行的操作）放在 o2 字段中。|
|b|bool|可选|是否为 upsert（update 自动转 insert），兼容 3.6 及更老的版本。|
|fromMigrate|bool|可选|是否为 chunk 迁移产生的，存在于分片集群中。|
|stmtId|int32|可选|产生这个oplog entry的 statementId, 一个事务可能对应多个变更操作，每个操作有独立的 statementId。|
|prevOpTime|optime|可选|同一个事务中，上一条 oplog 的 optime。通过这项信息，可以把事务的多个 oplog 串联起来。|
|preImageOpTime|optime|可选|prevImage 信息存储在另外一条 oplog entry 中，这个字段存的是其“指针”信息。<br>**一般存在于 retryWrites 模式下， findAndModify 命令返回旧值的场景。普通的读写操作不会有这条记录。**|
|postImageOpTime|optime|可选|本次 update 操作执行之后，文档是啥样（postImage）。postImage 信息存储在另外一条 oplog entry 中，这个字段存的是其“指针”信息。<br>**一般存在于 retryWrites 模式下， findAndModify 命令返回新值的场景。普通的读写操作不会有这条记录。**|
|needsRetryImage|string|可选|可以是 "preImage" 或者 "postImage"。|

# 3. 幂等性
## 3.1 为什么要保证幂等性
当我们使用 mongdump 工具或者 find+getmore 命令进行全表扫描时，默认情况下读到的并非某个时间点的一致性数据（除非在高版本内核主动指定 readConcern 为 snapshot， 不过这种方式会带来资源消耗风险）。为了防止一个批量扫描请求长时间锁住太多资源（维护老版本的开销），MongoDB 的执行模式为定期 yield 释放老 snapshot，然后创建新 snapshot 继续扫描。参考《[Tunable Consistency in MongoDB](https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf)》论文中的说明：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/67f71ba0-e233-4866-aeb8-f990483a96b1" width=350>
</p>

定期释放snapshot 能够降低长期维护老数据版本造成的内存压力，但是带来的数据一致性问题需要通过其他方式来解决。对于 mongodump 来说，可以通过[指定 --oplog 参数](https://www.mongodb.com/docs/v4.2/tutorial/backup-and-restore-tools/#create-backups-using-oplogs)来备份从扫描开始到结束这段时间的所有 oplog，来达到一致性备份的目的，示意图如下：    
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/903a9cec-59e8-4e12-8823-ec7b3bf48f64" width=450>
</p>

1. t1时刻，mongodump 开始全量扫描，将第 1 条文档 {a:1} 备份下来。   
2. t2 时刻，扫描到第2 条文档，刚好在此之前用户将这条文档的 b 字段进行了加 1 操作，全量备份得到 {b: 2}，对这条文档的操作会记录 oplog。   
3. t3 时刻，扫描到第 3 条文档，刚好在此之前用户将这条文档的 c 字段进行了加 2 操作，全量备份得到 {c: 3}，但与此同时对第1 条文档{a: 1}的修改不会再扫描一次。t3 时刻对第 1 条和第 3 条文档的操作都会记录 oplog。   

可以看到，全量备份结束后，全量备份数据和 t3 时刻数据库的状态并不一致。但是在回放时，如果结合 oplog 的增量回放，就能准确地将数据库回档到 t3 时刻的一致性状态。   
细心的读者可能已经发现，在回放 oplog 时可能会有操作重复执行的情况。比如上例的全量备份中，第 2 条文档已经是修改过的 {b:2}，但是在 oplog 回放时可是会执行一次这条文档的修改操作。**如果 oplog 本身不能保证幂等性，则上述操作重复执行的情况可能导致数据不正确。**  

**特别说明**   
MongoDB 还支持以下 2 种一致性备份方式：   
1. 使用 fsyncLock 命令将节点锁住，不接收任何写入。等备份完成后，使用fsyncUnlock 命令解锁。   
2. 高内核版本使用  snapshot一致性读，等备份完成后释放snapshot。但是如果备份时间比较长，会带来较大的系统压力。   


除了 mongodump，还有很多流程依赖 oplog 的幂等性，比如跨集群的 DTS 数据同步工具，内核中的 [initial sync](https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md#initial-sync) 逻辑同步流程等。因此，oplog 对于幂等性的要求是毋庸置疑的。

## 3.2 如何保证幂等性
Oplog 实现幂等性的方法，就是存储变更操作后的结果，而不仅是具体的变更动作。比如对文档  {a: 1} 的 "a" 字段做加 1  操作，则 oplog 中记录 {$set: {a: 2}} 。    

对于 insert操作，在 "o" 字段中记录插入的完成文档，例如插入一条文档 {"_id" :10, "a": 5}，得到的 oplog 为：   
>{ "ts" : Timestamp(1702090192, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "wall" : ISODate("2023-12-09T02:49:52.960Z"), "o" : { "_id" : 10, "a" : 5 } }

对于 delete操作，只需要记录文档的 "_id" 即可，例如删除上一步插入的文档，得到的 oplog 为：    
>{ "ts" : Timestamp(1702090210, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "d", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "wall" : ISODate("2023-12-09T02:50:10.013Z"), "o" : { "_id" : 10 } }

对于 update 操作，则情况要复杂一点。 MongoDB 支持多种 update 方式：   
- [Replace](https://www.mongodb.com/docs/v4.2/tutorial/update-documents/) 方式，客户端将要更新的文档准备好，然后使用**整文档替换**的方式进行 update，有点把  MongoDB 当作 KV 存储使用的感觉。比如将文档 {"_id":0, "a":1, "b":2, "c":3} 替换为 {"_id":0, "a":1, "b":2, "c":4}，则执行 replace 更新的方式是 db.coll1.update({"_id":0}, {"_id":0, "a":1, "b":2, "c":4})， 得到的 oplog 如下， "o" 字段中包含了整条文档，而不只是修改过的 "c" 字段：
>{ "ts" : Timestamp(1702090827, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "o2" : { "_id" : 0 }, "wall" : ISODate("2023-12-09T03:00:27.142Z"), "o" : { "_id" : 0, "a" : 1, "b" : 2, "c" : 4 } }
- [Pipeline](https://www.mongodb.com/docs/v4.2/tutorial/update-documents-with-aggregation-pipeline/) 方式，4.2 内核版本开始支持，支持更高级的语法在MongoDB 服务侧对文档进行更新。这种方式下，oplog 中存储的也是整条新文档，和 Replace 方式相同。
- [Operator](https://www.mongodb.com/docs/v4.2/reference/operator/update/) 方式。MongoDB 支持十几个算子，在服务侧对指定文档的一个或多个字段进行**局部更新**。这种方式下，oplog 中只需要存储涉及到的字段的操作结果。相比 Replace 方式，Operator 方式的网络传输流量更小、占用的 oplog 和 journal 空间更小。比如将文档 {"_id":0, "a":1, "b":2, "c":3} 中的 "c" 字段改成 5，则执行局部更新的命令是  db.coll1.update({"_id":0}, {$set:{"c":5}})，得到的 oplog 如下，只包含修改过的字段：   
>{ "ts" : Timestamp(1702091364, 1), "t" : NumberLong(1), "h" : NumberLong(0), "v" : 2, "op" : "u", "ns" : "db1.coll1", "ui" : UUID("65a840e8-ab6d-4f56-9daf-fe1ccc33d052"), "o2" : { "_id" : 0 }, "wall" : ISODate("2023-12-09T03:09:24.932Z"), "o" : { "$v" : 1, "$set" : { "c" : 5 } } }

Operator 方式下的 oplog 构造流程相对较复杂。为了弄清其 oplog 构造流程，我们需要先看看 operator 更新条件是如何执行的。   
由于 MongoDB 的文档模型中存在 Object 和 Array 嵌套，MongoDB 在请求解析阶段会先将更新语句解析成一个前缀树，比如对嵌套字段 "a.b.c" 和 "a.b.d" 的更新具有相同的前缀 "a.b"。前缀树的根节点就是对某个字段的具体变更操作，比如 $inc $mul 算数操作等。    
参考[内核代码](https://github.com/mongodb/mongo/tree/r4.2.24/src/mongo/db/update)，前缀树的节点定义关系如下：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/1418abba-22f9-45fd-a11c-0bcffe23efdc" width=800>
</p>

以 update(<queryFilter>, {$set:{"a.b": 5, "c":6}, $inc:{"a.c":1}}) 这条语句的执行流程为例，MongoDB 首先会根据解析更新操作生成如下前缀树（updateTree）：   
<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/3e37c3ed-4c57-4a9a-9c4a-9fab9708f670" width=400>
</p>

执行计划的构造流程为：先使用 queryFilter 构造 [queryStage](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L1086-L1094)，然后基于此构造 [updateStage](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L1099-L1103)。在 queryStage 阶段查找出需要修改的文档，然后在 updateStage 阶段对文档进行修改。    
而 updateStage 的执行流程，本质上就是对上述 updateTree 中各个叶子节点执行 apply 的流程。在执行 apply 操作时，会传入 logBuilder 进行 oplog 的流式构造。以 {$inc: {"a.c": 1}} 为例，会在 ArithmeticNode 的 apply 流程中对 "a.c" 字段进行 “加1” 操作，然后将操作后的 newValue 传递给 logBuilder 转换为 {$set : {"a.c": newValue}} 合并到最终的 oplog 中。   

# 4. 可见性
设想在 primary节点上并发执行了 2 个插入请求的场景：   
1. 请求A 先执行，此时获得的时间戳版本是 t1。   
2. 请求 B 后执行，此时获得的时间戳版本是 t2。   
3. B 先提交，提交时会按 t2 时间戳写 oplog 和 journal。   
4. A 后提交，提交时会按 t1 时间戳写 oplog 和 journal。      

假设有另外一个 secondary 节点一直从 primary 节点拉取 oplog 进行同步，在第 3 步看到了时间戳为 t2 的 oplog 并拉过去回放。然后又按照（时间戳 >= t2）拉取下一批日志，此时第 4 步时间戳为 t1的 oplog 再也不会被拉取到。**此时主从复制就出现了错误**。   

为了弄明白为什么会有上述风险，需要先说明以下背景。   
1. **写请求为什么要伴随一个时间戳？**    
MongoDB 通过时间戳来作为数据版本进行可见性判断。包括事务以及普通的写请求，都会分配一个时间戳作为该请求的数据版本。   
和事务 ID 一样，时间戳通过[算法](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98-L142)保证即使出现物理时钟回调、主从切换等问题也能严格单调递增。   
2. **为什么写请求的时间戳要和 oplog 中的时间戳绑定？**   
主从复制除了保证数据一样，也要保证数据版本信息相同。否则如果出现主从切换，或者客户端选择到 secondary 节点读取数据，就会遇到数据版本混乱的问题。   
显然，将时间戳信息放在 oplog 中传递到 secondary 节点，是最优的办法。   

那么，如何避免日志空洞造成的复制问题呢？   
能想到的做法有：   
1. 在 primary 节点上保证不将有空洞的日志“提前泄露”给其他节点。   
2. 在 secondary 节点回放日志前，先检查日志不存在空洞。如果存在空洞，要等待日志中的空洞被填补后再回放。   

以上 2 种解决方案各有优劣。第 1 种方案能够保证 secondary 节点拉倒的日志就是能回放的，但是 primary 节点上如果有个请求执行太慢，导致空洞的时间存在太长，则空洞后面的日志迟迟得不到复制。第 2 种方案能够第一时间拉取到最新的日志，但是需要 secondary 节点上执行空洞检查流程，而且需要日志具备空洞检查的能力（比如每条日志都有上一条日志的“指针”）。

MongoDB 实现的是第 1 种方案。由于 oplog 在设计上也没有“指针”将前后日志串联起来，因此也不具备实现第 2 种方案的能力。   
另外需要注意的是，MongoDB 支持链式复制，secondary 节点也能作为其他节点的同步源。因此，primary 和 secondary 节点都需要考虑 oplog 的可见性问题。   

下面结合代码，分别说明 primary 和 secondary 上 oplog 可见性的实现逻辑。   

## 4.1 Primary 节点的 oplog 可见性
依旧使用上面并发插入的例子，分析用户线程并发写 primary 节点时，如何处理 oplog 可见性。执行流程为：   
1. 写请求 A 开始执行，调用 [LogicalClock::reserveTicks](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/logical_clock.cpp#L98-L142) 分配时钟（[全局锁](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/local_oplog_info.cpp#L124)内）。   
2. Logical Clock 模块给请求 A 分配时钟 t1, 并[注册](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1916)到 WiredTiger 存储引擎，声明有一个 t1 时钟的事务开始执行（全局锁内）。   
3. 写请求 A 继续执行。   
4. 写请求 B 开始执行，调用 LogicalClock::reserveTicks 分配时钟（全局锁内）。   
5. Logical Clock 模块给请求 B 分配时钟 t2, 并注册到 WiredTiger 存储引擎，声明有一个 t2 时钟的事务开始执行（全局锁内）。   
6. 写请求 B 继续执行。   
7. 写请求 B 提交，提交的时候会使用 t2 写 oplog。   
8. 写请求 A 提交，提交的时候会使用 t1 写 oplog。   

<p align="center">
<img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/cc224ae6-7151-482f-adbc-9eda83b8ad38" width=700>
</p>

与此同时，存在一个 [WTOplogJournalThread](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.cpp#L152-L230) 守护线程。在请求提交时，会通知这个线程刷 journal 并更新oplog可见性。   
WTOplogJournalThread 线程工作时，会调用存储引擎的 [query_timestamp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L2052) 接口获取 "all_durable" 时间戳，然后将这个时间戳作为当前 oplog 可见的最大时间戳。   
对应到上图的 t3 时刻，虽然请求 B 提交并以 t2 写入了 oplog，但是请求 A 还没有提交。因此，存储引擎的 query_timestamp 接口返回的 "all_durable" 还是 t0。所以请求 B的 oplog 还是不可见，直到请求 A 也完成了提交。通过这种方式，避免了存在空洞的 oplog 被错误读取。   

另外需要说明的是，4.0 版本采用 "[all_committed](https://github.com/mongodb/mongo/blob/r4.0.28/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.h#L46)" 时间来设置 oplog 可见性。 而在 4.2 版本开始引入了 2 阶段事务后，采用了 "[all_durable](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_oplog_manager.h#L44)" 时间来设置可见性。   
关于 2 者的区别可以参考[代码注释](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/storage_engine.h#L580-L589)：
>The all_durable timestamp only includes non-prepared transactions that have been given a commit_timestamp and prepared transactions that have been given a durable_timestamp. Previously, the deprecated all_committed timestamp would also include prepared transactions that were prepared but not committed which could make the stable timestamp briefly jump back.


当其他节点到 primary 上读取 oplog 时，会[获取当前的 oplog 可见时间](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L536)，只[读取可见范围](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1978)内的 oplog：   
```
// WiredTigerRecoveryUnit::_txnOpen 获取 oplog 可见时间  
if (_isOplogReader) {
    _oplogVisibleTs = static_cast<std::int64_t>(_oplogManager->getOplogReadTimestamp());
}

// WiredTigerRecordStoreCursorBase::next 只读取可见范围内的 oplog
// 这里的 id 就是 oplog 的 recordId，也就是 ts 字段
if (_oplogVisibleTs && id.repr() > *_oplogVisibleTs) { 
    _eof = true;
    return {};
}
```

## 4.2 Secondary 节点的 oplog 可见性
Secondary 的 oplog 可见性处理要更加复杂，我们可以从以下几方面来看：   
1. MongoRaft 一致性协议保证了每个副本的数据都是一致的。这里的一致性除了要保证每条文档的内容一致之外，还要保证每条文档的版本一致。   
2. 每条文档的版本通过时间戳来标识。所以 secondary 节点在写数据时，不能和 primary 一样在自己节点上维护一个逻辑时钟模块来单独推进数据版本，而是必须使用和 primary 节点一样的时间戳。而这个时间戳，就是 oplog 中的 “ts” 字段。所有的副本都统一使用 primary 节点写到 oplog 中的时间戳来执行写入。   
3. 在上一篇文章，我们提到，secondary通过 batch 内乱序回放来保证 oplog 的回放性能。因此，如果还是按照 primary 节点上的那套获取存储引擎 "all_durable" 时间戳的机制，无法正确的处理 oplog 空洞问题。   

因此，除了包含 primary 节点中使用的存储引擎 "all_durable" 时间戳推进机制之外，Secondary 节点上还提供了额外的“校准”机制：   
- 当一个 oplog batch 整体回放完成之后，[主动推进](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/sync_tail.cpp#L876) oplog 可见性。推进的依据就是 oplog batch 最后一条的时间戳。
- 其他节点过来读 oplog 时，严格遵循 snapshot 读。而 snapshot 使用的时间戳，是当前 oplog batch 回放之前的时间戳（也就是每次 oplog batch整体回放完成后，才推进的 [lastAppliedTimestamp](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L550)）。

关于上述问题，可以参考[代码注释](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_recovery_unit.cpp#L663-L669)：   
>On secondaries, however, the all_durable time, as computed by the storage engine, can advance before oplog application completes a batch. This is because the all_durable time is only computed correctly if the storage engine is informed of commit timestamps in increasing order. Because oplog application processes a batch of oplog entries out of order, the timestamping requirement is not satisfied. Secondaries, however, only update the last applied time after a batch completes. Thus last applied is a valid no-overlap point on secondaries.

# 5. 性能优化
## 5.1 Oplog 表没有索引，难道每次查询要全表扫描？   
资深的 MongoDB 用户都知道，oplog 表是没有索引的：   
```
mymongo:PRIMARY> use local
switched to db local
mymongo:PRIMARY> db.oplog.rs.getIndexes()
[ ]
```
没有索引，是否意味着查询请求都要全表扫描了？   

在之前介绍文档模型的文章中，我们提到，MongoDB 的存储引擎是使用一个对用户不可见的 recordId 来组织表的。RecordId 是一个自增的 int64, 每个表都有独立的取值空间。   
当用户使用索引查询时，会先通过索引字段找到对应的 recordId，再根据 recordId 去回表找到对应的文档。   

**既然 MongoDB 使用 int64 类型的 recordId 来组织表，而 oplog 中必备的 "ts" 字段也是递增 int64，是否可以合二为一呢？**   

答案是肯定的，"ts" 字段就是 oplog 表事实上的主键。在每条 oplog 写入的时候，会直接将 ["ts" 字段作为 recordId](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1298-L1303)。在使用 "ts" 字段过滤 oplog 时，会直接[按照主键扫描](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/query/get_executor.cpp#L760-L763)。   

相比于MongoDB 中需要再次回表查询的二级索引，oplog 直接使用 "ts" 主键组织表。**虽然没有索引，反而还更快了！**

另外需要说明的是，在4.2 以及之前的版本，为了能让 MongoDB 正确的使用 "ts" 主键查询，需要在 find 命令中指定 oplogReplay 参数，方便生成正确的执行计划。比如 secondary 节点到 primary 节点拉取 oplog 时，就要[设置 oplogReplay](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/oplog_fetcher.cpp#L356) 为 true 。在低版本内核中就曾经出现过没有正确使用 oplogReplay 导致分片集群hang 住的问题，比如 [_getNextSessionMods doesn't include oplogReplay flag to query new oplog created during migration](https://jira.mongodb.org/browse/SERVER-35745).

在 4.4 以及之后的版本，oplogReplay 参数已经废弃，MongoDB 能够自动识别是否按 "ts" 进行主键查询。参考[官方文档](https://www.mongodb.com/docs/current/reference/command/find/)：
>Starting in MongoDB 4.4, the oplogReplay field is deprecated. find fields that use the $gte, $gt, or $eq filter predicated on the ts field will automatically utilize the storage format of the replica set's oplog to execute the command more efficiently. If specified, the server accepts the oplogReplay flag for backwards compatibility, but the flag has no effect.

## 5.2 删除性能
### 5.2.1 Capped collection 的删除效率问题
Oplog 表是 MongoDB 中的一种 [capped collection](https://www.mongodb.com/docs/v4.2/core/capped-collections/) , 也叫固定集合、限制集合等。   
Capped collection 在创建时可以指定存储大小和文档条数限制。逻辑上类似于一个固定大小的环状空间，新数据不断在头部插入，当超过配置的大小限制时，覆盖尾部的空间。   
从实现方式来看，capped collection 本质上还是一个 btree 组织的表。每次插入的新数据有最大的 key，在插入完成后可能会[删除](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L1345)1 条或者多条最小 key，直到表的大小再次满足大小限制。   

Oplog 表作为一种 capped collection，在创建时配置其 maxSize，默认为磁盘空间磁盘空间的 5%。用户也可以在[配置文件](https://www.mongodb.com/docs/v4.2/reference/configuration-options/#replication.oplogSizeMB)中自行指定，或者在运行过程中通过 [repsSetResizeOplog](https://www.mongodb.com/docs/v4.0/reference/command/replSetResizeOplog/) 命令动态调整。   
比如我在配置文件中设置 oplogSizeMB 为 200，则可以通过 collectionStats 命令查看 maxSize 为：   
```
mymongo:SECONDARY> use local
switched to db local
mymongo:SECONDARY> db.oplog.rs.stats()
{
	"ns" : "local.oplog.rs",
	"size" : 24293567,
	"count" : 220484,
	"avgObjSize" : 110,
	"storageSize" : 3780608,
	"capped" : true,
	"max" : -1,
	"maxSize" : 209715200,
    ...
}
```

如果我们将 oplog 表当做常规的 capped collection 处理，则在运行一段时间，oplog 写满后，每次数据库的写入都会伴随如下操作：   
1. 表和索引的更新。   
2. 插入一条记录到 oplog ，并记录 journal。   
3. 删除一条甚至多条 oplog，保证不超过maxSize ，并记录 journal。   

Oplog 表的清理操作存在于用户写入请求的关键处理路径上，有没有办法降低其维护开销呢？   

### 5.2.2 Oplog 表的优化
在阐述 oplog表具体的优化措施之前，我们不妨先想几个问题：   
1. 我们需要保证 maxSize 的限制绝对精确么？一点空间也不能超过？   
显然不是的。比如我限定了 oplog 表大小是 1GB，即使有段时间突破了这个限制，变成了 1.1GB，其实也没啥问题。只要不是太离谱，后续能够回收到 1GB 就行了。   
2. 一定需要用户线程去完成回收的动作吗？      
显然也不是。用户线程在写入成功时，需要保证 oplog 插入成功。但是最老的 oplog 有没有被及时清理，说实话它并不需要关心。如果需要用户线程完成清理操作，反而会增加写请求的整体处理耗时。   

理解了上面 2 个问题后，Oplog 表的优化策略也非常明确了：**使用后台线程异步回收。**   

<p align="center">
  <img src="https://github.com/pengzhenyi2015/MongoDB-Kernel-Study/assets/16788801/0f63fbb2-7e10-41e4-b3cd-f8e57c2d3d3d" width=800>
</p>

如上图所示，每个 mongod 进程启动时，会生成一个 [OplogTruncaterThread](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store_mongod.cpp#L64) 后台线程。后台线程会在特定的条件下执行oplog 表中老记录的清理，而用户线程不需要再执行清理操作。    
为了提升清理效率，会将 oplog 表按时间顺序逻辑上划分成多个块（[oplogStone](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store_oplog_stones.h#L46)），每次按 oplogStone 批量删除。   

**如何划分 oplogStone，划分多少个 oplogStone?**   
Mongod 进程启动时，会调用 [WiredTigerRecordStore::OplogStones](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L162) 的构造函数初始化相关参数，计算每个 oplogStone 的大小。步骤如下：   
1. 每个oplogStoneSize至少 16MB。   
2. OplogStone 最低 [10](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/oplog_stone_parameters.idl#L46) 个，最多 [100](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/oplog_stone_parameters.idl#L39) 个。   
3. 根据 Oplog 表配置的大小，计算真正的 oplogStone 个数 numStones = maxSize / oplogStoneSize。并根据上述最少 10 个，最多 100 个的条件得到最终的 oplogStone 个数 numStonesToKeep。  
4. 计算每个 oplogStone 的大小 minBytesPerStone = maxSize / numStonesToKeep。      

假设 mongod 配置的 oplog 表大小为 200 MB，则 numStonesToKeep = 200MB/16MB = 12， minBytesPerStone = 200MB/12 ~= 17MB。   
如果 mongod 配置的 oplog 表大小为 5GB，则 numStonesToKeep = min(5GB/16MB, 100) = 100, minBytesPerStone = 5GB /100 ~= 50MB。   

确定了 minBytesPerStone 之后，就进入对现有 oplog 的 oplogStone 划分逻辑。根据当前 oplog 表大小的不同，MongoDB 支持 2 种划分策略：   
- 数据量比较小（当前 oplog 表的文档总条数小于 [200 * oplogStone 个数](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L345)），进入[全表扫描逻辑](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L359)。   
- 数据量比较大，进入[采样逻辑](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L385)。此时通过牺牲一定的精度换取划分速度的提升。   

如果是全量扫描逻辑，会生成一个普通 cursor，不断进行迭代并统计；如果是采样逻辑，会在确定步长之后，生成 randomCursor 后进行迭代并统计。   
如果有一些剩余oplog 没有划分到 oplogStone 中，会将这些 oplog 的大小和条数记录到累加器中。随着后续 oplog 的插入，累加器中统计的大小达到 oplogStone  的 minBytesPerStone 要求后，再[创建一个新 oplogStone](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/oplog_stone_parameters.idl#L46)。     
然后将计数器清零后，继续累积到 minBytesPerStone 创建下一个 oplogStone。如此循环反复。   
 

**何时触发对 oplogStone 清理？**   
在进程启动初始化阶段、创建新 oplogStone、使用 repsSetResizeOplog 命令调整 oplog 表大小时，都会触发对 oplogStone 的清理。   
OplogTruncaterThread 在决定删除最旧的 oplogStone 时，还会考虑 stableOptime，如果大于 stableOptime 的 oplog 被删除，会对 RTT 物理回滚算法造成影响。具体参考 [WiredTigerKVEngine::getPinnedOplog](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L2166) 算法。   
如果主从延迟特别大，导致 stableOptime 比较老，则主节点上可能会存在 oplogStone 个数超出预期的情况，进而导致 oplog 表膨胀。在使用MongoDB 时需要注意这个问题。参考[官方文档](https://www.mongodb.com/docs/v4.2/core/capped-collections/)的说明：   
>Starting in MongoDB 4.0, unlike other capped collections, the oplog can grow past its configured size limit to avoid deleting the majority commit point.

# 6. 回放
关于 secondary 节点如何使用  oplog 同步，已经在上一章节进行了详细阐述。这里介绍如何在 primary 上回放 oplog 进行数据回档。   

MongoDB提供了 [applyOps](https://www.mongodb.com/docs/current/reference/command/applyOps/) 命令进行oplog 回放。比如在 MongoDB 实例上插入了 2 条文档，会对应生成 2 条 oplog。如果需要根据oplog 进行数据回放，可以执行如下命令：   
>db.adminCommand({applyOps:[{ "ts" : Timestamp(1704286579, 1), "t" : NumberLong(21), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.test1", "ui" : UUID("96e64020-2714-4024-b943-0b41988e723a"), "wall" : ISODate("2024-01-03T12:56:19.442Z"), "o" : { "_id" : ObjectId("659559730b9738bd45dc6a96"), "a" : 1 } }, { "ts" : Timestamp(1704286582, 1), "t" : NumberLong(21), "h" : NumberLong(0), "v" : 2, "op" : "i", "ns" : "db1.test1", "ui" : UUID("96e64020-2714-4024-b943-0b41988e723a"), "wall" : ISODate("2024-01-03T12:56:22.416Z"), "o" : { "_id" : ObjectId("659559760b9738bd45dc6a97"), "b" : 2 } }]})

applyOps命令在实现时充分考虑了 oplog 的幂等性，即使重复回放也能保证数据一致性。一些常见的重复回放操作如下：   
|操作类型|执行状态|
|:--|:--|
|重复插数据|数据已存在，则用 applyOps 中的值覆盖。|
|重复删数据|数据已删除，则忽略。|
|更新不存在的数据|如果不存在，则使用 upsert 插入数据。|
|重复建表|表已存在，则忽略。|
|重复删表|表已删除，则忽略。|
|重复删库|库已删除，则忽略。|
|重复建索引|索引已存在，不会重复创建。但是主节点的 oplog表中还是会记录一条日志。|
|重复删索引|索引已删除，日志文件中会打印一条 drop index日志，但是 oplog 中不会记录。|

一个典型的使用场景，就是 mongorestore 在进行一致性回档时，通过 applyOps 完成 oplog 的增量回放。

但是需要说明的是，**applyOps 并不支持并发**，绝大多数场景下都会[获取全局写锁](https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/db/repl/apply_ops.cpp#L391-L398)后执行。参考[官方文档](https://www.mongodb.com/docs/current/reference/command/applyOps/)的说明：
>This command obtains a global write lock and will block other operations until it has completed.

对于需要并发回放的 DTS 在线数据传输和极速数据回档等场景，一般都需要解析 oplog 后转换成普通的 Insert/Update/Delete 命令并发执行，而不是通过  applyOps 回放。

# 7. 总结
1. Oplog 在设计上充分考虑了幂等性，并尽可能保证精简。   
2. 通过精妙的可见性设计，避免了 oplog 空洞对复制流程的正确性造成影响。   
3. 通过将 "ts" 和存储引擎侧的 recordId 结合，Oplog 表采用按主键组织表的方式，既省略了索引，又提升了性能。   
4. Oplog 表采用异步批量回收的方式，提升了删除效率，降低了对用户写流程的影响。   



# 参考文档
1. https://www.mongodb.com/docs/v4.2/reference/operator/update/
2. https://www.mongodb.com/docs/v4.2/tutorial/update-documents-with-aggregation-pipeline/
3. https://www.mongodb.com/docs/v4.2/tutorial/backup-and-restore-tools/#create-backups-using-oplogs
4. https://www.mongodb.com/docs/v4.2/reference/method/db.fsyncLock/#db.fsyncLock
5. https://www.mongodb.com/docs/current/reference/command/find/
6. https://www.mongodb.com/docs/current/reference/read-concern-snapshot/#mongodb-readconcern-readconcern.-snapshot-
7. https://github.com/mongodb/mongo/tree/r4.2.24/src/mongo/db/update
8. Tunable Consistency in MongoDB： https://www.vldb.org/pvldb/vol12/p2071-schultz.pdf
9. https://github.com/mongodb/mongo/blob/r4.4.24/src/mongo/db/repl/README.md#initial-sync
10. https://github.com/mongodb/mongo/blob/r4.2.24/src/mongo/
11. https://jira.mongodb.org/browse/SERVER-35745
12. https://source.wiredtiger.com/3.2.1/struct_w_t___s_e_s_s_i_o_n.html#afb5b4a69c2c5cafe411b2b04fdc1c75d   
13. https://www.mongodb.com/docs/current/reference/command/applyOps/   
14. https://github.com/mongodb/mongo-tools/blob/100.7.0/mongorestore/oplog.go   
